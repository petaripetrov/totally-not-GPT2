{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd739fd7",
   "metadata": {
    "_cell_guid": "4c394a99-f5a3-4b23-95cd-477128ee993e",
    "_uuid": "1670d5c9-6e4f-477a-a5d1-356672775fbc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-07T15:42:54.918418Z",
     "iopub.status.busy": "2024-07-07T15:42:54.918050Z",
     "iopub.status.idle": "2024-07-07T15:45:18.998101Z",
     "shell.execute_reply": "2024-07-07T15:45:18.996675Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 144.089859,
     "end_time": "2024-07-07T15:45:19.000966",
     "exception": false,
     "start_time": "2024-07-07T15:42:54.911107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\r\n",
      "Collecting torch==2.3.1\r\n",
      "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2024.3.1)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting triton==2.3.1 (from torch==2.3.1)\r\n",
      "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n",
      "Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.1.2\r\n",
      "    Uninstalling torch-2.1.2:\r\n",
      "      Successfully uninstalled torch-2.1.2\r\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 tiktoken-0.7.0 torch-2.3.1 triton-2.3.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken tqdm datasets torch==2.3.1\n",
    "# by default (at the moment 18/06/2024) kaggle installs pytorch 2.1.something,\n",
    "# which for some reason breaks with the latest version of triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2addbea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T15:45:19.170697Z",
     "iopub.status.busy": "2024-07-07T15:45:19.170284Z",
     "iopub.status.idle": "2024-07-07T15:45:20.348439Z",
     "shell.execute_reply": "2024-07-07T15:45:20.347179Z"
    },
    "papermill": {
     "duration": 1.260031,
     "end_time": "2024-07-07T15:45:20.351181",
     "exception": false,
     "start_time": "2024-07-07T15:45:19.091150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-07 15:45:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: 'input.txt'\r\n",
      "\r\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \r\n",
      "\r\n",
      "2024-07-07 15:45:20 (26.2 MB/s) - 'input.txt' saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70355c0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T15:45:20.509969Z",
     "iopub.status.busy": "2024-07-07T15:45:20.509052Z",
     "iopub.status.idle": "2024-07-07T15:45:20.518756Z",
     "shell.execute_reply": "2024-07-07T15:45:20.517812Z"
    },
    "papermill": {
     "duration": 0.091543,
     "end_time": "2024-07-07T15:45:20.520901",
     "exception": false,
     "start_time": "2024-07-07T15:45:20.429358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fineweb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fineweb.py\n",
    "\n",
    "\"\"\"\n",
    "FineWeb-Edu dataset (for srs pretraining)\n",
    "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
    "Downloads and tokenizes the data and saves data shards to disk.\n",
    "Run simply as:\n",
    "$ python fineweb.py\n",
    "Will save shards to the local directory \"edu_fineweb10B\".\n",
    "\"\"\"\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------\n",
    "local_dir = \"edu_fineweb10B\"\n",
    "remote_name = \"sample-10BT\"\n",
    "shard_size = int(1e8) # 100M tokens per shard, total of 100 shards\n",
    "\n",
    "# create the cache if it doesn't exist yet\n",
    "DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), local_dir)\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# download the dataset\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n",
    "\n",
    "# init the tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
    "\n",
    "def tokenize(doc):\n",
    "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
    "    tokens = [eot]\n",
    "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "    return tokens_np_uint16\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "    \n",
    "# tokenize all documents and write output shards, each of shard_size tokens (last shard has remainder)\n",
    "max_shards = 50\n",
    "nprocs = max(1, os.cpu_count()//2)\n",
    "with mp.Pool(nprocs) as pool:\n",
    "    shard_index = 0\n",
    "    # preallocate buffer to hold current shard\n",
    "    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "    token_count = 0\n",
    "    progress_bar = None\n",
    "    for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
    "        \n",
    "        if shard_index > max_shards:\n",
    "            break\n",
    "\n",
    "        # is there enough space in the current shard for the new tokens?\n",
    "        if token_count + len(tokens) < shard_size:\n",
    "            # simply append tokens to current shard\n",
    "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "            token_count += len(tokens)\n",
    "            # update progress bar\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "            progress_bar.update(len(tokens))\n",
    "        else:\n",
    "            # write the current shard and start a new one\n",
    "            split = \"val\" if shard_index == 0 else \"train\"\n",
    "            filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
    "            remainder = shard_size - token_count\n",
    "            progress_bar.update(remainder)\n",
    "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
    "            write_datafile(filename, all_tokens_np)\n",
    "            shard_index += 1\n",
    "            progress_bar = None\n",
    "            # populate the next shard with the leftovers of the current doc\n",
    "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
    "            token_count = len(tokens)-remainder\n",
    "\n",
    "    # write any remaining tokens as the last shard\n",
    "    if token_count != 0 and not shard_index > max_shards:\n",
    "        split = \"val\" if shard_index == 0 else \"train\"\n",
    "        filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "        write_datafile(filename, all_tokens_np[:token_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed5a0b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T15:45:20.731456Z",
     "iopub.status.busy": "2024-07-07T15:45:20.731073Z",
     "iopub.status.idle": "2024-07-07T16:59:28.480567Z",
     "shell.execute_reply": "2024-07-07T16:59:28.479399Z"
    },
    "papermill": {
     "duration": 4447.832596,
     "end_time": "2024-07-07T16:59:28.484212",
     "exception": false,
     "start_time": "2024-07-07T15:45:20.651616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████| 22.9k/22.9k [00:00<00:00, 29.6MB/s]\r\n",
      "Resolving data files: 100%|███████████████| 1630/1630 [00:00<00:00, 2602.96it/s]\r\n",
      "Downloading data: 100%|████████████████████| 2.15G/2.15G [02:22<00:00, 15.1MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:08<00:00, 249MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:07<00:00, 273MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:07<00:00, 270MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:07<00:00, 293MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:07<00:00, 294MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:07<00:00, 291MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:07<00:00, 283MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:07<00:00, 279MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:08<00:00, 266MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:07<00:00, 274MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 2.15G/2.15G [00:24<00:00, 86.4MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 2.15G/2.15G [00:08<00:00, 262MB/s]\r\n",
      "Downloading data: 100%|██████████████████████| 541M/541M [00:06<00:00, 77.8MB/s]\r\n",
      "Generating train split: 100%|█| 9672101/9672101 [07:28<00:00, 21545.34 examples/\r\n",
      "Loading dataset shards: 100%|███████████████████| 98/98 [00:22<00:00,  4.39it/s]\r\n",
      "Shard 0: 100%|███████████| 100000000/100000000 [01:09<00:00, 1432109.62tokens/s]\r\n",
      "Shard 1: 100%|███████████▉| 99997871/100000000 [01:09<00:00, 1433223.24tokens/s]\r\n",
      "Shard 2: 100%|███████████▉| 99999982/100000000 [01:10<00:00, 1425441.34tokens/s]\r\n",
      "Shard 3: 100%|███████████▉| 99999934/100000000 [01:09<00:00, 1433470.35tokens/s]\r\n",
      "Shard 4: 100%|███████████▉| 99999796/100000000 [01:09<00:00, 1434102.17tokens/s]\r\n",
      "Shard 5: 100%|███████████▉| 99998855/100000000 [01:10<00:00, 1418274.15tokens/s]\r\n",
      "Shard 6: 100%|███████████▉| 99997594/100000000 [01:09<00:00, 1436690.42tokens/s]\r\n",
      "Shard 7: 100%|███████████▉| 99996910/100000000 [01:10<00:00, 1426026.31tokens/s]\r\n",
      "Shard 8: 100%|███████████▉| 99999293/100000000 [01:10<00:00, 1425734.13tokens/s]\r\n",
      "Shard 9: 100%|███████████▉| 99997859/100000000 [01:09<00:00, 1429824.07tokens/s]\r\n",
      "Shard 10: 100%|██████████▉| 99999807/100000000 [01:09<00:00, 1429937.87tokens/s]\r\n",
      "Shard 11: 100%|██████████▉| 99999526/100000000 [01:09<00:00, 1436357.68tokens/s]\r\n",
      "Shard 12: 100%|██████████▉| 99996983/100000000 [01:09<00:00, 1444725.28tokens/s]\r\n",
      "Shard 13: 100%|██████████▉| 99995675/100000000 [01:10<00:00, 1427769.86tokens/s]\r\n",
      "Shard 14: 100%|██████████▉| 99992321/100000000 [01:09<00:00, 1432307.66tokens/s]\r\n",
      "Shard 15: 100%|██████████▉| 99994498/100000000 [01:10<00:00, 1423561.62tokens/s]\r\n",
      "Shard 16: 100%|██████████▉| 99999908/100000000 [01:11<00:00, 1404528.60tokens/s]\r\n",
      "Shard 17: 100%|██████████▉| 99999237/100000000 [01:11<00:00, 1398900.38tokens/s]\r\n",
      "Shard 18: 100%|██████████▉| 99999699/100000000 [01:11<00:00, 1390978.15tokens/s]\r\n",
      "Shard 19: 100%|██████████▉| 99996354/100000000 [01:10<00:00, 1424265.41tokens/s]\r\n",
      "Shard 20: 100%|██████████▉| 99997912/100000000 [01:10<00:00, 1428158.90tokens/s]\r\n",
      "Shard 21: 100%|██████████▉| 99999786/100000000 [01:10<00:00, 1417430.19tokens/s]\r\n",
      "Shard 22: 100%|██████████▉| 99997168/100000000 [01:10<00:00, 1417022.77tokens/s]\r\n",
      "Shard 23: 100%|██████████▉| 99999515/100000000 [01:10<00:00, 1410946.84tokens/s]\r\n",
      "Shard 24: 100%|██████████▉| 99994716/100000000 [01:11<00:00, 1403922.35tokens/s]\r\n",
      "Shard 25: 100%|██████████▉| 99998669/100000000 [01:10<00:00, 1419004.16tokens/s]\r\n",
      "Shard 26: 100%|██████████▉| 99994587/100000000 [01:11<00:00, 1403262.20tokens/s]\r\n",
      "Shard 27: 100%|██████████▉| 99999173/100000000 [01:10<00:00, 1409426.25tokens/s]\r\n",
      "Shard 28: 100%|██████████▉| 99999394/100000000 [01:11<00:00, 1401922.30tokens/s]\r\n",
      "Shard 29: 100%|██████████▉| 99999350/100000000 [01:11<00:00, 1406307.92tokens/s]\r\n",
      "Shard 30: 100%|██████████▉| 99999935/100000000 [01:12<00:00, 1385750.04tokens/s]\r\n",
      "Shard 31: 100%|██████████▉| 99999984/100000000 [01:12<00:00, 1387659.54tokens/s]\r\n",
      "Shard 32: 100%|██████████▉| 99966920/100000000 [01:11<00:00, 1398012.93tokens/s]\r\n",
      "Shard 33: 100%|██████████▉| 99999906/100000000 [01:11<00:00, 1402461.98tokens/s]\r\n",
      "Shard 34: 100%|██████████▉| 99999699/100000000 [01:10<00:00, 1412838.53tokens/s]\r\n",
      "Shard 35: 100%|██████████▉| 99999462/100000000 [01:12<00:00, 1385999.30tokens/s]\r\n",
      "Shard 36: 100%|██████████▉| 99993819/100000000 [01:11<00:00, 1389277.24tokens/s]\r\n",
      "Shard 37: 100%|██████████▉| 99999986/100000000 [01:12<00:00, 1370671.22tokens/s]\r\n",
      "Shard 38: 100%|██████████▉| 99999506/100000000 [01:12<00:00, 1387493.77tokens/s]\r\n",
      "Shard 39: 100%|██████████▉| 99998491/100000000 [01:12<00:00, 1374800.14tokens/s]\r\n",
      "Shard 40: 100%|██████████▉| 99999686/100000000 [01:13<00:00, 1367834.79tokens/s]\r\n",
      "Shard 41: 100%|██████████▉| 99999210/100000000 [01:12<00:00, 1383646.98tokens/s]\r\n",
      "Shard 42: 100%|██████████▉| 99982614/100000000 [01:12<00:00, 1382653.34tokens/s]\r\n",
      "Shard 43: 100%|██████████▉| 99999919/100000000 [01:12<00:00, 1372105.59tokens/s]\r\n",
      "Shard 44: 100%|██████████▉| 99999958/100000000 [01:14<00:00, 1343598.30tokens/s]\r\n",
      "Shard 45: 100%|██████████▉| 99999884/100000000 [01:12<00:00, 1369933.38tokens/s]\r\n",
      "Shard 46: 100%|██████████▉| 99999708/100000000 [01:12<00:00, 1378988.66tokens/s]\r\n",
      "Shard 47: 100%|██████████▉| 99992492/100000000 [01:13<00:00, 1361640.96tokens/s]\r\n",
      "Shard 48: 100%|██████████▉| 99996591/100000000 [01:12<00:00, 1372201.48tokens/s]\r\n",
      "Shard 49: 100%|██████████▉| 99996813/100000000 [01:12<00:00, 1372594.79tokens/s]\r\n",
      "Shard 50: 100%|██████████▉| 99997449/100000000 [01:12<00:00, 1377803.54tokens/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python fineweb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc218e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:59:34.997668Z",
     "iopub.status.busy": "2024-07-07T16:59:34.997296Z",
     "iopub.status.idle": "2024-07-07T16:59:35.005949Z",
     "shell.execute_reply": "2024-07-07T16:59:35.005119Z"
    },
    "papermill": {
     "duration": 3.184858,
     "end_time": "2024-07-07T16:59:35.008096",
     "exception": false,
     "start_time": "2024-07-07T16:59:31.823238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_loader.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    \n",
    "    return ptt\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T, process_rank, num_processes, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.is_master = process_rank == 0\n",
    "        assert split in {'train', 'val'}, f\"Invalid split {split}, must be train or val\"\n",
    "        \n",
    "        # load tokens from disk and store them in memory\n",
    "        data_root = \"edu_fineweb10B\"\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        \n",
    "        assert len(shards) > 0, f\"no shards found for split: {split}\"\n",
    "        if self.is_master:\n",
    "            print(f\"found {len(shards)} shards for split: {split}\")\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds, advance to the next shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd151346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:59:41.356596Z",
     "iopub.status.busy": "2024-07-07T16:59:41.355986Z",
     "iopub.status.idle": "2024-07-07T16:59:41.369434Z",
     "shell.execute_reply": "2024-07-07T16:59:41.368526Z"
    },
    "papermill": {
     "duration": 3.201161,
     "end_time": "2024-07-07T16:59:41.371566",
     "exception": false,
     "start_time": "2024-07-07T16:59:38.170405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import inspect\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    path: str = \"gpt2\"\n",
    "    min_lr_factor: float = 0.1\n",
    "    warmup_steps: int = 10\n",
    "    max_lr: float = 6e-4\n",
    "    min_lr: float = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # TODO maybe make it so this is done only if min_lr is not defined\n",
    "        self.min_lr = self.max_lr * self.min_lr_factor\n",
    "        \n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to the next batch\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T, T) matrix for all the queries and keys)\n",
    "        \n",
    "#         att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "#         att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "#         att = F.softmax(att, dim=-1)\n",
    "#         y = att @ v # (B , nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output project\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd) # generally, this should be initialized too but pytorch does it correctly\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig, process_rank: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.is_master = process_rank == 0\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        # calculate loss if targets are defined\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained GPT: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head, and n_embd are determined from medel_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),    # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),   # 350M params \n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),   # 774M params \n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),   # 1558M params \n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257   # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024    # always 1024 for GPT model checkpoints\n",
    "        \n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard mask / buffer\n",
    "\n",
    "        # init a higgingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # also ignore\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically, the OpenAI checkpoints use a \"Conv1D\" module, but we only want to use a vanila\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_lr(self, it, max_steps): ## TODO maybe move\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < self.config.warmup_steps:\n",
    "            return self.config.max_lr * (it + 1) / self.config.warmup_steps\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > max_steps:\n",
    "            return self.config.min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - self.config.warmup_steps) / (max_steps - self.config.warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "        return self.config.min_lr + coeff * (self.config.max_lr - self.config.min_lr)\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all of the candidate parameters (that require gradients)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameter that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. weight tensors in matmuls + embeddings decay, all biases and layernorms don't\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused) # fused is slower on kaggle for some reason TODO figure out why\n",
    "        if self.is_master:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters in total\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters in total\")\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcdeef18",
   "metadata": {
    "_cell_guid": "35edd86f-4e81-4576-a17b-187e0c29f353",
    "_uuid": "26d76cd9-11aa-4fc3-94bb-724b5d0da166",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-07T16:59:47.678922Z",
     "iopub.status.busy": "2024-07-07T16:59:47.678562Z",
     "iopub.status.idle": "2024-07-07T16:59:47.688655Z",
     "shell.execute_reply": "2024-07-07T16:59:47.687752Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.160014,
     "end_time": "2024-07-07T16:59:47.690720",
     "exception": false,
     "start_time": "2024-07-07T16:59:44.530706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from data_loader import DataLoader\n",
    "from model import GPT, GPTConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ___________________________________Set up DDP_____________________________________\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), \"for now we need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla process\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process= True\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "\n",
    "print(f\"using device: {device}\")\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "    \n",
    "total_batch_size = 524288 # we see (roughly) half a shard per batch\n",
    "B = 8\n",
    "T = 1024\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f\"total desired batch size: {total_batch_size}\")\n",
    "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "train_loader = DataLoader(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
    "val_loader = DataLoader(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig(warmup_steps = 715), ddp_rank)\n",
    "model.to(device)\n",
    "model = torch.compile(model) # TODO explain this better ?\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    \n",
    "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "# max_steps = 9536 # We only load half of Fineweb-EDU due to drive constraints\n",
    "max_steps = 1000 # defo not enough but cant do more due to GPU compute constraints\n",
    "\n",
    "# optimize! the model\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), eps=1e-8) # TODO read about AdamW\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0, learning_rate=6e-4, device=device)\n",
    "\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
    "    pass\n",
    "\n",
    "for step in range(max_steps):\n",
    "    start = time.time()\n",
    "    last_step = (step == max_steps -1)\n",
    "    \n",
    "    if step % 250 == 0 or last_step: #abstract this into a property on the model config or other constant\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            \n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                with torch.autocast(device_type=device_type, dtype=torch.float16):\n",
    "                    logits, loss = model(x, y)\n",
    "                \n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "                \n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "                if step > 0 and (step % 5000 == 0 or last_step):\n",
    "                    # optionally write model checkpoints\n",
    "                    checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
    "                    checkpoint = {\n",
    "                        'model': raw_model.state_dict(),\n",
    "                        'config': raw_model.config,\n",
    "                        'step': step,\n",
    "                        'val_loss': val_loss_accum.item()\n",
    "                    }\n",
    "                    # you might also want to add optimizer.state_dict() and\n",
    "                    # rng seeds etc., if you want to resume training\n",
    "                    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    \n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1) # switch to the DDP context manager\n",
    "            \n",
    "        with torch.autocast(device_type=device_type, dtype=torch.float16): \n",
    "            # TODO look into how to get a gradient scaler working\n",
    "            logits, loss = model(x, y)\n",
    "        \n",
    "        # we have to scale the loss to account for gradient accumulation\n",
    "        # because the gradients just add on each successive backward()\n",
    "        # addition of gradients corresponds to a SUM in the objective, but\n",
    "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "        \n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = model.module.get_lr(step, max_steps) # try out the scheduler provided by pytorch (if there are any)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    \n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    miliseconds = total_time*1000\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / total_time\n",
    "    \n",
    "    if master_process:\n",
    "        log_str = f\"step: {step:5d} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | time: {miliseconds:.2f}ms | tok/sec: {tokens_per_sec:.2f}\"\n",
    "        print(log_str)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{log_str}\\n\")\n",
    "    \n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ffa23c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:59:54.069113Z",
     "iopub.status.busy": "2024-07-07T16:59:54.068639Z",
     "iopub.status.idle": "2024-07-07T20:34:42.900677Z",
     "shell.execute_reply": "2024-07-07T20:34:42.899472Z"
    },
    "papermill": {
     "duration": 12892.008283,
     "end_time": "2024-07-07T20:34:42.903263",
     "exception": false,
     "start_time": "2024-07-07T16:59:50.894980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0707 16:59:59.346000 134020790073152 torch/distributed/run.py:757] \r\n",
      "W0707 16:59:59.346000 134020790073152 torch/distributed/run.py:757] *****************************************\r\n",
      "W0707 16:59:59.346000 134020790073152 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0707 16:59:59.346000 134020790073152 torch/distributed/run.py:757] *****************************************\r\n",
      "using device: cuda:0\r\n",
      "total desired batch size: 524288\r\n",
      "=> calculated gradient accumulation steps: 32\r\n",
      "found 50 shards for split: train\r\n",
      "using device: cuda:1\r\n",
      "found 1 shards for split: val\r\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters in total\r\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters in total\r\n",
      "using fused AdamW: True\r\n",
      "validation loss: 10.9499\r\n",
      "step:     0 | loss: 10.954966 | lr: 8.3916e-07 | norm: 11.5024 | time: 102004.17ms | tok/sec: 5139.87\r\n",
      "step:     1 | loss: 10.914789 | lr: 1.6783e-06 | norm: 11.2425 | time: 12231.13ms | tok/sec: 42865.04\r\n",
      "step:     2 | loss: 10.839107 | lr: 2.5175e-06 | norm: 11.2906 | time: 12381.78ms | tok/sec: 42343.51\r\n",
      "step:     3 | loss: 10.724237 | lr: 3.3566e-06 | norm: 10.9706 | time: 12542.40ms | tok/sec: 41801.24\r\n",
      "step:     4 | loss: 10.597439 | lr: 4.1958e-06 | norm: 10.0052 | time: 12730.09ms | tok/sec: 41184.93\r\n",
      "step:     5 | loss: 10.463644 | lr: 5.0350e-06 | norm: 9.0438 | time: 12889.96ms | tok/sec: 40674.12\r\n",
      "step:     6 | loss: 10.341341 | lr: 5.8741e-06 | norm: 7.9328 | time: 13137.85ms | tok/sec: 39906.69\r\n",
      "step:     7 | loss: 10.229029 | lr: 6.7133e-06 | norm: 6.8499 | time: 13362.06ms | tok/sec: 39237.07\r\n",
      "step:     8 | loss: 10.121150 | lr: 7.5524e-06 | norm: 6.1515 | time: 13540.19ms | tok/sec: 38720.88\r\n",
      "step:     9 | loss: 10.047916 | lr: 8.3916e-06 | norm: 5.3950 | time: 13472.28ms | tok/sec: 38916.06\r\n",
      "step:    10 | loss: 9.964489 | lr: 9.2308e-06 | norm: 4.8106 | time: 13287.05ms | tok/sec: 39458.56\r\n",
      "step:    11 | loss: 9.914412 | lr: 1.0070e-05 | norm: 4.2891 | time: 13196.34ms | tok/sec: 39729.80\r\n",
      "step:    12 | loss: 9.868090 | lr: 1.0909e-05 | norm: 3.8355 | time: 13201.78ms | tok/sec: 39713.43\r\n",
      "step:    13 | loss: 9.832897 | lr: 1.1748e-05 | norm: 3.3929 | time: 13233.71ms | tok/sec: 39617.62\r\n",
      "step:    14 | loss: 9.777735 | lr: 1.2587e-05 | norm: 3.1241 | time: 13248.25ms | tok/sec: 39574.13\r\n",
      "step:    15 | loss: 9.759793 | lr: 1.3427e-05 | norm: 2.9099 | time: 13272.38ms | tok/sec: 39502.19\r\n",
      "step:    16 | loss: 9.748272 | lr: 1.4266e-05 | norm: 2.7387 | time: 13251.46ms | tok/sec: 39564.56\r\n",
      "step:    17 | loss: 9.732422 | lr: 1.5105e-05 | norm: 2.6829 | time: 13232.02ms | tok/sec: 39622.66\r\n",
      "step:    18 | loss: 9.785121 | lr: 1.5944e-05 | norm: 2.6221 | time: 13205.90ms | tok/sec: 39701.05\r\n",
      "step:    19 | loss: 9.787571 | lr: 1.6783e-05 | norm: 2.5960 | time: 13181.48ms | tok/sec: 39774.59\r\n",
      "step:    20 | loss: 9.707809 | lr: 1.7622e-05 | norm: 2.4666 | time: 13164.44ms | tok/sec: 39826.08\r\n",
      "step:    21 | loss: 9.689201 | lr: 1.8462e-05 | norm: 2.5071 | time: 13180.48ms | tok/sec: 39777.62\r\n",
      "step:    22 | loss: 9.693264 | lr: 1.9301e-05 | norm: 2.4703 | time: 13182.88ms | tok/sec: 39770.37\r\n",
      "step:    23 | loss: 9.701868 | lr: 2.0140e-05 | norm: 2.5700 | time: 13145.28ms | tok/sec: 39884.12\r\n",
      "step:    24 | loss: 9.668468 | lr: 2.0979e-05 | norm: 2.7346 | time: 13157.09ms | tok/sec: 39848.31\r\n",
      "step:    25 | loss: 9.671922 | lr: 2.1818e-05 | norm: 2.7783 | time: 13151.11ms | tok/sec: 39866.45\r\n",
      "step:    26 | loss: 9.677711 | lr: 2.2657e-05 | norm: 2.7409 | time: 13148.22ms | tok/sec: 39875.19\r\n",
      "step:    27 | loss: 9.667549 | lr: 2.3497e-05 | norm: 2.6554 | time: 13125.57ms | tok/sec: 39944.03\r\n",
      "step:    28 | loss: 9.669088 | lr: 2.4336e-05 | norm: 2.6127 | time: 13098.45ms | tok/sec: 40026.74\r\n",
      "step:    29 | loss: 9.660765 | lr: 2.5175e-05 | norm: 2.6380 | time: 13068.42ms | tok/sec: 40118.71\r\n",
      "step:    30 | loss: 9.698627 | lr: 2.6014e-05 | norm: 2.5478 | time: 13058.87ms | tok/sec: 40148.05\r\n",
      "step:    31 | loss: 9.665654 | lr: 2.6853e-05 | norm: 2.5384 | time: 13034.69ms | tok/sec: 40222.51\r\n",
      "step:    32 | loss: 9.668077 | lr: 2.7692e-05 | norm: 2.4944 | time: 13031.25ms | tok/sec: 40233.12\r\n",
      "step:    33 | loss: 9.655342 | lr: 2.8531e-05 | norm: 2.5831 | time: 13017.53ms | tok/sec: 40275.52\r\n",
      "step:    34 | loss: 9.600391 | lr: 2.9371e-05 | norm: 2.9079 | time: 13026.13ms | tok/sec: 40248.94\r\n",
      "step:    35 | loss: 9.635698 | lr: 3.0210e-05 | norm: 3.0559 | time: 13023.77ms | tok/sec: 40256.25\r\n",
      "step:    36 | loss: 9.640217 | lr: 3.1049e-05 | norm: 3.0934 | time: 13053.65ms | tok/sec: 40164.09\r\n",
      "step:    37 | loss: 9.610008 | lr: 3.1888e-05 | norm: 3.0059 | time: 13056.93ms | tok/sec: 40154.01\r\n",
      "step:    38 | loss: 9.609441 | lr: 3.2727e-05 | norm: 2.8482 | time: 13053.91ms | tok/sec: 40163.28\r\n",
      "step:    39 | loss: 9.625265 | lr: 3.3566e-05 | norm: 2.6172 | time: 13056.12ms | tok/sec: 40156.50\r\n",
      "step:    40 | loss: 9.637423 | lr: 3.4406e-05 | norm: 2.5161 | time: 13069.49ms | tok/sec: 40115.40\r\n",
      "step:    41 | loss: 9.599441 | lr: 3.5245e-05 | norm: 2.7684 | time: 13063.74ms | tok/sec: 40133.08\r\n",
      "step:    42 | loss: 9.604374 | lr: 3.6084e-05 | norm: 2.7949 | time: 13072.71ms | tok/sec: 40105.53\r\n",
      "step:    43 | loss: 9.602610 | lr: 3.6923e-05 | norm: 2.8446 | time: 13062.72ms | tok/sec: 40136.22\r\n",
      "step:    44 | loss: 9.601161 | lr: 3.7762e-05 | norm: 2.7793 | time: 13065.18ms | tok/sec: 40128.66\r\n",
      "step:    45 | loss: 9.596151 | lr: 3.8601e-05 | norm: 2.7266 | time: 13064.38ms | tok/sec: 40131.09\r\n",
      "step:    46 | loss: 9.562472 | lr: 3.9441e-05 | norm: 2.6848 | time: 13062.03ms | tok/sec: 40138.34\r\n",
      "step:    47 | loss: 9.546063 | lr: 4.0280e-05 | norm: 2.5647 | time: 13056.11ms | tok/sec: 40156.52\r\n",
      "step:    48 | loss: 9.539248 | lr: 4.1119e-05 | norm: 2.4219 | time: 13051.64ms | tok/sec: 40170.29\r\n",
      "step:    49 | loss: 9.494709 | lr: 4.1958e-05 | norm: 2.4070 | time: 13046.79ms | tok/sec: 40185.21\r\n",
      "step:    50 | loss: 9.510490 | lr: 4.2797e-05 | norm: 2.3849 | time: 13056.74ms | tok/sec: 40154.59\r\n",
      "step:    51 | loss: 9.484267 | lr: 4.3636e-05 | norm: 2.4333 | time: 13070.74ms | tok/sec: 40111.58\r\n",
      "step:    52 | loss: 9.458845 | lr: 4.4476e-05 | norm: 2.3940 | time: 13050.32ms | tok/sec: 40174.34\r\n",
      "step:    53 | loss: 9.441285 | lr: 4.5315e-05 | norm: 2.2959 | time: 13078.04ms | tok/sec: 40089.18\r\n",
      "step:    54 | loss: 9.453382 | lr: 4.6154e-05 | norm: 2.2020 | time: 13070.42ms | tok/sec: 40112.56\r\n",
      "step:    55 | loss: 9.432543 | lr: 4.6993e-05 | norm: 2.2400 | time: 13057.30ms | tok/sec: 40152.85\r\n",
      "step:    56 | loss: 9.426391 | lr: 4.7832e-05 | norm: 2.2490 | time: 13062.18ms | tok/sec: 40137.87\r\n",
      "step:    57 | loss: 9.424367 | lr: 4.8671e-05 | norm: 2.2504 | time: 13059.75ms | tok/sec: 40145.35\r\n",
      "step:    58 | loss: 9.409922 | lr: 4.9510e-05 | norm: 2.2602 | time: 13052.27ms | tok/sec: 40168.33\r\n",
      "step:    59 | loss: 9.402095 | lr: 5.0350e-05 | norm: 2.2381 | time: 13052.82ms | tok/sec: 40166.64\r\n",
      "step:    60 | loss: 9.398891 | lr: 5.1189e-05 | norm: 2.2583 | time: 13058.70ms | tok/sec: 40148.57\r\n",
      "step:    61 | loss: 9.403846 | lr: 5.2028e-05 | norm: 2.2652 | time: 13073.88ms | tok/sec: 40101.93\r\n",
      "step:    62 | loss: 9.352718 | lr: 5.2867e-05 | norm: 2.2823 | time: 13070.00ms | tok/sec: 40113.85\r\n",
      "step:    63 | loss: 9.334201 | lr: 5.3706e-05 | norm: 2.1925 | time: 13056.95ms | tok/sec: 40153.93\r\n",
      "step:    64 | loss: 9.312204 | lr: 5.4545e-05 | norm: 2.1481 | time: 13052.83ms | tok/sec: 40166.63\r\n",
      "step:    65 | loss: 9.305506 | lr: 5.5385e-05 | norm: 2.1464 | time: 13062.02ms | tok/sec: 40138.37\r\n",
      "step:    66 | loss: 9.285654 | lr: 5.6224e-05 | norm: 2.1320 | time: 13062.36ms | tok/sec: 40137.30\r\n",
      "step:    67 | loss: 9.282367 | lr: 5.7063e-05 | norm: 2.0957 | time: 13075.45ms | tok/sec: 40097.12\r\n",
      "step:    68 | loss: 9.274360 | lr: 5.7902e-05 | norm: 2.0763 | time: 13072.25ms | tok/sec: 40106.93\r\n",
      "step:    69 | loss: 9.235561 | lr: 5.8741e-05 | norm: 2.0901 | time: 13070.31ms | tok/sec: 40112.88\r\n",
      "step:    70 | loss: 9.213326 | lr: 5.9580e-05 | norm: 2.0921 | time: 13050.25ms | tok/sec: 40174.57\r\n",
      "step:    71 | loss: 9.231867 | lr: 6.0420e-05 | norm: 2.0817 | time: 13058.59ms | tok/sec: 40148.91\r\n",
      "step:    72 | loss: 9.184559 | lr: 6.1259e-05 | norm: 2.1127 | time: 13073.63ms | tok/sec: 40102.71\r\n",
      "step:    73 | loss: 9.153473 | lr: 6.2098e-05 | norm: 2.0883 | time: 13046.00ms | tok/sec: 40187.64\r\n",
      "step:    74 | loss: 9.143454 | lr: 6.2937e-05 | norm: 2.0609 | time: 13057.34ms | tok/sec: 40152.74\r\n",
      "step:    75 | loss: 9.141332 | lr: 6.3776e-05 | norm: 2.0614 | time: 13060.18ms | tok/sec: 40144.01\r\n",
      "step:    76 | loss: 9.112627 | lr: 6.4615e-05 | norm: 2.0411 | time: 13063.63ms | tok/sec: 40133.42\r\n",
      "step:    77 | loss: 9.122432 | lr: 6.5455e-05 | norm: 2.0298 | time: 13060.75ms | tok/sec: 40142.27\r\n",
      "step:    78 | loss: 9.091106 | lr: 6.6294e-05 | norm: 2.1072 | time: 13060.72ms | tok/sec: 40142.36\r\n",
      "step:    79 | loss: 9.067099 | lr: 6.7133e-05 | norm: 2.0839 | time: 13066.13ms | tok/sec: 40125.72\r\n",
      "step:    80 | loss: 9.027266 | lr: 6.7972e-05 | norm: 2.0350 | time: 13052.94ms | tok/sec: 40166.27\r\n",
      "step:    81 | loss: 9.024484 | lr: 6.8811e-05 | norm: 2.0352 | time: 13066.65ms | tok/sec: 40124.12\r\n",
      "step:    82 | loss: 8.997189 | lr: 6.9650e-05 | norm: 2.0726 | time: 13069.54ms | tok/sec: 40115.27\r\n",
      "step:    83 | loss: 8.957346 | lr: 7.0490e-05 | norm: 2.0897 | time: 13085.78ms | tok/sec: 40065.48\r\n",
      "step:    84 | loss: 8.968821 | lr: 7.1329e-05 | norm: 2.0719 | time: 13072.94ms | tok/sec: 40104.83\r\n",
      "step:    85 | loss: 8.938494 | lr: 7.2168e-05 | norm: 2.1247 | time: 13052.19ms | tok/sec: 40168.59\r\n",
      "step:    86 | loss: 8.930243 | lr: 7.3007e-05 | norm: 2.0858 | time: 13078.07ms | tok/sec: 40089.11\r\n",
      "step:    87 | loss: 8.914961 | lr: 7.3846e-05 | norm: 2.0650 | time: 13071.78ms | tok/sec: 40108.37\r\n",
      "step:    88 | loss: 8.905031 | lr: 7.4685e-05 | norm: 2.0388 | time: 13084.51ms | tok/sec: 40069.36\r\n",
      "step:    89 | loss: 8.927563 | lr: 7.5524e-05 | norm: 2.0043 | time: 13097.56ms | tok/sec: 40029.43\r\n",
      "step:    90 | loss: 8.867359 | lr: 7.6364e-05 | norm: 2.0384 | time: 13092.89ms | tok/sec: 40043.72\r\n",
      "step:    91 | loss: 8.856619 | lr: 7.7203e-05 | norm: 2.0492 | time: 13078.21ms | tok/sec: 40088.65\r\n",
      "step:    92 | loss: 8.864770 | lr: 7.8042e-05 | norm: 2.0678 | time: 13072.29ms | tok/sec: 40106.82\r\n",
      "step:    93 | loss: 8.894148 | lr: 7.8881e-05 | norm: 2.0588 | time: 13083.07ms | tok/sec: 40073.77\r\n",
      "step:    94 | loss: 8.890619 | lr: 7.9720e-05 | norm: 2.0457 | time: 13094.43ms | tok/sec: 40039.00\r\n",
      "step:    95 | loss: 8.882792 | lr: 8.0559e-05 | norm: 2.0073 | time: 13098.36ms | tok/sec: 40026.98\r\n",
      "step:    96 | loss: 8.852522 | lr: 8.1399e-05 | norm: 2.0393 | time: 13088.78ms | tok/sec: 40056.29\r\n",
      "step:    97 | loss: 8.796746 | lr: 8.2238e-05 | norm: 2.0943 | time: 13086.54ms | tok/sec: 40063.14\r\n",
      "step:    98 | loss: 8.802765 | lr: 8.3077e-05 | norm: 2.0882 | time: 13101.72ms | tok/sec: 40016.73\r\n",
      "step:    99 | loss: 8.772258 | lr: 8.3916e-05 | norm: 2.0855 | time: 13088.28ms | tok/sec: 40057.81\r\n",
      "step:   100 | loss: 8.815993 | lr: 8.4755e-05 | norm: 2.0743 | time: 13101.17ms | tok/sec: 40018.42\r\n",
      "step:   101 | loss: 8.775186 | lr: 8.5594e-05 | norm: 2.0948 | time: 13082.27ms | tok/sec: 40076.23\r\n",
      "step:   102 | loss: 8.754992 | lr: 8.6434e-05 | norm: 2.1464 | time: 13091.07ms | tok/sec: 40049.28\r\n",
      "step:   103 | loss: 8.687366 | lr: 8.7273e-05 | norm: 2.1548 | time: 13062.12ms | tok/sec: 40138.05\r\n",
      "step:   104 | loss: 8.739408 | lr: 8.8112e-05 | norm: 2.0820 | time: 13078.08ms | tok/sec: 40089.06\r\n",
      "step:   105 | loss: 8.743885 | lr: 8.8951e-05 | norm: 2.0809 | time: 13090.60ms | tok/sec: 40050.73\r\n",
      "step:   106 | loss: 8.678052 | lr: 8.9790e-05 | norm: 2.1589 | time: 13086.47ms | tok/sec: 40063.36\r\n",
      "step:   107 | loss: 8.676544 | lr: 9.0629e-05 | norm: 2.1135 | time: 13080.70ms | tok/sec: 40081.05\r\n",
      "step:   108 | loss: 8.705941 | lr: 9.1469e-05 | norm: 2.1940 | time: 13063.89ms | tok/sec: 40132.62\r\n",
      "step:   109 | loss: 8.735613 | lr: 9.2308e-05 | norm: 2.1449 | time: 13053.70ms | tok/sec: 40163.93\r\n",
      "step:   110 | loss: 8.664276 | lr: 9.3147e-05 | norm: 2.1857 | time: 13054.56ms | tok/sec: 40161.30\r\n",
      "step:   111 | loss: 8.649658 | lr: 9.3986e-05 | norm: 2.2096 | time: 13053.95ms | tok/sec: 40163.18\r\n",
      "step:   112 | loss: 8.618437 | lr: 9.4825e-05 | norm: 2.2303 | time: 13059.61ms | tok/sec: 40145.76\r\n",
      "step:   113 | loss: 8.653085 | lr: 9.5664e-05 | norm: 2.1881 | time: 13079.65ms | tok/sec: 40084.27\r\n",
      "step:   114 | loss: 8.628075 | lr: 9.6503e-05 | norm: 2.2074 | time: 13092.87ms | tok/sec: 40043.79\r\n",
      "step:   115 | loss: 8.621107 | lr: 9.7343e-05 | norm: 2.2535 | time: 13092.11ms | tok/sec: 40046.09\r\n",
      "step:   116 | loss: 8.561441 | lr: 9.8182e-05 | norm: 2.3078 | time: 13093.85ms | tok/sec: 40040.77\r\n",
      "step:   117 | loss: 8.499664 | lr: 9.9021e-05 | norm: 2.4426 | time: 13080.29ms | tok/sec: 40082.29\r\n",
      "step:   118 | loss: 8.530375 | lr: 9.9860e-05 | norm: 2.4751 | time: 13086.00ms | tok/sec: 40064.81\r\n",
      "step:   119 | loss: 8.551788 | lr: 1.0070e-04 | norm: 2.4390 | time: 13085.48ms | tok/sec: 40066.40\r\n",
      "step:   120 | loss: 8.528854 | lr: 1.0154e-04 | norm: 2.4244 | time: 13082.94ms | tok/sec: 40074.19\r\n",
      "step:   121 | loss: 8.510538 | lr: 1.0238e-04 | norm: 2.4826 | time: 13086.12ms | tok/sec: 40064.42\r\n",
      "step:   122 | loss: 8.489099 | lr: 1.0322e-04 | norm: 2.6308 | time: 13081.52ms | tok/sec: 40078.52\r\n",
      "step:   123 | loss: 8.492231 | lr: 1.0406e-04 | norm: 2.8349 | time: 13068.43ms | tok/sec: 40118.66\r\n",
      "step:   124 | loss: 8.521805 | lr: 1.0490e-04 | norm: 2.7089 | time: 13070.57ms | tok/sec: 40112.09\r\n",
      "step:   125 | loss: 8.507872 | lr: 1.0573e-04 | norm: 2.7756 | time: 13055.15ms | tok/sec: 40159.49\r\n",
      "step:   126 | loss: 8.511438 | lr: 1.0657e-04 | norm: 2.7609 | time: 13049.40ms | tok/sec: 40177.17\r\n",
      "step:   127 | loss: 8.478914 | lr: 1.0741e-04 | norm: 2.8372 | time: 13056.36ms | tok/sec: 40155.76\r\n",
      "step:   128 | loss: 8.451950 | lr: 1.0825e-04 | norm: 2.8980 | time: 13061.73ms | tok/sec: 40139.25\r\n",
      "step:   129 | loss: 8.439960 | lr: 1.0909e-04 | norm: 3.0813 | time: 13057.24ms | tok/sec: 40153.06\r\n",
      "step:   130 | loss: 8.464930 | lr: 1.0993e-04 | norm: 3.0665 | time: 13052.65ms | tok/sec: 40167.15\r\n",
      "step:   131 | loss: 8.442141 | lr: 1.1077e-04 | norm: 3.4435 | time: 13057.66ms | tok/sec: 40151.76\r\n",
      "step:   132 | loss: 8.435051 | lr: 1.1161e-04 | norm: 3.1722 | time: 13047.81ms | tok/sec: 40182.08\r\n",
      "step:   133 | loss: 8.407640 | lr: 1.1245e-04 | norm: 3.4064 | time: 13065.18ms | tok/sec: 40128.65\r\n",
      "step:   134 | loss: 8.426641 | lr: 1.1329e-04 | norm: 3.5411 | time: 13054.35ms | tok/sec: 40161.93\r\n",
      "step:   135 | loss: 8.482645 | lr: 1.1413e-04 | norm: 3.1501 | time: 13050.98ms | tok/sec: 40172.30\r\n",
      "step:   136 | loss: 8.455584 | lr: 1.1497e-04 | norm: 3.4804 | time: 13048.50ms | tok/sec: 40179.96\r\n",
      "step:   137 | loss: 8.444942 | lr: 1.1580e-04 | norm: 3.7732 | time: 13032.44ms | tok/sec: 40229.46\r\n",
      "step:   138 | loss: 8.424629 | lr: 1.1664e-04 | norm: 4.0033 | time: 13043.15ms | tok/sec: 40196.44\r\n",
      "step:   139 | loss: 8.458643 | lr: 1.1748e-04 | norm: 4.3318 | time: 13043.39ms | tok/sec: 40195.69\r\n",
      "step:   140 | loss: 8.520720 | lr: 1.1832e-04 | norm: 3.7371 | time: 13036.89ms | tok/sec: 40215.74\r\n",
      "step:   141 | loss: 8.465691 | lr: 1.1916e-04 | norm: 4.0175 | time: 13031.66ms | tok/sec: 40231.87\r\n",
      "step:   142 | loss: 8.508868 | lr: 1.2000e-04 | norm: 3.6806 | time: 13017.15ms | tok/sec: 40276.70\r\n",
      "step:   143 | loss: 8.467205 | lr: 1.2084e-04 | norm: 3.4485 | time: 13054.13ms | tok/sec: 40162.63\r\n",
      "step:   144 | loss: 8.449450 | lr: 1.2168e-04 | norm: 5.2667 | time: 13038.95ms | tok/sec: 40209.36\r\n",
      "step:   145 | loss: 8.481318 | lr: 1.2252e-04 | norm: 4.1968 | time: 13018.11ms | tok/sec: 40273.73\r\n",
      "step:   146 | loss: 8.466054 | lr: 1.2336e-04 | norm: 7.7694 | time: 13035.76ms | tok/sec: 40219.20\r\n",
      "step:   147 | loss: 8.451355 | lr: 1.2420e-04 | norm: 4.5592 | time: 13018.04ms | tok/sec: 40273.96\r\n",
      "step:   148 | loss: 8.454132 | lr: 1.2503e-04 | norm: 4.8326 | time: 13016.14ms | tok/sec: 40279.84\r\n",
      "step:   149 | loss: 8.492235 | lr: 1.2587e-04 | norm: 11.1678 | time: 13043.31ms | tok/sec: 40195.95\r\n",
      "step:   150 | loss: 8.481305 | lr: 1.2671e-04 | norm: 4.8710 | time: 12998.83ms | tok/sec: 40333.46\r\n",
      "step:   151 | loss: 8.453247 | lr: 1.2755e-04 | norm: 4.2771 | time: 13002.48ms | tok/sec: 40322.16\r\n",
      "step:   152 | loss: 8.467186 | lr: 1.2839e-04 | norm: 5.9567 | time: 12996.35ms | tok/sec: 40341.17\r\n",
      "step:   153 | loss: 8.529798 | lr: 1.2923e-04 | norm: 4.0686 | time: 12977.82ms | tok/sec: 40398.78\r\n",
      "step:   154 | loss: 8.467163 | lr: 1.3007e-04 | norm: 3.6668 | time: 12953.84ms | tok/sec: 40473.55\r\n",
      "step:   155 | loss: 8.464495 | lr: 1.3091e-04 | norm: 3.5257 | time: 12933.22ms | tok/sec: 40538.09\r\n",
      "step:   156 | loss: 8.545780 | lr: 1.3175e-04 | norm: 3.1997 | time: 12916.33ms | tok/sec: 40591.08\r\n",
      "step:   157 | loss: 8.549981 | lr: 1.3259e-04 | norm: 3.6470 | time: 12920.86ms | tok/sec: 40576.87\r\n",
      "step:   158 | loss: 8.547834 | lr: 1.3343e-04 | norm: 4.8690 | time: 12901.38ms | tok/sec: 40638.14\r\n",
      "step:   159 | loss: 8.525244 | lr: 1.3427e-04 | norm: 4.5165 | time: 12888.55ms | tok/sec: 40678.58\r\n",
      "step:   160 | loss: 8.538635 | lr: 1.3510e-04 | norm: 3.5429 | time: 12891.08ms | tok/sec: 40670.62\r\n",
      "step:   161 | loss: 8.571297 | lr: 1.3594e-04 | norm: 3.5730 | time: 12877.55ms | tok/sec: 40713.35\r\n",
      "step:   162 | loss: 8.527319 | lr: 1.3678e-04 | norm: 3.7190 | time: 12875.39ms | tok/sec: 40720.18\r\n",
      "step:   163 | loss: 8.511244 | lr: 1.3762e-04 | norm: 3.9697 | time: 12890.12ms | tok/sec: 40673.63\r\n",
      "step:   164 | loss: 8.512180 | lr: 1.3846e-04 | norm: 4.5232 | time: 12899.36ms | tok/sec: 40644.51\r\n",
      "step:   165 | loss: 8.538584 | lr: 1.3930e-04 | norm: 3.7321 | time: 12914.74ms | tok/sec: 40596.10\r\n",
      "step:   166 | loss: 8.432570 | lr: 1.4014e-04 | norm: 4.3790 | time: 12919.05ms | tok/sec: 40582.55\r\n",
      "step:   167 | loss: 8.454123 | lr: 1.4098e-04 | norm: 4.1103 | time: 12923.96ms | tok/sec: 40567.12\r\n",
      "step:   168 | loss: 8.425034 | lr: 1.4182e-04 | norm: 4.3466 | time: 12915.05ms | tok/sec: 40595.14\r\n",
      "step:   169 | loss: 8.446432 | lr: 1.4266e-04 | norm: 4.5785 | time: 12911.21ms | tok/sec: 40607.20\r\n",
      "step:   170 | loss: 8.424548 | lr: 1.4350e-04 | norm: 4.1400 | time: 12916.92ms | tok/sec: 40589.24\r\n",
      "step:   171 | loss: 8.444562 | lr: 1.4434e-04 | norm: 4.0782 | time: 12922.22ms | tok/sec: 40572.60\r\n",
      "step:   172 | loss: 8.449982 | lr: 1.4517e-04 | norm: 3.9543 | time: 12927.95ms | tok/sec: 40554.60\r\n",
      "step:   173 | loss: 8.431665 | lr: 1.4601e-04 | norm: 3.9939 | time: 12912.81ms | tok/sec: 40602.16\r\n",
      "step:   174 | loss: 8.443424 | lr: 1.4685e-04 | norm: 3.7440 | time: 12926.92ms | tok/sec: 40557.84\r\n",
      "step:   175 | loss: 8.488991 | lr: 1.4769e-04 | norm: 3.7896 | time: 12938.78ms | tok/sec: 40520.67\r\n",
      "step:   176 | loss: 8.408272 | lr: 1.4853e-04 | norm: 3.7177 | time: 12921.98ms | tok/sec: 40573.36\r\n",
      "step:   177 | loss: 8.440817 | lr: 1.4937e-04 | norm: 3.6805 | time: 12927.89ms | tok/sec: 40554.80\r\n",
      "step:   178 | loss: 8.391636 | lr: 1.5021e-04 | norm: 3.6832 | time: 12922.89ms | tok/sec: 40570.49\r\n",
      "step:   179 | loss: 8.370794 | lr: 1.5105e-04 | norm: 4.2563 | time: 12930.66ms | tok/sec: 40546.12\r\n",
      "step:   180 | loss: 8.363731 | lr: 1.5189e-04 | norm: 3.8401 | time: 12938.85ms | tok/sec: 40520.46\r\n",
      "step:   181 | loss: 8.395532 | lr: 1.5273e-04 | norm: 3.6444 | time: 12928.58ms | tok/sec: 40552.63\r\n",
      "step:   182 | loss: 8.379492 | lr: 1.5357e-04 | norm: 3.8842 | time: 12922.63ms | tok/sec: 40571.32\r\n",
      "step:   183 | loss: 8.357720 | lr: 1.5441e-04 | norm: 3.7784 | time: 12925.32ms | tok/sec: 40562.87\r\n",
      "step:   184 | loss: 8.373292 | lr: 1.5524e-04 | norm: 3.7982 | time: 12926.43ms | tok/sec: 40559.39\r\n",
      "step:   185 | loss: 8.351522 | lr: 1.5608e-04 | norm: 3.9711 | time: 12921.17ms | tok/sec: 40575.88\r\n",
      "step:   186 | loss: 8.376364 | lr: 1.5692e-04 | norm: 3.6988 | time: 12927.64ms | tok/sec: 40555.59\r\n",
      "step:   187 | loss: 8.431871 | lr: 1.5776e-04 | norm: 3.4310 | time: 12905.17ms | tok/sec: 40626.19\r\n",
      "step:   188 | loss: 8.387438 | lr: 1.5860e-04 | norm: 4.1455 | time: 12906.39ms | tok/sec: 40622.35\r\n",
      "step:   189 | loss: 8.361860 | lr: 1.5944e-04 | norm: 3.6434 | time: 12906.79ms | tok/sec: 40621.10\r\n",
      "step:   190 | loss: 8.399532 | lr: 1.6028e-04 | norm: 3.8933 | time: 17290.92ms | tok/sec: 30321.58\r\n",
      "step:   191 | loss: 8.347370 | lr: 1.6112e-04 | norm: 4.0928 | time: 12909.04ms | tok/sec: 40614.01\r\n",
      "step:   192 | loss: 8.357305 | lr: 1.6196e-04 | norm: 3.7169 | time: 13065.68ms | tok/sec: 40127.11\r\n",
      "step:   193 | loss: 8.393102 | lr: 1.6280e-04 | norm: 4.3157 | time: 13022.78ms | tok/sec: 40259.29\r\n",
      "step:   194 | loss: 8.308777 | lr: 1.6364e-04 | norm: 4.4095 | time: 12930.05ms | tok/sec: 40548.03\r\n",
      "step:   195 | loss: 8.326389 | lr: 1.6448e-04 | norm: 4.0808 | time: 12864.20ms | tok/sec: 40755.60\r\n",
      "step:   196 | loss: 8.339224 | lr: 1.6531e-04 | norm: 3.8581 | time: 12860.11ms | tok/sec: 40768.54\r\n",
      "step:   197 | loss: 8.353263 | lr: 1.6615e-04 | norm: 3.8113 | time: 12885.20ms | tok/sec: 40689.16\r\n",
      "step:   198 | loss: 8.379818 | lr: 1.6699e-04 | norm: 3.8341 | time: 12894.06ms | tok/sec: 40661.21\r\n",
      "step:   199 | loss: 8.356407 | lr: 1.6783e-04 | norm: 4.7058 | time: 12904.47ms | tok/sec: 40628.40\r\n",
      "step:   200 | loss: 8.333344 | lr: 1.6867e-04 | norm: 4.7104 | time: 12896.26ms | tok/sec: 40654.26\r\n",
      "step:   201 | loss: 8.329489 | lr: 1.6951e-04 | norm: 4.7925 | time: 12904.65ms | tok/sec: 40627.83\r\n",
      "step:   202 | loss: 8.330706 | lr: 1.7035e-04 | norm: 3.9295 | time: 12907.08ms | tok/sec: 40620.20\r\n",
      "step:   203 | loss: 8.373077 | lr: 1.7119e-04 | norm: 3.6062 | time: 12908.18ms | tok/sec: 40616.74\r\n",
      "step:   204 | loss: 8.333775 | lr: 1.7203e-04 | norm: 4.0504 | time: 12918.77ms | tok/sec: 40583.42\r\n",
      "step:   205 | loss: 8.474312 | lr: 1.7287e-04 | norm: 4.3042 | time: 12908.25ms | tok/sec: 40616.50\r\n",
      "step:   206 | loss: 8.434701 | lr: 1.7371e-04 | norm: 3.9357 | time: 12906.19ms | tok/sec: 40622.97\r\n",
      "step:   207 | loss: 8.355343 | lr: 1.7455e-04 | norm: 4.3677 | time: 12919.44ms | tok/sec: 40581.33\r\n",
      "step:   208 | loss: 8.327614 | lr: 1.7538e-04 | norm: 4.5998 | time: 12919.42ms | tok/sec: 40581.39\r\n",
      "step:   209 | loss: 8.312449 | lr: 1.7622e-04 | norm: 4.7938 | time: 12919.68ms | tok/sec: 40580.57\r\n",
      "step:   210 | loss: 8.308937 | lr: 1.7706e-04 | norm: 4.5371 | time: 12915.32ms | tok/sec: 40594.28\r\n",
      "step:   211 | loss: 8.326460 | lr: 1.7790e-04 | norm: 4.7493 | time: 12911.07ms | tok/sec: 40607.63\r\n",
      "step:   212 | loss: 8.380920 | lr: 1.7874e-04 | norm: 4.5345 | time: 12920.43ms | tok/sec: 40578.22\r\n",
      "step:   213 | loss: 8.371999 | lr: 1.7958e-04 | norm: 4.8813 | time: 12928.11ms | tok/sec: 40554.10\r\n",
      "step:   214 | loss: 8.356337 | lr: 1.8042e-04 | norm: 4.6992 | time: 12930.38ms | tok/sec: 40546.99\r\n",
      "step:   215 | loss: 8.410470 | lr: 1.8126e-04 | norm: 4.6321 | time: 12926.00ms | tok/sec: 40560.74\r\n",
      "step:   216 | loss: 8.327526 | lr: 1.8210e-04 | norm: 4.5699 | time: 12927.49ms | tok/sec: 40556.05\r\n",
      "step:   217 | loss: 8.308540 | lr: 1.8294e-04 | norm: 4.3858 | time: 12929.50ms | tok/sec: 40549.75\r\n",
      "step:   218 | loss: 8.307432 | lr: 1.8378e-04 | norm: 4.8571 | time: 12935.35ms | tok/sec: 40531.40\r\n",
      "step:   219 | loss: 8.307093 | lr: 1.8462e-04 | norm: 5.0106 | time: 12928.78ms | tok/sec: 40552.01\r\n",
      "step:   220 | loss: 8.319242 | lr: 1.8545e-04 | norm: 6.0139 | time: 12926.97ms | tok/sec: 40557.67\r\n",
      "step:   221 | loss: 8.304915 | lr: 1.8629e-04 | norm: 5.6430 | time: 12903.11ms | tok/sec: 40632.69\r\n",
      "step:   222 | loss: 8.351576 | lr: 1.8713e-04 | norm: 8.9086 | time: 12905.86ms | tok/sec: 40624.03\r\n",
      "step:   223 | loss: 8.304676 | lr: 1.8797e-04 | norm: 7.9593 | time: 12907.47ms | tok/sec: 40618.95\r\n",
      "step:   224 | loss: 8.294340 | lr: 1.8881e-04 | norm: 11.1037 | time: 12909.04ms | tok/sec: 40614.03\r\n",
      "step:   225 | loss: 8.289907 | lr: 1.8965e-04 | norm: 10.8150 | time: 12910.62ms | tok/sec: 40609.06\r\n",
      "step:   226 | loss: 8.302242 | lr: 1.9049e-04 | norm: 14.0456 | time: 12932.23ms | tok/sec: 40541.18\r\n",
      "step:   227 | loss: 8.364911 | lr: 1.9133e-04 | norm: 12.2830 | time: 12916.17ms | tok/sec: 40591.60\r\n",
      "step:   228 | loss: 8.324600 | lr: 1.9217e-04 | norm: 12.9896 | time: 12924.08ms | tok/sec: 40566.76\r\n",
      "step:   229 | loss: 8.298326 | lr: 1.9301e-04 | norm: 10.4122 | time: 12926.82ms | tok/sec: 40558.16\r\n",
      "step:   230 | loss: 8.334930 | lr: 1.9385e-04 | norm: 12.7213 | time: 12922.70ms | tok/sec: 40571.08\r\n",
      "step:   231 | loss: 8.319824 | lr: 1.9469e-04 | norm: 16.5094 | time: 12953.14ms | tok/sec: 40475.75\r\n",
      "step:   232 | loss: 8.439005 | lr: 1.9552e-04 | norm: 11.9389 | time: 12922.54ms | tok/sec: 40571.60\r\n",
      "step:   233 | loss: 8.413525 | lr: 1.9636e-04 | norm: 8.6501 | time: 12915.94ms | tok/sec: 40592.33\r\n",
      "step:   234 | loss: 8.424589 | lr: 1.9720e-04 | norm: 13.0709 | time: 12923.36ms | tok/sec: 40569.03\r\n",
      "step:   235 | loss: 8.442844 | lr: 1.9804e-04 | norm: 9.3511 | time: 12921.99ms | tok/sec: 40573.32\r\n",
      "step:   236 | loss: 8.479603 | lr: 1.9888e-04 | norm: 9.1894 | time: 12919.66ms | tok/sec: 40580.64\r\n",
      "step:   237 | loss: 8.481844 | lr: 1.9972e-04 | norm: 7.0993 | time: 12902.80ms | tok/sec: 40633.66\r\n",
      "step:   238 | loss: 8.405508 | lr: 2.0056e-04 | norm: 7.5178 | time: 12892.12ms | tok/sec: 40667.31\r\n",
      "step:   239 | loss: 8.410343 | lr: 2.0140e-04 | norm: 7.0557 | time: 12891.99ms | tok/sec: 40667.73\r\n",
      "step:   240 | loss: 8.460703 | lr: 2.0224e-04 | norm: 6.4324 | time: 12886.70ms | tok/sec: 40684.43\r\n",
      "step:   241 | loss: 8.472891 | lr: 2.0308e-04 | norm: 4.6072 | time: 12868.06ms | tok/sec: 40743.38\r\n",
      "step:   242 | loss: 8.428474 | lr: 2.0392e-04 | norm: 3.6283 | time: 12847.62ms | tok/sec: 40808.18\r\n",
      "step:   243 | loss: 8.504087 | lr: 2.0476e-04 | norm: 3.9674 | time: 12850.84ms | tok/sec: 40797.96\r\n",
      "step:   244 | loss: 8.407271 | lr: 2.0559e-04 | norm: 3.6976 | time: 12843.79ms | tok/sec: 40820.35\r\n",
      "step:   245 | loss: 8.408491 | lr: 2.0643e-04 | norm: 3.5066 | time: 12840.94ms | tok/sec: 40829.41\r\n",
      "step:   246 | loss: 8.464767 | lr: 2.0727e-04 | norm: 3.1623 | time: 12840.26ms | tok/sec: 40831.59\r\n",
      "step:   247 | loss: 8.498568 | lr: 2.0811e-04 | norm: 2.9990 | time: 12833.74ms | tok/sec: 40852.31\r\n",
      "step:   248 | loss: 8.500125 | lr: 2.0895e-04 | norm: 2.9743 | time: 12835.83ms | tok/sec: 40845.65\r\n",
      "step:   249 | loss: 8.480202 | lr: 2.0979e-04 | norm: 3.2039 | time: 12831.32ms | tok/sec: 40860.02\r\n",
      "validation loss: 8.5084\r\n",
      "step:   250 | loss: 8.500988 | lr: 2.1063e-04 | norm: 3.3714 | time: 18591.56ms | tok/sec: 28200.32\r\n",
      "step:   251 | loss: 8.455712 | lr: 2.1147e-04 | norm: 4.2350 | time: 12907.58ms | tok/sec: 40618.60\r\n",
      "step:   252 | loss: 8.585610 | lr: 2.1231e-04 | norm: 5.2088 | time: 12864.55ms | tok/sec: 40754.47\r\n",
      "step:   253 | loss: 8.495494 | lr: 2.1315e-04 | norm: 5.2065 | time: 12832.25ms | tok/sec: 40857.07\r\n",
      "step:   254 | loss: 8.517105 | lr: 2.1399e-04 | norm: 6.3739 | time: 12816.80ms | tok/sec: 40906.32\r\n",
      "step:   255 | loss: 8.481586 | lr: 2.1483e-04 | norm: 6.6387 | time: 12819.63ms | tok/sec: 40897.27\r\n",
      "step:   256 | loss: 8.530833 | lr: 2.1566e-04 | norm: 5.4326 | time: 12830.16ms | tok/sec: 40863.71\r\n",
      "step:   257 | loss: 8.502426 | lr: 2.1650e-04 | norm: 4.6724 | time: 12849.76ms | tok/sec: 40801.38\r\n",
      "step:   258 | loss: 8.455372 | lr: 2.1734e-04 | norm: 4.7278 | time: 12865.32ms | tok/sec: 40752.05\r\n",
      "step:   259 | loss: 8.471933 | lr: 2.1818e-04 | norm: 4.4529 | time: 12879.48ms | tok/sec: 40707.23\r\n",
      "step:   260 | loss: 8.394032 | lr: 2.1902e-04 | norm: 4.3189 | time: 12875.62ms | tok/sec: 40719.43\r\n",
      "step:   261 | loss: 8.495903 | lr: 2.1986e-04 | norm: 4.4259 | time: 12875.12ms | tok/sec: 40721.01\r\n",
      "step:   262 | loss: 8.448730 | lr: 2.2070e-04 | norm: 3.7445 | time: 12864.73ms | tok/sec: 40753.92\r\n",
      "step:   263 | loss: 8.453335 | lr: 2.2154e-04 | norm: 3.6952 | time: 12864.87ms | tok/sec: 40753.47\r\n",
      "step:   264 | loss: 8.493956 | lr: 2.2238e-04 | norm: 3.7487 | time: 12853.35ms | tok/sec: 40789.99\r\n",
      "step:   265 | loss: 8.569512 | lr: 2.2322e-04 | norm: 3.2895 | time: 12841.10ms | tok/sec: 40828.91\r\n",
      "step:   266 | loss: 8.599548 | lr: 2.2406e-04 | norm: 3.2194 | time: 12825.19ms | tok/sec: 40879.55\r\n",
      "step:   267 | loss: 8.503793 | lr: 2.2490e-04 | norm: 3.1895 | time: 12806.66ms | tok/sec: 40938.70\r\n",
      "step:   268 | loss: 8.553268 | lr: 2.2573e-04 | norm: 3.0071 | time: 12795.15ms | tok/sec: 40975.53\r\n",
      "step:   269 | loss: 8.578371 | lr: 2.2657e-04 | norm: 2.8650 | time: 12778.81ms | tok/sec: 41027.92\r\n",
      "step:   270 | loss: 8.608366 | lr: 2.2741e-04 | norm: 2.8222 | time: 12790.66ms | tok/sec: 40989.92\r\n",
      "step:   271 | loss: 8.611374 | lr: 2.2825e-04 | norm: 2.8259 | time: 12787.18ms | tok/sec: 41001.07\r\n",
      "step:   272 | loss: 8.537418 | lr: 2.2909e-04 | norm: 2.9251 | time: 12782.06ms | tok/sec: 41017.49\r\n",
      "step:   273 | loss: 8.489135 | lr: 2.2993e-04 | norm: 3.1701 | time: 12795.25ms | tok/sec: 40975.21\r\n",
      "step:   274 | loss: 8.535707 | lr: 2.3077e-04 | norm: 3.3678 | time: 12806.31ms | tok/sec: 40939.83\r\n",
      "step:   275 | loss: 8.586470 | lr: 2.3161e-04 | norm: 3.5217 | time: 12807.56ms | tok/sec: 40935.81\r\n",
      "step:   276 | loss: 8.848104 | lr: 2.3245e-04 | norm: 3.5487 | time: 12810.07ms | tok/sec: 40927.82\r\n",
      "step:   277 | loss: 8.545326 | lr: 2.3329e-04 | norm: 3.8434 | time: 12813.53ms | tok/sec: 40916.76\r\n",
      "step:   278 | loss: 8.644419 | lr: 2.3413e-04 | norm: 3.5621 | time: 12812.94ms | tok/sec: 40918.64\r\n",
      "step:   279 | loss: 8.631283 | lr: 2.3497e-04 | norm: 3.2200 | time: 12815.30ms | tok/sec: 40911.10\r\n",
      "step:   280 | loss: 8.716410 | lr: 2.3580e-04 | norm: 3.4732 | time: 12826.71ms | tok/sec: 40874.72\r\n",
      "step:   281 | loss: 8.692122 | lr: 2.3664e-04 | norm: 3.5615 | time: 12825.69ms | tok/sec: 40877.97\r\n",
      "step:   282 | loss: 8.690975 | lr: 2.3748e-04 | norm: 3.1544 | time: 12827.91ms | tok/sec: 40870.89\r\n",
      "step:   283 | loss: 8.711460 | lr: 2.3832e-04 | norm: 3.0167 | time: 12822.03ms | tok/sec: 40889.63\r\n",
      "step:   284 | loss: 8.791479 | lr: 2.3916e-04 | norm: 3.1187 | time: 12834.13ms | tok/sec: 40851.07\r\n",
      "step:   285 | loss: 8.801490 | lr: 2.4000e-04 | norm: 3.3437 | time: 12826.93ms | tok/sec: 40874.01\r\n",
      "step:   286 | loss: 8.795521 | lr: 2.4084e-04 | norm: 3.4398 | time: 12834.12ms | tok/sec: 40851.11\r\n",
      "step:   287 | loss: 8.710621 | lr: 2.4168e-04 | norm: 3.6366 | time: 12828.15ms | tok/sec: 40870.13\r\n",
      "step:   288 | loss: 8.708747 | lr: 2.4252e-04 | norm: 3.4564 | time: 12820.96ms | tok/sec: 40893.04\r\n",
      "step:   289 | loss: 8.821732 | lr: 2.4336e-04 | norm: 3.3945 | time: 12817.56ms | tok/sec: 40903.88\r\n",
      "step:   290 | loss: 8.718708 | lr: 2.4420e-04 | norm: 3.3907 | time: 12823.45ms | tok/sec: 40885.09\r\n",
      "step:   291 | loss: 8.769402 | lr: 2.4503e-04 | norm: 3.1235 | time: 12823.22ms | tok/sec: 40885.82\r\n",
      "step:   292 | loss: 8.663351 | lr: 2.4587e-04 | norm: 3.2137 | time: 12814.32ms | tok/sec: 40914.24\r\n",
      "step:   293 | loss: 8.732581 | lr: 2.4671e-04 | norm: 3.5252 | time: 12786.85ms | tok/sec: 41002.14\r\n",
      "step:   294 | loss: 8.911242 | lr: 2.4755e-04 | norm: 3.2530 | time: 12772.49ms | tok/sec: 41048.21\r\n",
      "step:   295 | loss: 8.864978 | lr: 2.4839e-04 | norm: 3.3154 | time: 12778.42ms | tok/sec: 41029.16\r\n",
      "step:   296 | loss: 8.861788 | lr: 2.4923e-04 | norm: 3.5887 | time: 12783.97ms | tok/sec: 41011.35\r\n",
      "step:   297 | loss: 8.899269 | lr: 2.5007e-04 | norm: 3.8295 | time: 12782.60ms | tok/sec: 41015.75\r\n",
      "step:   298 | loss: 8.818550 | lr: 2.5091e-04 | norm: 3.5986 | time: 12802.57ms | tok/sec: 40951.78\r\n",
      "step:   299 | loss: 8.910093 | lr: 2.5175e-04 | norm: 4.0652 | time: 12806.77ms | tok/sec: 40938.36\r\n",
      "step:   300 | loss: 9.005507 | lr: 2.5259e-04 | norm: 4.0515 | time: 12809.92ms | tok/sec: 40928.28\r\n",
      "step:   301 | loss: 8.792913 | lr: 2.5343e-04 | norm: 3.6794 | time: 12809.68ms | tok/sec: 40929.04\r\n",
      "step:   302 | loss: 8.851332 | lr: 2.5427e-04 | norm: 3.8261 | time: 12811.33ms | tok/sec: 40923.78\r\n",
      "step:   303 | loss: 8.868321 | lr: 2.5510e-04 | norm: 3.9960 | time: 12818.81ms | tok/sec: 40899.90\r\n",
      "step:   304 | loss: 9.035460 | lr: 2.5594e-04 | norm: 3.7412 | time: 12808.33ms | tok/sec: 40933.38\r\n",
      "step:   305 | loss: 8.871449 | lr: 2.5678e-04 | norm: 3.3720 | time: 12818.67ms | tok/sec: 40900.36\r\n",
      "step:   306 | loss: 8.848777 | lr: 2.5762e-04 | norm: 3.4156 | time: 12806.88ms | tok/sec: 40937.98\r\n",
      "step:   307 | loss: 8.857529 | lr: 2.5846e-04 | norm: 2.9767 | time: 12777.17ms | tok/sec: 41033.17\r\n",
      "step:   308 | loss: 8.879831 | lr: 2.5930e-04 | norm: 2.8151 | time: 12761.34ms | tok/sec: 41084.08\r\n",
      "step:   309 | loss: 8.938112 | lr: 2.6014e-04 | norm: 2.7520 | time: 12740.54ms | tok/sec: 41151.17\r\n",
      "step:   310 | loss: 8.849216 | lr: 2.6098e-04 | norm: 2.6669 | time: 12750.16ms | tok/sec: 41120.10\r\n",
      "step:   311 | loss: 8.838804 | lr: 2.6182e-04 | norm: 2.5514 | time: 12744.80ms | tok/sec: 41137.41\r\n",
      "step:   312 | loss: 8.865530 | lr: 2.6266e-04 | norm: 2.5915 | time: 12768.82ms | tok/sec: 41060.03\r\n",
      "step:   313 | loss: 9.027002 | lr: 2.6350e-04 | norm: 2.5407 | time: 12777.89ms | tok/sec: 41030.88\r\n",
      "step:   314 | loss: 8.925926 | lr: 2.6434e-04 | norm: 2.6103 | time: 12771.30ms | tok/sec: 41052.06\r\n",
      "step:   315 | loss: 8.940224 | lr: 2.6517e-04 | norm: 2.6487 | time: 12788.51ms | tok/sec: 40996.82\r\n",
      "step:   316 | loss: 9.008917 | lr: 2.6601e-04 | norm: 2.5547 | time: 12778.42ms | tok/sec: 41029.17\r\n",
      "step:   317 | loss: 8.995073 | lr: 2.6685e-04 | norm: 2.5118 | time: 12766.76ms | tok/sec: 41066.66\r\n",
      "step:   318 | loss: 9.098869 | lr: 2.6769e-04 | norm: 2.5126 | time: 12751.47ms | tok/sec: 41115.88\r\n",
      "step:   319 | loss: 8.994708 | lr: 2.6853e-04 | norm: 2.5332 | time: 12740.03ms | tok/sec: 41152.79\r\n",
      "step:   320 | loss: 8.939656 | lr: 2.6937e-04 | norm: 2.5671 | time: 12733.70ms | tok/sec: 41173.27\r\n",
      "step:   321 | loss: 9.039932 | lr: 2.7021e-04 | norm: 2.5080 | time: 12740.10ms | tok/sec: 41152.57\r\n",
      "step:   322 | loss: 9.122313 | lr: 2.7105e-04 | norm: 2.4892 | time: 12715.08ms | tok/sec: 41233.55\r\n",
      "step:   323 | loss: 9.036846 | lr: 2.7189e-04 | norm: 2.5625 | time: 12723.23ms | tok/sec: 41207.16\r\n",
      "step:   324 | loss: 9.150496 | lr: 2.7273e-04 | norm: 2.4778 | time: 12720.91ms | tok/sec: 41214.66\r\n",
      "step:   325 | loss: 9.271052 | lr: 2.7357e-04 | norm: 2.4989 | time: 12729.45ms | tok/sec: 41187.00\r\n",
      "step:   326 | loss: 9.272341 | lr: 2.7441e-04 | norm: 2.4148 | time: 12718.43ms | tok/sec: 41222.71\r\n",
      "step:   327 | loss: 9.273486 | lr: 2.7524e-04 | norm: 2.4208 | time: 12728.25ms | tok/sec: 41190.88\r\n",
      "step:   328 | loss: 9.356654 | lr: 2.7608e-04 | norm: 2.5234 | time: 12725.07ms | tok/sec: 41201.18\r\n",
      "step:   329 | loss: 9.445080 | lr: 2.7692e-04 | norm: 2.5908 | time: 12726.46ms | tok/sec: 41196.68\r\n",
      "step:   330 | loss: 9.347498 | lr: 2.7776e-04 | norm: 2.5117 | time: 12727.34ms | tok/sec: 41193.84\r\n",
      "step:   331 | loss: 9.313745 | lr: 2.7860e-04 | norm: 2.4496 | time: 12724.52ms | tok/sec: 41202.97\r\n",
      "step:   332 | loss: 9.353743 | lr: 2.7944e-04 | norm: 2.3205 | time: 12721.33ms | tok/sec: 41213.29\r\n",
      "step:   333 | loss: 9.285168 | lr: 2.8028e-04 | norm: 2.4668 | time: 12729.05ms | tok/sec: 41188.29\r\n",
      "step:   334 | loss: 9.661092 | lr: 2.8112e-04 | norm: 2.4405 | time: 12734.96ms | tok/sec: 41169.21\r\n",
      "step:   335 | loss: 9.433367 | lr: 2.8196e-04 | norm: 2.4476 | time: 12721.30ms | tok/sec: 41213.41\r\n",
      "step:   336 | loss: 9.275801 | lr: 2.8280e-04 | norm: 2.6394 | time: 12740.61ms | tok/sec: 41150.94\r\n",
      "step:   337 | loss: 9.428633 | lr: 2.8364e-04 | norm: 2.5646 | time: 12721.49ms | tok/sec: 41212.77\r\n",
      "step:   338 | loss: 9.470201 | lr: 2.8448e-04 | norm: 2.5226 | time: 12718.99ms | tok/sec: 41220.89\r\n",
      "step:   339 | loss: 9.436113 | lr: 2.8531e-04 | norm: 2.4519 | time: 12714.78ms | tok/sec: 41234.54\r\n",
      "step:   340 | loss: 9.377972 | lr: 2.8615e-04 | norm: 2.3915 | time: 12695.12ms | tok/sec: 41298.39\r\n",
      "step:   341 | loss: 9.473602 | lr: 2.8699e-04 | norm: 2.4390 | time: 12701.42ms | tok/sec: 41277.90\r\n",
      "step:   342 | loss: 9.484264 | lr: 2.8783e-04 | norm: 2.3327 | time: 12685.84ms | tok/sec: 41328.59\r\n",
      "step:   343 | loss: 9.359575 | lr: 2.8867e-04 | norm: 2.4307 | time: 12677.89ms | tok/sec: 41354.53\r\n",
      "step:   344 | loss: 9.484484 | lr: 2.8951e-04 | norm: 2.5605 | time: 12663.46ms | tok/sec: 41401.63\r\n",
      "step:   345 | loss: 9.367249 | lr: 2.9035e-04 | norm: 2.6079 | time: 12659.10ms | tok/sec: 41415.90\r\n",
      "step:   346 | loss: 9.387928 | lr: 2.9119e-04 | norm: 2.5771 | time: 12638.00ms | tok/sec: 41485.06\r\n",
      "step:   347 | loss: 9.568241 | lr: 2.9203e-04 | norm: 2.6514 | time: 12649.11ms | tok/sec: 41448.61\r\n",
      "step:   348 | loss: 9.335063 | lr: 2.9287e-04 | norm: 2.6912 | time: 12664.12ms | tok/sec: 41399.49\r\n",
      "step:   349 | loss: 9.477308 | lr: 2.9371e-04 | norm: 2.7249 | time: 12638.07ms | tok/sec: 41484.81\r\n",
      "step:   350 | loss: 9.504443 | lr: 2.9455e-04 | norm: 2.7108 | time: 12646.00ms | tok/sec: 41458.81\r\n",
      "step:   351 | loss: 9.494314 | lr: 2.9538e-04 | norm: 2.7425 | time: 12646.50ms | tok/sec: 41457.16\r\n",
      "step:   352 | loss: 9.421558 | lr: 2.9622e-04 | norm: 2.7377 | time: 12645.84ms | tok/sec: 41459.33\r\n",
      "step:   353 | loss: 9.467339 | lr: 2.9706e-04 | norm: 2.7674 | time: 12658.35ms | tok/sec: 41418.35\r\n",
      "step:   354 | loss: 9.603474 | lr: 2.9790e-04 | norm: 2.7625 | time: 12651.59ms | tok/sec: 41440.47\r\n",
      "step:   355 | loss: 9.570316 | lr: 2.9874e-04 | norm: 2.7113 | time: 12662.80ms | tok/sec: 41403.80\r\n",
      "step:   356 | loss: 9.461246 | lr: 2.9958e-04 | norm: 2.7384 | time: 12668.35ms | tok/sec: 41385.64\r\n",
      "step:   357 | loss: 9.559362 | lr: 3.0042e-04 | norm: 2.8336 | time: 12671.53ms | tok/sec: 41375.27\r\n",
      "step:   358 | loss: 9.577837 | lr: 3.0126e-04 | norm: 2.5677 | time: 12670.11ms | tok/sec: 41379.91\r\n",
      "step:   359 | loss: 9.608036 | lr: 3.0210e-04 | norm: 2.6620 | time: 12686.04ms | tok/sec: 41327.93\r\n",
      "step:   360 | loss: 9.665548 | lr: 3.0294e-04 | norm: 2.7036 | time: 12681.05ms | tok/sec: 41344.22\r\n",
      "step:   361 | loss: 9.633514 | lr: 3.0378e-04 | norm: 2.6043 | time: 12675.90ms | tok/sec: 41361.00\r\n",
      "step:   362 | loss: 9.783222 | lr: 3.0462e-04 | norm: 2.5481 | time: 12671.60ms | tok/sec: 41375.04\r\n",
      "step:   363 | loss: 9.564602 | lr: 3.0545e-04 | norm: 2.6761 | time: 12691.33ms | tok/sec: 41310.71\r\n",
      "step:   364 | loss: 9.844028 | lr: 3.0629e-04 | norm: 2.7240 | time: 12671.19ms | tok/sec: 41376.38\r\n",
      "step:   365 | loss: 9.658884 | lr: 3.0713e-04 | norm: 2.7467 | time: 12666.62ms | tok/sec: 41391.30\r\n",
      "step:   366 | loss: 9.583053 | lr: 3.0797e-04 | norm: 2.6684 | time: 12648.78ms | tok/sec: 41449.69\r\n",
      "step:   367 | loss: 10.010944 | lr: 3.0881e-04 | norm: 2.6086 | time: 12648.52ms | tok/sec: 41450.53\r\n",
      "step:   368 | loss: 9.866062 | lr: 3.0965e-04 | norm: 2.7210 | time: 12654.96ms | tok/sec: 41429.44\r\n",
      "step:   369 | loss: 9.711419 | lr: 3.1049e-04 | norm: 2.8532 | time: 12637.04ms | tok/sec: 41488.21\r\n",
      "step:   370 | loss: 9.716825 | lr: 3.1133e-04 | norm: 2.7731 | time: 12636.91ms | tok/sec: 41488.61\r\n",
      "step:   371 | loss: 9.832464 | lr: 3.1217e-04 | norm: 2.9316 | time: 12641.14ms | tok/sec: 41474.75\r\n",
      "step:   372 | loss: 9.998698 | lr: 3.1301e-04 | norm: 2.8691 | time: 12653.63ms | tok/sec: 41433.81\r\n",
      "step:   373 | loss: 9.923294 | lr: 3.1385e-04 | norm: 2.7620 | time: 12647.59ms | tok/sec: 41453.57\r\n",
      "step:   374 | loss: 10.180756 | lr: 3.1469e-04 | norm: 2.9704 | time: 12655.12ms | tok/sec: 41428.91\r\n",
      "step:   375 | loss: 9.980593 | lr: 3.1552e-04 | norm: 2.9404 | time: 12656.23ms | tok/sec: 41425.29\r\n",
      "step:   376 | loss: 10.038300 | lr: 3.1636e-04 | norm: 2.9656 | time: 12657.20ms | tok/sec: 41422.11\r\n",
      "step:   377 | loss: 10.090166 | lr: 3.1720e-04 | norm: 2.9345 | time: 12673.82ms | tok/sec: 41367.79\r\n",
      "step:   378 | loss: 10.249251 | lr: 3.1804e-04 | norm: 2.7875 | time: 12658.59ms | tok/sec: 41417.56\r\n",
      "step:   379 | loss: 9.957449 | lr: 3.1888e-04 | norm: 2.2881 | time: 12665.33ms | tok/sec: 41395.53\r\n",
      "step:   380 | loss: 10.082463 | lr: 3.1972e-04 | norm: 2.8481 | time: 12670.12ms | tok/sec: 41379.87\r\n",
      "step:   381 | loss: 10.074081 | lr: 3.2056e-04 | norm: 2.6875 | time: 14136.51ms | tok/sec: 37087.51\r\n",
      "step:   382 | loss: 10.100627 | lr: 3.2140e-04 | norm: 2.9130 | time: 12661.84ms | tok/sec: 41406.95\r\n",
      "step:   383 | loss: 10.022575 | lr: 3.2224e-04 | norm: 2.9029 | time: 12675.83ms | tok/sec: 41361.24\r\n",
      "step:   384 | loss: 9.987257 | lr: 3.2308e-04 | norm: 2.9339 | time: 12667.88ms | tok/sec: 41387.19\r\n",
      "step:   385 | loss: 10.341532 | lr: 3.2392e-04 | norm: 2.8002 | time: 12677.27ms | tok/sec: 41356.52\r\n",
      "step:   386 | loss: 10.345207 | lr: 3.2476e-04 | norm: 3.0002 | time: 12673.11ms | tok/sec: 41370.12\r\n",
      "step:   387 | loss: 10.362804 | lr: 3.2559e-04 | norm: 3.0433 | time: 12647.41ms | tok/sec: 41454.17\r\n",
      "step:   388 | loss: 10.145387 | lr: 3.2643e-04 | norm: 2.9142 | time: 12650.81ms | tok/sec: 41443.04\r\n",
      "step:   389 | loss: 10.067554 | lr: 3.2727e-04 | norm: 3.0524 | time: 12633.28ms | tok/sec: 41500.54\r\n",
      "step:   390 | loss: 10.235137 | lr: 3.2811e-04 | norm: 2.9718 | time: 12633.99ms | tok/sec: 41498.21\r\n",
      "step:   391 | loss: 10.389492 | lr: 3.2895e-04 | norm: 2.8292 | time: 12642.21ms | tok/sec: 41471.23\r\n",
      "step:   392 | loss: 10.159583 | lr: 3.2979e-04 | norm: 3.0034 | time: 12638.06ms | tok/sec: 41484.86\r\n",
      "step:   393 | loss: 10.329771 | lr: 3.3063e-04 | norm: 3.0093 | time: 12644.25ms | tok/sec: 41464.55\r\n",
      "step:   394 | loss: 10.102780 | lr: 3.3147e-04 | norm: 3.0212 | time: 12645.00ms | tok/sec: 41462.09\r\n",
      "step:   395 | loss: 10.168783 | lr: 3.3231e-04 | norm: 2.9194 | time: 12655.88ms | tok/sec: 41426.42\r\n",
      "step:   396 | loss: 10.272283 | lr: 3.3315e-04 | norm: 2.9110 | time: 12652.12ms | tok/sec: 41438.73\r\n",
      "step:   397 | loss: 10.123178 | lr: 3.3399e-04 | norm: 3.0011 | time: 12673.44ms | tok/sec: 41369.03\r\n",
      "step:   398 | loss: 10.237329 | lr: 3.3483e-04 | norm: 2.8852 | time: 12663.94ms | tok/sec: 41400.07\r\n",
      "step:   399 | loss: 10.313976 | lr: 3.3566e-04 | norm: 2.7991 | time: 12668.35ms | tok/sec: 41385.66\r\n",
      "step:   400 | loss: 10.229509 | lr: 3.3650e-04 | norm: 2.9460 | time: 12667.83ms | tok/sec: 41387.34\r\n",
      "step:   401 | loss: 10.271923 | lr: 3.3734e-04 | norm: 2.9909 | time: 12656.97ms | tok/sec: 41422.86\r\n",
      "step:   402 | loss: 10.428830 | lr: 3.3818e-04 | norm: 3.0775 | time: 12661.98ms | tok/sec: 41406.49\r\n",
      "step:   403 | loss: 10.316242 | lr: 3.3902e-04 | norm: 3.1965 | time: 12647.26ms | tok/sec: 41454.68\r\n",
      "step:   404 | loss: 10.384435 | lr: 3.3986e-04 | norm: 3.1167 | time: 12636.18ms | tok/sec: 41491.03\r\n",
      "step:   405 | loss: 10.417019 | lr: 3.4070e-04 | norm: 3.1281 | time: 12638.75ms | tok/sec: 41482.57\r\n",
      "step:   406 | loss: 10.500522 | lr: 3.4154e-04 | norm: 3.1258 | time: 12629.63ms | tok/sec: 41512.52\r\n",
      "step:   407 | loss: 10.231522 | lr: 3.4238e-04 | norm: 3.1729 | time: 12636.16ms | tok/sec: 41491.09\r\n",
      "step:   408 | loss: 10.412919 | lr: 3.4322e-04 | norm: 3.2465 | time: 12631.16ms | tok/sec: 41507.51\r\n",
      "step:   409 | loss: 10.372124 | lr: 3.4406e-04 | norm: 3.1727 | time: 12630.14ms | tok/sec: 41510.86\r\n",
      "step:   410 | loss: 10.468516 | lr: 3.4490e-04 | norm: 3.1667 | time: 12609.62ms | tok/sec: 41578.42\r\n",
      "step:   411 | loss: 10.728115 | lr: 3.4573e-04 | norm: 3.1549 | time: 12635.50ms | tok/sec: 41493.26\r\n",
      "step:   412 | loss: 10.469241 | lr: 3.4657e-04 | norm: 3.2967 | time: 12639.21ms | tok/sec: 41481.07\r\n",
      "step:   413 | loss: 10.643772 | lr: 3.4741e-04 | norm: 3.2157 | time: 12626.49ms | tok/sec: 41522.87\r\n",
      "step:   414 | loss: 10.468341 | lr: 3.4825e-04 | norm: 3.3101 | time: 12630.58ms | tok/sec: 41509.41\r\n",
      "step:   415 | loss: 10.755146 | lr: 3.4909e-04 | norm: 3.3082 | time: 12634.90ms | tok/sec: 41495.23\r\n",
      "step:   416 | loss: 10.777648 | lr: 3.4993e-04 | norm: 3.2053 | time: 12617.70ms | tok/sec: 41551.78\r\n",
      "step:   417 | loss: 10.974817 | lr: 3.5077e-04 | norm: 3.2523 | time: 12625.74ms | tok/sec: 41525.32\r\n",
      "step:   418 | loss: 10.824343 | lr: 3.5161e-04 | norm: 3.3496 | time: 12621.55ms | tok/sec: 41539.11\r\n",
      "step:   419 | loss: 10.968936 | lr: 3.5245e-04 | norm: 3.3367 | time: 12628.72ms | tok/sec: 41515.53\r\n",
      "step:   420 | loss: 10.786299 | lr: 3.5329e-04 | norm: 3.3830 | time: 12629.92ms | tok/sec: 41511.59\r\n",
      "step:   421 | loss: 11.075560 | lr: 3.5413e-04 | norm: 3.3549 | time: 12651.93ms | tok/sec: 41439.38\r\n",
      "step:   422 | loss: 11.066429 | lr: 3.5497e-04 | norm: 3.0868 | time: 12633.92ms | tok/sec: 41498.45\r\n",
      "step:   423 | loss: 11.058245 | lr: 3.5580e-04 | norm: 3.2708 | time: 12636.88ms | tok/sec: 41488.71\r\n",
      "step:   424 | loss: 10.937635 | lr: 3.5664e-04 | norm: 3.2056 | time: 12627.97ms | tok/sec: 41517.99\r\n",
      "step:   425 | loss: 11.005800 | lr: 3.5748e-04 | norm: 3.2237 | time: 12632.81ms | tok/sec: 41502.08\r\n",
      "step:   426 | loss: 10.951616 | lr: 3.5832e-04 | norm: 3.3530 | time: 12632.01ms | tok/sec: 41504.73\r\n",
      "step:   427 | loss: 10.999704 | lr: 3.5916e-04 | norm: 3.3604 | time: 12628.63ms | tok/sec: 41515.81\r\n",
      "step:   428 | loss: 10.856470 | lr: 3.6000e-04 | norm: 3.3896 | time: 12632.41ms | tok/sec: 41503.41\r\n",
      "step:   429 | loss: 11.433446 | lr: 3.6084e-04 | norm: 3.3506 | time: 12621.82ms | tok/sec: 41538.21\r\n",
      "step:   430 | loss: 11.289279 | lr: 3.6168e-04 | norm: 2.9374 | time: 12637.94ms | tok/sec: 41485.24\r\n",
      "step:   431 | loss: 11.113186 | lr: 3.6252e-04 | norm: 3.3865 | time: 12635.87ms | tok/sec: 41492.05\r\n",
      "step:   432 | loss: 11.440874 | lr: 3.6336e-04 | norm: 2.7774 | time: 12640.04ms | tok/sec: 41478.33\r\n",
      "step:   433 | loss: 11.126129 | lr: 3.6420e-04 | norm: 3.3791 | time: 12652.84ms | tok/sec: 41436.40\r\n",
      "step:   434 | loss: 11.112925 | lr: 3.6503e-04 | norm: 3.3942 | time: 12651.06ms | tok/sec: 41442.22\r\n",
      "step:   435 | loss: 11.027851 | lr: 3.6587e-04 | norm: 3.3727 | time: 12662.46ms | tok/sec: 41404.91\r\n",
      "step:   436 | loss: 11.179253 | lr: 3.6671e-04 | norm: 3.4067 | time: 12665.27ms | tok/sec: 41395.72\r\n",
      "step:   437 | loss: 11.601618 | lr: 3.6755e-04 | norm: 3.3998 | time: 12669.27ms | tok/sec: 41382.65\r\n",
      "step:   438 | loss: 11.441025 | lr: 3.6839e-04 | norm: 3.2844 | time: 12677.65ms | tok/sec: 41355.31\r\n",
      "step:   439 | loss: 11.370396 | lr: 3.6923e-04 | norm: 3.4435 | time: 12660.40ms | tok/sec: 41411.66\r\n",
      "step:   440 | loss: 11.027822 | lr: 3.7007e-04 | norm: 3.4678 | time: 12643.00ms | tok/sec: 41468.65\r\n",
      "step:   441 | loss: 11.238823 | lr: 3.7091e-04 | norm: 3.3176 | time: 12653.33ms | tok/sec: 41434.77\r\n",
      "step:   442 | loss: 11.254282 | lr: 3.7175e-04 | norm: 3.4866 | time: 12632.52ms | tok/sec: 41503.04\r\n",
      "step:   443 | loss: 11.180209 | lr: 3.7259e-04 | norm: 3.5025 | time: 12633.01ms | tok/sec: 41501.42\r\n",
      "step:   444 | loss: 11.350586 | lr: 3.7343e-04 | norm: 3.4717 | time: 12633.84ms | tok/sec: 41498.71\r\n",
      "step:   445 | loss: 11.173296 | lr: 3.7427e-04 | norm: 3.6057 | time: 12639.94ms | tok/sec: 41478.68\r\n",
      "step:   446 | loss: 11.235409 | lr: 3.7510e-04 | norm: 3.6136 | time: 12637.37ms | tok/sec: 41487.13\r\n",
      "step:   447 | loss: 11.458115 | lr: 3.7594e-04 | norm: 3.5693 | time: 12621.43ms | tok/sec: 41539.50\r\n",
      "step:   448 | loss: 11.610308 | lr: 3.7678e-04 | norm: 3.4957 | time: 12621.34ms | tok/sec: 41539.82\r\n",
      "step:   449 | loss: 11.378593 | lr: 3.7762e-04 | norm: 3.5597 | time: 12615.99ms | tok/sec: 41557.42\r\n",
      "step:   450 | loss: 11.547323 | lr: 3.7846e-04 | norm: 3.5566 | time: 12610.35ms | tok/sec: 41576.00\r\n",
      "step:   451 | loss: 11.437531 | lr: 3.7930e-04 | norm: 3.6233 | time: 12616.82ms | tok/sec: 41554.68\r\n",
      "step:   452 | loss: 12.087669 | lr: 3.8014e-04 | norm: 3.5477 | time: 12605.82ms | tok/sec: 41590.93\r\n",
      "step:   453 | loss: 11.329775 | lr: 3.8098e-04 | norm: 3.6063 | time: 12613.04ms | tok/sec: 41567.13\r\n",
      "step:   454 | loss: 11.380833 | lr: 3.8182e-04 | norm: 3.5018 | time: 12635.78ms | tok/sec: 41492.34\r\n",
      "step:   455 | loss: 11.345897 | lr: 3.8266e-04 | norm: 3.7365 | time: 12629.54ms | tok/sec: 41512.85\r\n",
      "step:   456 | loss: 11.757250 | lr: 3.8350e-04 | norm: 3.7201 | time: 12635.10ms | tok/sec: 41494.58\r\n",
      "step:   457 | loss: 11.555497 | lr: 3.8434e-04 | norm: 3.7145 | time: 12645.95ms | tok/sec: 41458.97\r\n",
      "step:   458 | loss: 11.652058 | lr: 3.8517e-04 | norm: 3.7272 | time: 12637.23ms | tok/sec: 41487.56\r\n",
      "step:   459 | loss: 11.850779 | lr: 3.8601e-04 | norm: 3.6628 | time: 12627.97ms | tok/sec: 41517.99\r\n",
      "step:   460 | loss: 11.768667 | lr: 3.8685e-04 | norm: 3.7846 | time: 12629.77ms | tok/sec: 41512.07\r\n",
      "step:   461 | loss: 11.663430 | lr: 3.8769e-04 | norm: 3.7925 | time: 12616.52ms | tok/sec: 41555.66\r\n",
      "step:   462 | loss: 11.428391 | lr: 3.8853e-04 | norm: 3.7538 | time: 12613.75ms | tok/sec: 41564.81\r\n",
      "step:   463 | loss: 12.145009 | lr: 3.8937e-04 | norm: 3.8019 | time: 12619.25ms | tok/sec: 41546.70\r\n",
      "step:   464 | loss: 11.715261 | lr: 3.9021e-04 | norm: 3.7947 | time: 12626.15ms | tok/sec: 41523.97\r\n",
      "step:   465 | loss: 12.236500 | lr: 3.9105e-04 | norm: 3.5471 | time: 12632.73ms | tok/sec: 41502.35\r\n",
      "step:   466 | loss: 12.184748 | lr: 3.9189e-04 | norm: 3.6740 | time: 12624.05ms | tok/sec: 41530.88\r\n",
      "step:   467 | loss: 12.073448 | lr: 3.9273e-04 | norm: 3.6746 | time: 12634.58ms | tok/sec: 41496.29\r\n",
      "step:   468 | loss: 12.238984 | lr: 3.9357e-04 | norm: 3.8198 | time: 12630.04ms | tok/sec: 41511.17\r\n",
      "step:   469 | loss: 12.284998 | lr: 3.9441e-04 | norm: 3.8230 | time: 12646.40ms | tok/sec: 41457.50\r\n",
      "step:   470 | loss: 12.148674 | lr: 3.9524e-04 | norm: 3.7693 | time: 12647.51ms | tok/sec: 41453.84\r\n",
      "step:   471 | loss: 11.952844 | lr: 3.9608e-04 | norm: 3.4990 | time: 12652.82ms | tok/sec: 41436.44\r\n",
      "step:   472 | loss: 12.615696 | lr: 3.9692e-04 | norm: 3.8883 | time: 12665.67ms | tok/sec: 41394.41\r\n",
      "step:   473 | loss: 12.447147 | lr: 3.9776e-04 | norm: 3.9193 | time: 12672.90ms | tok/sec: 41370.81\r\n",
      "step:   474 | loss: 12.393972 | lr: 3.9860e-04 | norm: 3.9507 | time: 12662.79ms | tok/sec: 41403.82\r\n",
      "step:   475 | loss: 12.345271 | lr: 3.9944e-04 | norm: 3.8480 | time: 12668.65ms | tok/sec: 41384.68\r\n",
      "step:   476 | loss: 12.045336 | lr: 4.0028e-04 | norm: 3.9952 | time: 12641.16ms | tok/sec: 41474.68\r\n",
      "step:   477 | loss: 12.179476 | lr: 4.0112e-04 | norm: 3.9178 | time: 12635.98ms | tok/sec: 41491.67\r\n",
      "step:   478 | loss: 12.168442 | lr: 4.0196e-04 | norm: 3.6615 | time: 12617.64ms | tok/sec: 41551.98\r\n",
      "step:   479 | loss: 12.740126 | lr: 4.0280e-04 | norm: 3.9624 | time: 12598.70ms | tok/sec: 41614.47\r\n",
      "step:   480 | loss: 12.462288 | lr: 4.0364e-04 | norm: 3.7320 | time: 12615.25ms | tok/sec: 41559.87\r\n",
      "step:   481 | loss: 12.640835 | lr: 4.0448e-04 | norm: 4.0028 | time: 12614.78ms | tok/sec: 41561.40\r\n",
      "step:   482 | loss: 12.394598 | lr: 4.0531e-04 | norm: 4.0470 | time: 12616.09ms | tok/sec: 41557.08\r\n",
      "step:   483 | loss: 12.772077 | lr: 4.0615e-04 | norm: 3.8299 | time: 12625.60ms | tok/sec: 41525.80\r\n",
      "step:   484 | loss: 12.469769 | lr: 4.0699e-04 | norm: 3.5510 | time: 12630.29ms | tok/sec: 41510.37\r\n",
      "step:   485 | loss: 12.413054 | lr: 4.0783e-04 | norm: 3.9643 | time: 12645.33ms | tok/sec: 41460.98\r\n",
      "step:   486 | loss: 12.353493 | lr: 4.0867e-04 | norm: 4.0187 | time: 12650.61ms | tok/sec: 41443.68\r\n",
      "step:   487 | loss: 12.565351 | lr: 4.0951e-04 | norm: 3.9772 | time: 12650.93ms | tok/sec: 41442.63\r\n",
      "step:   488 | loss: 12.669040 | lr: 4.1035e-04 | norm: 3.9689 | time: 12655.41ms | tok/sec: 41427.96\r\n",
      "step:   489 | loss: 12.403868 | lr: 4.1119e-04 | norm: 3.8460 | time: 12638.33ms | tok/sec: 41483.95\r\n",
      "step:   490 | loss: 12.611029 | lr: 4.1203e-04 | norm: 4.0344 | time: 12637.58ms | tok/sec: 41486.41\r\n",
      "step:   491 | loss: 12.443248 | lr: 4.1287e-04 | norm: 3.9605 | time: 12649.71ms | tok/sec: 41446.65\r\n",
      "step:   492 | loss: 12.418793 | lr: 4.1371e-04 | norm: 4.1845 | time: 12632.64ms | tok/sec: 41502.63\r\n",
      "step:   493 | loss: 12.756044 | lr: 4.1455e-04 | norm: 4.0540 | time: 12642.98ms | tok/sec: 41468.69\r\n",
      "step:   494 | loss: 12.937828 | lr: 4.1538e-04 | norm: 3.8815 | time: 12636.85ms | tok/sec: 41488.81\r\n",
      "step:   495 | loss: 12.552133 | lr: 4.1622e-04 | norm: 4.1441 | time: 12641.09ms | tok/sec: 41474.89\r\n",
      "step:   496 | loss: 12.721796 | lr: 4.1706e-04 | norm: 3.9450 | time: 12645.48ms | tok/sec: 41460.51\r\n",
      "step:   497 | loss: 12.699356 | lr: 4.1790e-04 | norm: 3.9923 | time: 12634.10ms | tok/sec: 41497.84\r\n",
      "step:   498 | loss: 12.624626 | lr: 4.1874e-04 | norm: 4.2535 | time: 12631.45ms | tok/sec: 41506.57\r\n",
      "step:   499 | loss: 12.632141 | lr: 4.1958e-04 | norm: 4.2687 | time: 12641.54ms | tok/sec: 41473.44\r\n",
      "validation loss: 13.1694\r\n",
      "step:   500 | loss: 12.898394 | lr: 4.2042e-04 | norm: 4.2490 | time: 16089.21ms | tok/sec: 32586.31\r\n",
      "step:   501 | loss: 12.772675 | lr: 4.2126e-04 | norm: 4.2144 | time: 12616.13ms | tok/sec: 41556.95\r\n",
      "step:   502 | loss: 13.025011 | lr: 4.2210e-04 | norm: 4.2754 | time: 12625.21ms | tok/sec: 41527.08\r\n",
      "step:   503 | loss: 12.762770 | lr: 4.2294e-04 | norm: 4.3106 | time: 12624.51ms | tok/sec: 41529.37\r\n",
      "step:   504 | loss: 12.908252 | lr: 4.2378e-04 | norm: 4.1531 | time: 12610.27ms | tok/sec: 41576.29\r\n",
      "step:   505 | loss: 12.732849 | lr: 4.2462e-04 | norm: 4.3212 | time: 12607.50ms | tok/sec: 41585.40\r\n",
      "step:   506 | loss: 12.861622 | lr: 4.2545e-04 | norm: 4.3336 | time: 12593.36ms | tok/sec: 41632.10\r\n",
      "step:   507 | loss: 13.356709 | lr: 4.2629e-04 | norm: 4.2763 | time: 12620.32ms | tok/sec: 41543.17\r\n",
      "step:   508 | loss: 12.645534 | lr: 4.2713e-04 | norm: 4.3762 | time: 12608.13ms | tok/sec: 41583.34\r\n",
      "step:   509 | loss: 13.114632 | lr: 4.2797e-04 | norm: 4.3720 | time: 12613.43ms | tok/sec: 41565.85\r\n",
      "step:   510 | loss: 13.367425 | lr: 4.2881e-04 | norm: 4.4308 | time: 12624.70ms | tok/sec: 41528.74\r\n",
      "step:   511 | loss: 13.501659 | lr: 4.2965e-04 | norm: 4.0899 | time: 12611.78ms | tok/sec: 41571.30\r\n",
      "step:   512 | loss: 13.617603 | lr: 4.3049e-04 | norm: 4.3568 | time: 12605.72ms | tok/sec: 41591.28\r\n",
      "step:   513 | loss: 13.531046 | lr: 4.3133e-04 | norm: 3.9662 | time: 12607.00ms | tok/sec: 41587.06\r\n",
      "step:   514 | loss: 13.823383 | lr: 4.3217e-04 | norm: 4.4232 | time: 12606.24ms | tok/sec: 41589.56\r\n",
      "step:   515 | loss: 13.800787 | lr: 4.3301e-04 | norm: 4.4001 | time: 12606.08ms | tok/sec: 41590.08\r\n",
      "step:   516 | loss: 13.625055 | lr: 4.3385e-04 | norm: 4.2109 | time: 12618.62ms | tok/sec: 41548.74\r\n",
      "step:   517 | loss: 14.093600 | lr: 4.3469e-04 | norm: 4.4415 | time: 12623.29ms | tok/sec: 41533.38\r\n",
      "step:   518 | loss: 13.879732 | lr: 4.3552e-04 | norm: 4.1155 | time: 12631.95ms | tok/sec: 41504.90\r\n",
      "step:   519 | loss: 13.960779 | lr: 4.3636e-04 | norm: 4.4852 | time: 12633.32ms | tok/sec: 41500.42\r\n",
      "step:   520 | loss: 13.901749 | lr: 4.3720e-04 | norm: 4.3607 | time: 12631.71ms | tok/sec: 41505.69\r\n",
      "step:   521 | loss: 13.660540 | lr: 4.3804e-04 | norm: 4.2504 | time: 12645.43ms | tok/sec: 41460.68\r\n",
      "step:   522 | loss: 14.020561 | lr: 4.3888e-04 | norm: 4.5042 | time: 12636.56ms | tok/sec: 41489.76\r\n",
      "step:   523 | loss: 13.683655 | lr: 4.3972e-04 | norm: 4.3087 | time: 12647.01ms | tok/sec: 41455.50\r\n",
      "step:   524 | loss: 13.623460 | lr: 4.4056e-04 | norm: 4.1551 | time: 12642.11ms | tok/sec: 41471.57\r\n",
      "step:   525 | loss: 13.527719 | lr: 4.4140e-04 | norm: 4.2461 | time: 12641.01ms | tok/sec: 41475.16\r\n",
      "step:   526 | loss: 13.971471 | lr: 4.4224e-04 | norm: 4.2369 | time: 12629.83ms | tok/sec: 41511.89\r\n",
      "step:   527 | loss: 13.727634 | lr: 4.4308e-04 | norm: 4.2493 | time: 12627.30ms | tok/sec: 41520.20\r\n",
      "step:   528 | loss: 13.786486 | lr: 4.4392e-04 | norm: 4.5415 | time: 12614.17ms | tok/sec: 41563.42\r\n",
      "step:   529 | loss: 14.099228 | lr: 4.4476e-04 | norm: 4.4360 | time: 12626.12ms | tok/sec: 41524.08\r\n",
      "step:   530 | loss: 14.302282 | lr: 4.4559e-04 | norm: 4.4641 | time: 12605.92ms | tok/sec: 41590.60\r\n",
      "step:   531 | loss: 13.876377 | lr: 4.4643e-04 | norm: 4.4904 | time: 12612.66ms | tok/sec: 41568.39\r\n",
      "step:   532 | loss: 14.054887 | lr: 4.4727e-04 | norm: 4.3459 | time: 12614.33ms | tok/sec: 41562.90\r\n",
      "step:   533 | loss: 14.051103 | lr: 4.4811e-04 | norm: 4.5418 | time: 12607.26ms | tok/sec: 41586.19\r\n",
      "step:   534 | loss: 13.795214 | lr: 4.4895e-04 | norm: 4.5564 | time: 12594.33ms | tok/sec: 41628.89\r\n",
      "step:   535 | loss: 14.056250 | lr: 4.4979e-04 | norm: 4.3742 | time: 12602.33ms | tok/sec: 41602.46\r\n",
      "step:   536 | loss: 14.536232 | lr: 4.5063e-04 | norm: 4.6712 | time: 12611.32ms | tok/sec: 41572.81\r\n",
      "step:   537 | loss: 13.989124 | lr: 4.5147e-04 | norm: 4.4093 | time: 12624.15ms | tok/sec: 41530.56\r\n",
      "step:   538 | loss: 14.014130 | lr: 4.5231e-04 | norm: 4.3653 | time: 12635.19ms | tok/sec: 41494.26\r\n",
      "step:   539 | loss: 14.294720 | lr: 4.5315e-04 | norm: 4.4875 | time: 12639.27ms | tok/sec: 41480.86\r\n",
      "step:   540 | loss: 15.085681 | lr: 4.5399e-04 | norm: 4.7428 | time: 12635.48ms | tok/sec: 41493.31\r\n",
      "step:   541 | loss: 13.734228 | lr: 4.5483e-04 | norm: 4.6599 | time: 12638.34ms | tok/sec: 41483.93\r\n",
      "step:   542 | loss: 14.416381 | lr: 4.5566e-04 | norm: 4.7143 | time: 12628.84ms | tok/sec: 41515.15\r\n",
      "step:   543 | loss: 14.228298 | lr: 4.5650e-04 | norm: 4.8235 | time: 12622.28ms | tok/sec: 41536.72\r\n",
      "step:   544 | loss: 14.128202 | lr: 4.5734e-04 | norm: 4.8641 | time: 12627.39ms | tok/sec: 41519.90\r\n",
      "step:   545 | loss: 14.218878 | lr: 4.5818e-04 | norm: 4.7183 | time: 12632.15ms | tok/sec: 41504.26\r\n",
      "step:   546 | loss: 14.117613 | lr: 4.5902e-04 | norm: 4.6647 | time: 12614.78ms | tok/sec: 41561.40\r\n",
      "step:   547 | loss: 14.217014 | lr: 4.5986e-04 | norm: 4.9093 | time: 12613.02ms | tok/sec: 41567.21\r\n",
      "step:   548 | loss: 14.207952 | lr: 4.6070e-04 | norm: 4.8150 | time: 12611.74ms | tok/sec: 41571.44\r\n",
      "step:   549 | loss: 14.367203 | lr: 4.6154e-04 | norm: 4.6727 | time: 12608.94ms | tok/sec: 41580.66\r\n",
      "step:   550 | loss: 14.473686 | lr: 4.6238e-04 | norm: 4.8763 | time: 12620.05ms | tok/sec: 41544.06\r\n",
      "step:   551 | loss: 14.365200 | lr: 4.6322e-04 | norm: 4.9768 | time: 12611.03ms | tok/sec: 41573.77\r\n",
      "step:   552 | loss: 14.360664 | lr: 4.6406e-04 | norm: 4.9249 | time: 12606.06ms | tok/sec: 41590.14\r\n",
      "step:   553 | loss: 15.125868 | lr: 4.6490e-04 | norm: 4.7561 | time: 12613.72ms | tok/sec: 41564.90\r\n",
      "step:   554 | loss: 14.470518 | lr: 4.6573e-04 | norm: 4.9109 | time: 12623.55ms | tok/sec: 41532.54\r\n",
      "step:   555 | loss: 14.867290 | lr: 4.6657e-04 | norm: 4.7100 | time: 12607.38ms | tok/sec: 41585.81\r\n",
      "step:   556 | loss: 14.835466 | lr: 4.6741e-04 | norm: 4.7374 | time: 12625.22ms | tok/sec: 41527.03\r\n",
      "step:   557 | loss: 15.146516 | lr: 4.6825e-04 | norm: 4.8379 | time: 12613.01ms | tok/sec: 41567.22\r\n",
      "step:   558 | loss: 15.446853 | lr: 4.6909e-04 | norm: 4.7992 | time: 12632.65ms | tok/sec: 41502.61\r\n",
      "step:   559 | loss: 15.408281 | lr: 4.6993e-04 | norm: 5.0372 | time: 12619.88ms | tok/sec: 41544.61\r\n",
      "step:   560 | loss: 15.532206 | lr: 4.7077e-04 | norm: 5.0479 | time: 12613.59ms | tok/sec: 41565.33\r\n",
      "step:   561 | loss: 15.394950 | lr: 4.7161e-04 | norm: 4.8051 | time: 12613.29ms | tok/sec: 41566.32\r\n",
      "step:   562 | loss: 15.505533 | lr: 4.7245e-04 | norm: 4.5825 | time: 12641.91ms | tok/sec: 41472.23\r\n",
      "step:   563 | loss: 15.096482 | lr: 4.7329e-04 | norm: 4.8114 | time: 12648.92ms | tok/sec: 41449.23\r\n",
      "step:   564 | loss: 15.739491 | lr: 4.7413e-04 | norm: 4.6810 | time: 12644.76ms | tok/sec: 41462.86\r\n",
      "step:   565 | loss: 15.589630 | lr: 4.7497e-04 | norm: 4.6764 | time: 12635.15ms | tok/sec: 41494.40\r\n",
      "step:   566 | loss: 15.883582 | lr: 4.7580e-04 | norm: 4.6538 | time: 12626.86ms | tok/sec: 41521.66\r\n",
      "step:   567 | loss: 15.845787 | lr: 4.7664e-04 | norm: 4.9092 | time: 12642.16ms | tok/sec: 41471.40\r\n",
      "step:   568 | loss: 15.493751 | lr: 4.7748e-04 | norm: 4.7175 | time: 12623.55ms | tok/sec: 41532.54\r\n",
      "step:   569 | loss: 15.476907 | lr: 4.7832e-04 | norm: 4.8317 | time: 12629.58ms | tok/sec: 41512.71\r\n",
      "step:   570 | loss: 15.231485 | lr: 4.7916e-04 | norm: 4.8551 | time: 12615.12ms | tok/sec: 41560.29\r\n",
      "step:   571 | loss: 15.203270 | lr: 4.8000e-04 | norm: 4.8604 | time: 12643.46ms | tok/sec: 41467.15\r\n",
      "step:   572 | loss: 15.669135 | lr: 4.8084e-04 | norm: 5.1427 | time: 14034.91ms | tok/sec: 37355.98\r\n",
      "step:   573 | loss: 15.573736 | lr: 4.8168e-04 | norm: 5.0905 | time: 12632.59ms | tok/sec: 41502.81\r\n",
      "step:   574 | loss: 15.817371 | lr: 4.8252e-04 | norm: 5.2900 | time: 12658.98ms | tok/sec: 41416.28\r\n",
      "step:   575 | loss: 15.843815 | lr: 4.8336e-04 | norm: 5.0952 | time: 12662.53ms | tok/sec: 41404.67\r\n",
      "step:   576 | loss: 15.844511 | lr: 4.8420e-04 | norm: 5.3414 | time: 12665.64ms | tok/sec: 41394.50\r\n",
      "step:   577 | loss: 15.579498 | lr: 4.8503e-04 | norm: 5.3922 | time: 12650.99ms | tok/sec: 41442.45\r\n",
      "step:   578 | loss: 15.639794 | lr: 4.8587e-04 | norm: 5.0327 | time: 12642.38ms | tok/sec: 41470.66\r\n",
      "step:   579 | loss: 15.574839 | lr: 4.8671e-04 | norm: 5.0684 | time: 12646.34ms | tok/sec: 41457.68\r\n",
      "step:   580 | loss: 15.860687 | lr: 4.8755e-04 | norm: 5.3960 | time: 12640.18ms | tok/sec: 41477.88\r\n",
      "step:   581 | loss: 15.709114 | lr: 4.8839e-04 | norm: 5.4184 | time: 12641.87ms | tok/sec: 41472.36\r\n",
      "step:   582 | loss: 20.645054 | lr: 4.8923e-04 | norm: 4.9393 | time: 12616.88ms | tok/sec: 41554.49\r\n",
      "step:   583 | loss: 15.748199 | lr: 4.9007e-04 | norm: 5.5068 | time: 12622.82ms | tok/sec: 41534.95\r\n",
      "step:   584 | loss: 15.795366 | lr: 4.9091e-04 | norm: 5.1844 | time: 12610.54ms | tok/sec: 41575.38\r\n",
      "step:   585 | loss: 15.797986 | lr: 4.9175e-04 | norm: 5.2146 | time: 12622.95ms | tok/sec: 41534.50\r\n",
      "step:   586 | loss: 16.154911 | lr: 4.9259e-04 | norm: 5.1231 | time: 12624.43ms | tok/sec: 41529.65\r\n",
      "step:   587 | loss: 16.256680 | lr: 4.9343e-04 | norm: 5.1421 | time: 12636.46ms | tok/sec: 41490.10\r\n",
      "step:   588 | loss: 16.461460 | lr: 4.9427e-04 | norm: 5.1196 | time: 12620.19ms | tok/sec: 41543.59\r\n",
      "step:   589 | loss: 16.113363 | lr: 4.9510e-04 | norm: 5.6240 | time: 12632.32ms | tok/sec: 41503.70\r\n",
      "step:   590 | loss: 16.212452 | lr: 4.9594e-04 | norm: 5.4734 | time: 12606.05ms | tok/sec: 41590.20\r\n",
      "step:   591 | loss: 16.681946 | lr: 4.9678e-04 | norm: 5.5020 | time: 12626.62ms | tok/sec: 41522.43\r\n",
      "step:   592 | loss: 15.969545 | lr: 4.9762e-04 | norm: 5.5421 | time: 12620.73ms | tok/sec: 41541.82\r\n",
      "step:   593 | loss: 15.707198 | lr: 4.9846e-04 | norm: 5.2722 | time: 12610.31ms | tok/sec: 41576.13\r\n",
      "step:   594 | loss: 16.164356 | lr: 4.9930e-04 | norm: 5.5911 | time: 12620.16ms | tok/sec: 41543.70\r\n",
      "step:   595 | loss: 16.850189 | lr: 5.0014e-04 | norm: 5.6203 | time: 12650.26ms | tok/sec: 41444.85\r\n",
      "step:   596 | loss: 15.996212 | lr: 5.0098e-04 | norm: 5.7987 | time: 12640.11ms | tok/sec: 41478.11\r\n",
      "step:   597 | loss: 16.617502 | lr: 5.0182e-04 | norm: 5.6434 | time: 12638.69ms | tok/sec: 41482.78\r\n",
      "step:   598 | loss: 16.770758 | lr: 5.0266e-04 | norm: 5.4811 | time: 12627.55ms | tok/sec: 41519.37\r\n",
      "step:   599 | loss: 16.171413 | lr: 5.0350e-04 | norm: 5.5647 | time: 12639.43ms | tok/sec: 41480.36\r\n",
      "step:   600 | loss: 16.985239 | lr: 5.0434e-04 | norm: 5.7418 | time: 12637.12ms | tok/sec: 41487.95\r\n",
      "step:   601 | loss: 16.226725 | lr: 5.0517e-04 | norm: 5.5589 | time: 12643.09ms | tok/sec: 41468.33\r\n",
      "step:   602 | loss: 16.143665 | lr: 5.0601e-04 | norm: 5.7841 | time: 12637.64ms | tok/sec: 41486.23\r\n",
      "step:   603 | loss: 17.243608 | lr: 5.0685e-04 | norm: 5.6652 | time: 12651.29ms | tok/sec: 41441.46\r\n",
      "step:   604 | loss: 17.430687 | lr: 5.0769e-04 | norm: 5.8800 | time: 12634.22ms | tok/sec: 41497.45\r\n",
      "step:   605 | loss: 17.635590 | lr: 5.0853e-04 | norm: 5.7337 | time: 12639.13ms | tok/sec: 41481.35\r\n",
      "step:   606 | loss: 17.691580 | lr: 5.0937e-04 | norm: 5.4575 | time: 12637.40ms | tok/sec: 41487.01\r\n",
      "step:   607 | loss: 17.299789 | lr: 5.1021e-04 | norm: 5.9075 | time: 12659.32ms | tok/sec: 41415.19\r\n",
      "step:   608 | loss: 17.315208 | lr: 5.1105e-04 | norm: 5.2621 | time: 12646.94ms | tok/sec: 41455.73\r\n",
      "step:   609 | loss: 17.808304 | lr: 5.1189e-04 | norm: 5.8438 | time: 12632.23ms | tok/sec: 41503.98\r\n",
      "step:   610 | loss: 17.770973 | lr: 5.1273e-04 | norm: 5.2979 | time: 12639.29ms | tok/sec: 41480.81\r\n",
      "step:   611 | loss: 17.205990 | lr: 5.1357e-04 | norm: 5.7631 | time: 12654.22ms | tok/sec: 41431.88\r\n",
      "step:   612 | loss: 17.871037 | lr: 5.1441e-04 | norm: 5.6487 | time: 12666.11ms | tok/sec: 41392.98\r\n",
      "step:   613 | loss: 17.602213 | lr: 5.1524e-04 | norm: 5.7531 | time: 12649.84ms | tok/sec: 41446.20\r\n",
      "step:   614 | loss: 17.716850 | lr: 5.1608e-04 | norm: 5.4730 | time: 12649.37ms | tok/sec: 41447.76\r\n",
      "step:   615 | loss: 17.723791 | lr: 5.1692e-04 | norm: 5.3155 | time: 12643.83ms | tok/sec: 41465.92\r\n",
      "step:   616 | loss: 16.692188 | lr: 5.1776e-04 | norm: 5.6739 | time: 12658.17ms | tok/sec: 41418.95\r\n",
      "step:   617 | loss: 17.391609 | lr: 5.1860e-04 | norm: 5.9116 | time: 12655.51ms | tok/sec: 41427.64\r\n",
      "step:   618 | loss: 17.217434 | lr: 5.1944e-04 | norm: 5.7126 | time: 12657.42ms | tok/sec: 41421.40\r\n",
      "step:   619 | loss: 17.435154 | lr: 5.2028e-04 | norm: 5.5872 | time: 12655.02ms | tok/sec: 41429.26\r\n",
      "step:   620 | loss: 18.202389 | lr: 5.2112e-04 | norm: 6.3006 | time: 12670.06ms | tok/sec: 41380.06\r\n",
      "step:   621 | loss: 17.519436 | lr: 5.2196e-04 | norm: 5.7242 | time: 12657.27ms | tok/sec: 41421.88\r\n",
      "step:   622 | loss: 17.754669 | lr: 5.2280e-04 | norm: 6.0207 | time: 12648.74ms | tok/sec: 41449.83\r\n",
      "step:   623 | loss: 17.748081 | lr: 5.2364e-04 | norm: 6.1606 | time: 12650.95ms | tok/sec: 41442.58\r\n",
      "step:   624 | loss: 18.031986 | lr: 5.2448e-04 | norm: 6.2566 | time: 12646.59ms | tok/sec: 41456.86\r\n",
      "step:   625 | loss: 18.528172 | lr: 5.2531e-04 | norm: 6.1437 | time: 12645.96ms | tok/sec: 41458.94\r\n",
      "step:   626 | loss: 18.783199 | lr: 5.2615e-04 | norm: 6.0261 | time: 12647.43ms | tok/sec: 41454.11\r\n",
      "step:   627 | loss: 17.604813 | lr: 5.2699e-04 | norm: 6.5390 | time: 12657.51ms | tok/sec: 41421.12\r\n",
      "step:   628 | loss: 17.838619 | lr: 5.2783e-04 | norm: 6.2350 | time: 12654.78ms | tok/sec: 41430.03\r\n",
      "step:   629 | loss: 18.098745 | lr: 5.2867e-04 | norm: 5.9912 | time: 12645.28ms | tok/sec: 41461.18\r\n",
      "step:   630 | loss: 18.217682 | lr: 5.2951e-04 | norm: 5.7991 | time: 12650.12ms | tok/sec: 41445.31\r\n",
      "step:   631 | loss: 18.443810 | lr: 5.3035e-04 | norm: 5.9748 | time: 12655.76ms | tok/sec: 41426.84\r\n",
      "step:   632 | loss: 17.756985 | lr: 5.3119e-04 | norm: 6.4846 | time: 12671.42ms | tok/sec: 41375.62\r\n",
      "step:   633 | loss: 18.936218 | lr: 5.3203e-04 | norm: 6.2277 | time: 12659.33ms | tok/sec: 41415.16\r\n",
      "step:   634 | loss: 18.333803 | lr: 5.3287e-04 | norm: 5.7945 | time: 12634.68ms | tok/sec: 41495.94\r\n",
      "step:   635 | loss: 18.440170 | lr: 5.3371e-04 | norm: 6.5635 | time: 12677.46ms | tok/sec: 41355.93\r\n",
      "step:   636 | loss: 18.741112 | lr: 5.3455e-04 | norm: 5.8337 | time: 12662.89ms | tok/sec: 41403.51\r\n",
      "step:   637 | loss: 20.363560 | lr: 5.3538e-04 | norm: 6.1710 | time: 12671.53ms | tok/sec: 41375.28\r\n",
      "step:   638 | loss: 18.158897 | lr: 5.3622e-04 | norm: 6.1064 | time: 12644.92ms | tok/sec: 41462.35\r\n",
      "step:   639 | loss: 18.587151 | lr: 5.3706e-04 | norm: 6.0652 | time: 12655.60ms | tok/sec: 41427.35\r\n",
      "step:   640 | loss: 17.851696 | lr: 5.3790e-04 | norm: 6.5301 | time: 12664.13ms | tok/sec: 41399.44\r\n",
      "step:   641 | loss: 18.815380 | lr: 5.3874e-04 | norm: 6.3817 | time: 12666.99ms | tok/sec: 41390.09\r\n",
      "step:   642 | loss: 18.039936 | lr: 5.3958e-04 | norm: 6.6382 | time: 12657.55ms | tok/sec: 41420.97\r\n",
      "step:   643 | loss: 18.481159 | lr: 5.4042e-04 | norm: 6.7766 | time: 12666.59ms | tok/sec: 41391.40\r\n",
      "step:   644 | loss: 18.400776 | lr: 5.4126e-04 | norm: 6.3877 | time: 12666.96ms | tok/sec: 41390.20\r\n",
      "step:   645 | loss: 18.332979 | lr: 5.4210e-04 | norm: 6.6256 | time: 12670.33ms | tok/sec: 41379.21\r\n",
      "step:   646 | loss: 18.527420 | lr: 5.4294e-04 | norm: 6.5945 | time: 12674.37ms | tok/sec: 41366.00\r\n",
      "step:   647 | loss: 18.484528 | lr: 5.4378e-04 | norm: 6.7132 | time: 12656.42ms | tok/sec: 41424.66\r\n",
      "step:   648 | loss: 18.702370 | lr: 5.4462e-04 | norm: 6.3162 | time: 12670.65ms | tok/sec: 41378.15\r\n",
      "step:   649 | loss: 18.718508 | lr: 5.4545e-04 | norm: 6.6631 | time: 12672.06ms | tok/sec: 41373.55\r\n",
      "step:   650 | loss: 19.136530 | lr: 5.4629e-04 | norm: 7.0596 | time: 12672.59ms | tok/sec: 41371.81\r\n",
      "step:   651 | loss: 20.009850 | lr: 5.4713e-04 | norm: 6.4314 | time: 12652.73ms | tok/sec: 41436.75\r\n",
      "step:   652 | loss: 20.224461 | lr: 5.4797e-04 | norm: 6.6409 | time: 12671.40ms | tok/sec: 41375.70\r\n",
      "step:   653 | loss: 19.844002 | lr: 5.4881e-04 | norm: 6.2406 | time: 12662.17ms | tok/sec: 41405.87\r\n",
      "step:   654 | loss: 19.997097 | lr: 5.4965e-04 | norm: 6.8879 | time: 12662.71ms | tok/sec: 41404.09\r\n",
      "step:   655 | loss: 20.189854 | lr: 5.5049e-04 | norm: 5.8118 | time: 12646.40ms | tok/sec: 41457.48\r\n",
      "step:   656 | loss: 20.062977 | lr: 5.5133e-04 | norm: 7.0866 | time: 12669.84ms | tok/sec: 41380.79\r\n",
      "step:   657 | loss: 19.728123 | lr: 5.5217e-04 | norm: 6.3199 | time: 12657.70ms | tok/sec: 41420.46\r\n",
      "step:   658 | loss: 19.735401 | lr: 5.5301e-04 | norm: 6.1217 | time: 12647.32ms | tok/sec: 41454.46\r\n",
      "step:   659 | loss: 20.437042 | lr: 5.5385e-04 | norm: 6.3896 | time: 12652.44ms | tok/sec: 41437.69\r\n",
      "step:   660 | loss: 20.538433 | lr: 5.5469e-04 | norm: 6.6892 | time: 12663.98ms | tok/sec: 41399.95\r\n",
      "step:   661 | loss: 19.772585 | lr: 5.5552e-04 | norm: 6.4213 | time: 12653.54ms | tok/sec: 41434.11\r\n",
      "step:   662 | loss: 19.801081 | lr: 5.5636e-04 | norm: 6.8093 | time: 12658.57ms | tok/sec: 41417.65\r\n",
      "step:   663 | loss: 20.381744 | lr: 5.5720e-04 | norm: 6.4216 | time: 12653.23ms | tok/sec: 41435.13\r\n",
      "step:   664 | loss: 19.406862 | lr: 5.5804e-04 | norm: 6.5921 | time: 12643.21ms | tok/sec: 41467.95\r\n",
      "step:   665 | loss: 20.107588 | lr: 5.5888e-04 | norm: 6.4251 | time: 12649.34ms | tok/sec: 41447.85\r\n",
      "step:   666 | loss: 20.210846 | lr: 5.5972e-04 | norm: 6.5630 | time: 12655.21ms | tok/sec: 41428.62\r\n",
      "step:   667 | loss: 19.905088 | lr: 5.6056e-04 | norm: 7.3315 | time: 12656.23ms | tok/sec: 41425.30\r\n",
      "step:   668 | loss: 20.069082 | lr: 5.6140e-04 | norm: 7.0448 | time: 12648.80ms | tok/sec: 41449.63\r\n",
      "step:   669 | loss: 20.077877 | lr: 5.6224e-04 | norm: 7.0139 | time: 12660.13ms | tok/sec: 41412.54\r\n",
      "step:   670 | loss: 19.535946 | lr: 5.6308e-04 | norm: 7.1664 | time: 12669.83ms | tok/sec: 41380.82\r\n",
      "step:   671 | loss: 20.200138 | lr: 5.6392e-04 | norm: 6.0980 | time: 12660.11ms | tok/sec: 41412.60\r\n",
      "step:   672 | loss: 20.329960 | lr: 5.6476e-04 | norm: 5.9157 | time: 12651.97ms | tok/sec: 41439.24\r\n",
      "step:   673 | loss: 20.475941 | lr: 5.6559e-04 | norm: 6.5190 | time: 12669.07ms | tok/sec: 41383.30\r\n",
      "step:   674 | loss: 20.150124 | lr: 5.6643e-04 | norm: 7.2939 | time: 12662.45ms | tok/sec: 41404.95\r\n",
      "step:   675 | loss: 20.029781 | lr: 5.6727e-04 | norm: 6.5060 | time: 12658.98ms | tok/sec: 41416.28\r\n",
      "step:   676 | loss: 21.146244 | lr: 5.6811e-04 | norm: 7.2026 | time: 12657.78ms | tok/sec: 41420.22\r\n",
      "step:   677 | loss: 20.473301 | lr: 5.6895e-04 | norm: 7.2012 | time: 12663.42ms | tok/sec: 41401.76\r\n",
      "step:   678 | loss: 21.010525 | lr: 5.6979e-04 | norm: 6.7234 | time: 12650.09ms | tok/sec: 41445.41\r\n",
      "step:   679 | loss: 20.684425 | lr: 5.7063e-04 | norm: 6.9376 | time: 12664.75ms | tok/sec: 41397.43\r\n",
      "step:   680 | loss: 20.435665 | lr: 5.7147e-04 | norm: 7.4280 | time: 12659.72ms | tok/sec: 41413.86\r\n",
      "step:   681 | loss: 20.572464 | lr: 5.7231e-04 | norm: 7.1672 | time: 12654.52ms | tok/sec: 41430.88\r\n",
      "step:   682 | loss: 21.149963 | lr: 5.7315e-04 | norm: 7.2410 | time: 12662.66ms | tok/sec: 41404.25\r\n",
      "step:   683 | loss: 21.215654 | lr: 5.7399e-04 | norm: 6.6887 | time: 12642.39ms | tok/sec: 41470.64\r\n",
      "step:   684 | loss: 21.481129 | lr: 5.7483e-04 | norm: 7.7453 | time: 12657.48ms | tok/sec: 41421.20\r\n",
      "step:   685 | loss: 21.229799 | lr: 5.7566e-04 | norm: 6.7974 | time: 12648.20ms | tok/sec: 41451.59\r\n",
      "step:   686 | loss: 21.161501 | lr: 5.7650e-04 | norm: 7.5330 | time: 12663.97ms | tok/sec: 41399.96\r\n",
      "step:   687 | loss: 21.371845 | lr: 5.7734e-04 | norm: 7.3563 | time: 12662.23ms | tok/sec: 41405.67\r\n",
      "step:   688 | loss: 21.016874 | lr: 5.7818e-04 | norm: 6.9851 | time: 12636.80ms | tok/sec: 41488.99\r\n",
      "step:   689 | loss: 20.912521 | lr: 5.7902e-04 | norm: 7.7391 | time: 12651.49ms | tok/sec: 41440.81\r\n",
      "step:   690 | loss: 21.196270 | lr: 5.7986e-04 | norm: 7.5732 | time: 12661.80ms | tok/sec: 41407.07\r\n",
      "step:   691 | loss: 21.206135 | lr: 5.8070e-04 | norm: 7.2881 | time: 12661.74ms | tok/sec: 41407.26\r\n",
      "step:   692 | loss: 21.370695 | lr: 5.8154e-04 | norm: 7.3905 | time: 12666.43ms | tok/sec: 41391.94\r\n",
      "step:   693 | loss: 21.314495 | lr: 5.8238e-04 | norm: 6.5585 | time: 12653.17ms | tok/sec: 41435.29\r\n",
      "step:   694 | loss: 22.076303 | lr: 5.8322e-04 | norm: 7.5264 | time: 12670.50ms | tok/sec: 41378.64\r\n",
      "step:   695 | loss: 21.015676 | lr: 5.8406e-04 | norm: 7.1051 | time: 12664.86ms | tok/sec: 41397.07\r\n",
      "step:   696 | loss: 22.161455 | lr: 5.8490e-04 | norm: 6.8689 | time: 12671.38ms | tok/sec: 41375.75\r\n",
      "step:   697 | loss: 22.737045 | lr: 5.8573e-04 | norm: 8.1645 | time: 12671.76ms | tok/sec: 41374.52\r\n",
      "step:   698 | loss: 22.705330 | lr: 5.8657e-04 | norm: 7.0499 | time: 12651.53ms | tok/sec: 41440.68\r\n",
      "step:   699 | loss: 22.500523 | lr: 5.8741e-04 | norm: 7.3556 | time: 12659.39ms | tok/sec: 41414.96\r\n",
      "step:   700 | loss: 23.665798 | lr: 5.8825e-04 | norm: 7.4466 | time: 12660.81ms | tok/sec: 41410.29\r\n",
      "step:   701 | loss: 22.984537 | lr: 5.8909e-04 | norm: 7.6514 | time: 12664.47ms | tok/sec: 41398.34\r\n",
      "step:   702 | loss: 22.319918 | lr: 5.8993e-04 | norm: 7.3773 | time: 12671.57ms | tok/sec: 41375.14\r\n",
      "step:   703 | loss: 22.815872 | lr: 5.9077e-04 | norm: 7.2686 | time: 12658.43ms | tok/sec: 41418.09\r\n",
      "step:   704 | loss: 22.671156 | lr: 5.9161e-04 | norm: 6.9745 | time: 12656.61ms | tok/sec: 41424.05\r\n",
      "step:   705 | loss: 22.413712 | lr: 5.9245e-04 | norm: 7.6715 | time: 12658.55ms | tok/sec: 41417.68\r\n",
      "step:   706 | loss: 22.884338 | lr: 5.9329e-04 | norm: 7.5502 | time: 12673.92ms | tok/sec: 41367.48\r\n",
      "step:   707 | loss: 23.417126 | lr: 5.9413e-04 | norm: 8.1817 | time: 12665.42ms | tok/sec: 41395.22\r\n",
      "step:   708 | loss: 23.199123 | lr: 5.9497e-04 | norm: 7.9531 | time: 12676.54ms | tok/sec: 41358.91\r\n",
      "step:   709 | loss: 23.195892 | lr: 5.9580e-04 | norm: 8.5175 | time: 12658.15ms | tok/sec: 41419.01\r\n",
      "step:   710 | loss: 22.577290 | lr: 5.9664e-04 | norm: 7.1096 | time: 12647.72ms | tok/sec: 41453.16\r\n",
      "step:   711 | loss: 22.591904 | lr: 5.9748e-04 | norm: 7.5691 | time: 12664.25ms | tok/sec: 41399.06\r\n",
      "step:   712 | loss: 23.189655 | lr: 5.9832e-04 | norm: 8.0612 | time: 12666.96ms | tok/sec: 41390.21\r\n",
      "step:   713 | loss: 23.785755 | lr: 5.9916e-04 | norm: 7.9837 | time: 12674.76ms | tok/sec: 41364.74\r\n",
      "step:   714 | loss: 22.838650 | lr: 6.0000e-04 | norm: 7.9682 | time: 12663.20ms | tok/sec: 41402.49\r\n",
      "step:   715 | loss: 22.452446 | lr: 6.0000e-04 | norm: 7.9139 | time: 12664.46ms | tok/sec: 41398.36\r\n",
      "step:   716 | loss: 22.653694 | lr: 5.9998e-04 | norm: 7.9011 | time: 12652.70ms | tok/sec: 41436.86\r\n",
      "step:   717 | loss: 23.612709 | lr: 5.9993e-04 | norm: 7.5763 | time: 12658.51ms | tok/sec: 41417.83\r\n",
      "step:   718 | loss: 23.699940 | lr: 5.9985e-04 | norm: 9.1407 | time: 12662.78ms | tok/sec: 41403.87\r\n",
      "step:   719 | loss: 23.389435 | lr: 5.9974e-04 | norm: 7.7272 | time: 12639.98ms | tok/sec: 41478.54\r\n",
      "step:   720 | loss: 23.595730 | lr: 5.9959e-04 | norm: 7.7837 | time: 12654.73ms | tok/sec: 41430.19\r\n",
      "step:   721 | loss: 23.345131 | lr: 5.9941e-04 | norm: 8.1129 | time: 12637.35ms | tok/sec: 41487.18\r\n",
      "step:   722 | loss: 23.617577 | lr: 5.9920e-04 | norm: 7.8937 | time: 12652.95ms | tok/sec: 41436.03\r\n",
      "step:   723 | loss: 23.646757 | lr: 5.9895e-04 | norm: 8.8302 | time: 12646.50ms | tok/sec: 41457.15\r\n",
      "step:   724 | loss: 23.810690 | lr: 5.9867e-04 | norm: 8.5286 | time: 12646.98ms | tok/sec: 41455.57\r\n",
      "step:   725 | loss: 24.247543 | lr: 5.9836e-04 | norm: 8.9821 | time: 12646.73ms | tok/sec: 41456.41\r\n",
      "step:   726 | loss: 23.978338 | lr: 5.9802e-04 | norm: 8.7714 | time: 12652.56ms | tok/sec: 41437.30\r\n",
      "step:   727 | loss: 23.717232 | lr: 5.9764e-04 | norm: 9.0384 | time: 12665.70ms | tok/sec: 41394.32\r\n",
      "step:   728 | loss: 23.849350 | lr: 5.9723e-04 | norm: 8.2165 | time: 12666.04ms | tok/sec: 41393.20\r\n",
      "step:   729 | loss: 24.627911 | lr: 5.9679e-04 | norm: 9.2145 | time: 12673.90ms | tok/sec: 41367.53\r\n",
      "step:   730 | loss: 24.258072 | lr: 5.9632e-04 | norm: 8.2123 | time: 12666.55ms | tok/sec: 41391.54\r\n",
      "step:   731 | loss: 24.503317 | lr: 5.9581e-04 | norm: 8.3326 | time: 12672.37ms | tok/sec: 41372.54\r\n",
      "step:   732 | loss: 24.005613 | lr: 5.9527e-04 | norm: 7.7721 | time: 12671.51ms | tok/sec: 41375.34\r\n",
      "step:   733 | loss: 24.013878 | lr: 5.9470e-04 | norm: 8.7604 | time: 12659.19ms | tok/sec: 41415.60\r\n",
      "step:   734 | loss: 23.590784 | lr: 5.9410e-04 | norm: 8.9853 | time: 12666.51ms | tok/sec: 41391.66\r\n",
      "step:   735 | loss: 23.880135 | lr: 5.9347e-04 | norm: 8.4265 | time: 12670.03ms | tok/sec: 41380.16\r\n",
      "step:   736 | loss: 23.835804 | lr: 5.9280e-04 | norm: 9.8545 | time: 12668.45ms | tok/sec: 41385.34\r\n",
      "step:   737 | loss: 24.219519 | lr: 5.9210e-04 | norm: 8.1818 | time: 12658.03ms | tok/sec: 41419.40\r\n",
      "step:   738 | loss: 24.138435 | lr: 5.9137e-04 | norm: 9.1165 | time: 12665.36ms | tok/sec: 41395.44\r\n",
      "step:   739 | loss: 23.561274 | lr: 5.9061e-04 | norm: 8.6945 | time: 12661.70ms | tok/sec: 41407.40\r\n",
      "step:   740 | loss: 23.836222 | lr: 5.8981e-04 | norm: 9.0571 | time: 12676.25ms | tok/sec: 41359.87\r\n",
      "step:   741 | loss: 24.924988 | lr: 5.8899e-04 | norm: 8.3446 | time: 12661.85ms | tok/sec: 41406.89\r\n",
      "step:   742 | loss: 24.463013 | lr: 5.8813e-04 | norm: 8.1514 | time: 12650.58ms | tok/sec: 41443.79\r\n",
      "step:   743 | loss: 24.571892 | lr: 5.8724e-04 | norm: 8.2537 | time: 12652.95ms | tok/sec: 41436.02\r\n",
      "step:   744 | loss: 25.861094 | lr: 5.8632e-04 | norm: 8.6707 | time: 12652.50ms | tok/sec: 41437.49\r\n",
      "step:   745 | loss: 26.090065 | lr: 5.8537e-04 | norm: 7.0044 | time: 12647.53ms | tok/sec: 41453.78\r\n",
      "step:   746 | loss: 25.399521 | lr: 5.8439e-04 | norm: 8.8535 | time: 12647.52ms | tok/sec: 41453.81\r\n",
      "step:   747 | loss: 25.802191 | lr: 5.8338e-04 | norm: 7.6080 | time: 12652.85ms | tok/sec: 41436.36\r\n",
      "step:   748 | loss: 26.404816 | lr: 5.8233e-04 | norm: 8.1204 | time: 12641.05ms | tok/sec: 41475.04\r\n",
      "step:   749 | loss: 26.506886 | lr: 5.8126e-04 | norm: 8.9928 | time: 12645.02ms | tok/sec: 41462.02\r\n",
      "validation loss: 26.1655\r\n",
      "step:   750 | loss: 26.209084 | lr: 5.8015e-04 | norm: 8.5151 | time: 17448.78ms | tok/sec: 30047.25\r\n",
      "step:   751 | loss: 26.491024 | lr: 5.7902e-04 | norm: 8.0701 | time: 12678.02ms | tok/sec: 41354.08\r\n",
      "step:   752 | loss: 26.432020 | lr: 5.7785e-04 | norm: 7.4056 | time: 12679.27ms | tok/sec: 41350.02\r\n",
      "step:   753 | loss: 25.808908 | lr: 5.7666e-04 | norm: 8.0991 | time: 12669.50ms | tok/sec: 41381.90\r\n",
      "step:   754 | loss: 26.141525 | lr: 5.7543e-04 | norm: 8.4129 | time: 12648.60ms | tok/sec: 41450.26\r\n",
      "step:   755 | loss: 26.233753 | lr: 5.7418e-04 | norm: 7.2703 | time: 12633.29ms | tok/sec: 41500.52\r\n",
      "step:   756 | loss: 26.354202 | lr: 5.7289e-04 | norm: 8.0547 | time: 12626.34ms | tok/sec: 41523.36\r\n",
      "step:   757 | loss: 25.868456 | lr: 5.7158e-04 | norm: 8.8053 | time: 12653.43ms | tok/sec: 41434.46\r\n",
      "step:   758 | loss: 25.747078 | lr: 5.7023e-04 | norm: 8.1771 | time: 12647.32ms | tok/sec: 41454.49\r\n",
      "step:   759 | loss: 26.499624 | lr: 5.6886e-04 | norm: 9.2276 | time: 12658.89ms | tok/sec: 41416.57\r\n",
      "step:   760 | loss: 26.325409 | lr: 5.6746e-04 | norm: 8.3361 | time: 12646.48ms | tok/sec: 41457.21\r\n",
      "step:   761 | loss: 26.333334 | lr: 5.6603e-04 | norm: 9.3560 | time: 12623.97ms | tok/sec: 41531.14\r\n",
      "step:   762 | loss: 26.687769 | lr: 5.6457e-04 | norm: 7.9846 | time: 13795.43ms | tok/sec: 38004.48\r\n",
      "step:   763 | loss: 27.136791 | lr: 5.6308e-04 | norm: 9.2940 | time: 12634.22ms | tok/sec: 41497.47\r\n",
      "step:   764 | loss: 26.415506 | lr: 5.6156e-04 | norm: 8.9309 | time: 12654.76ms | tok/sec: 41430.09\r\n",
      "step:   765 | loss: 28.198689 | lr: 5.6002e-04 | norm: 8.3628 | time: 12667.61ms | tok/sec: 41388.06\r\n",
      "step:   766 | loss: 26.551311 | lr: 5.5845e-04 | norm: 8.3534 | time: 12699.08ms | tok/sec: 41285.51\r\n",
      "step:   767 | loss: 26.697254 | lr: 5.5685e-04 | norm: 9.2292 | time: 12700.62ms | tok/sec: 41280.50\r\n",
      "step:   768 | loss: 26.786575 | lr: 5.5522e-04 | norm: 8.7437 | time: 12683.71ms | tok/sec: 41335.55\r\n",
      "step:   769 | loss: 26.269663 | lr: 5.5356e-04 | norm: 9.1347 | time: 12676.28ms | tok/sec: 41359.76\r\n",
      "step:   770 | loss: 26.582157 | lr: 5.5188e-04 | norm: 8.7428 | time: 12674.43ms | tok/sec: 41365.80\r\n",
      "step:   771 | loss: 27.306513 | lr: 5.5017e-04 | norm: 8.7118 | time: 12658.42ms | tok/sec: 41418.13\r\n",
      "step:   772 | loss: 27.053837 | lr: 5.4843e-04 | norm: 8.8129 | time: 12669.01ms | tok/sec: 41383.51\r\n",
      "step:   773 | loss: 26.697847 | lr: 5.4667e-04 | norm: 8.5721 | time: 12676.06ms | tok/sec: 41360.49\r\n",
      "step:   774 | loss: 26.827826 | lr: 5.4488e-04 | norm: 9.3468 | time: 12674.12ms | tok/sec: 41366.82\r\n",
      "step:   775 | loss: 26.936504 | lr: 5.4307e-04 | norm: 9.5883 | time: 12670.98ms | tok/sec: 41377.06\r\n",
      "step:   776 | loss: 26.993580 | lr: 5.4123e-04 | norm: 8.4306 | time: 12647.42ms | tok/sec: 41454.15\r\n",
      "step:   777 | loss: 27.011755 | lr: 5.3936e-04 | norm: 8.7901 | time: 12660.97ms | tok/sec: 41409.77\r\n",
      "step:   778 | loss: 26.726643 | lr: 5.3747e-04 | norm: 9.0179 | time: 12658.81ms | tok/sec: 41416.84\r\n",
      "step:   779 | loss: 27.707521 | lr: 5.3555e-04 | norm: 8.2830 | time: 12669.59ms | tok/sec: 41381.62\r\n",
      "step:   780 | loss: 26.763412 | lr: 5.3361e-04 | norm: 8.4719 | time: 12655.72ms | tok/sec: 41426.95\r\n",
      "step:   781 | loss: 27.061077 | lr: 5.3164e-04 | norm: 9.0303 | time: 12673.06ms | tok/sec: 41370.27\r\n",
      "step:   782 | loss: 26.575653 | lr: 5.2965e-04 | norm: 9.0375 | time: 12660.50ms | tok/sec: 41411.31\r\n",
      "step:   783 | loss: 27.097727 | lr: 5.2763e-04 | norm: 8.1482 | time: 12655.55ms | tok/sec: 41427.51\r\n",
      "step:   784 | loss: 27.140404 | lr: 5.2559e-04 | norm: 9.1234 | time: 12652.11ms | tok/sec: 41438.78\r\n",
      "step:   785 | loss: 28.871010 | lr: 5.2353e-04 | norm: 8.6719 | time: 12640.90ms | tok/sec: 41475.54\r\n",
      "step:   786 | loss: 26.745720 | lr: 5.2144e-04 | norm: 8.5009 | time: 12629.64ms | tok/sec: 41512.49\r\n",
      "step:   787 | loss: 26.973221 | lr: 5.1933e-04 | norm: 9.6331 | time: 12639.59ms | tok/sec: 41479.83\r\n",
      "step:   788 | loss: 27.324940 | lr: 5.1720e-04 | norm: 8.5390 | time: 12635.21ms | tok/sec: 41494.21\r\n",
      "step:   789 | loss: 27.467203 | lr: 5.1504e-04 | norm: 9.8837 | time: 12671.76ms | tok/sec: 41374.53\r\n",
      "step:   790 | loss: 29.125517 | lr: 5.1287e-04 | norm: 8.5019 | time: 12649.08ms | tok/sec: 41448.69\r\n",
      "step:   791 | loss: 29.833765 | lr: 5.1067e-04 | norm: 8.7998 | time: 12651.14ms | tok/sec: 41441.95\r\n",
      "step:   792 | loss: 29.239910 | lr: 5.0844e-04 | norm: 8.3133 | time: 12632.69ms | tok/sec: 41502.47\r\n",
      "step:   793 | loss: 29.604309 | lr: 5.0620e-04 | norm: 8.7958 | time: 12669.21ms | tok/sec: 41382.84\r\n",
      "step:   794 | loss: 29.161655 | lr: 5.0393e-04 | norm: 8.6642 | time: 12658.26ms | tok/sec: 41418.66\r\n",
      "step:   795 | loss: 31.846581 | lr: 5.0165e-04 | norm: 8.3250 | time: 12651.54ms | tok/sec: 41440.63\r\n",
      "step:   796 | loss: 29.432770 | lr: 4.9934e-04 | norm: 8.2801 | time: 12648.82ms | tok/sec: 41449.55\r\n",
      "step:   797 | loss: 27.892990 | lr: 4.9701e-04 | norm: 8.7410 | time: 12651.75ms | tok/sec: 41439.96\r\n",
      "step:   798 | loss: 28.683319 | lr: 4.9466e-04 | norm: 8.5836 | time: 12637.10ms | tok/sec: 41488.00\r\n",
      "step:   799 | loss: 30.342529 | lr: 4.9229e-04 | norm: 8.5498 | time: 12639.27ms | tok/sec: 41480.87\r\n",
      "step:   800 | loss: 29.748478 | lr: 4.8990e-04 | norm: 9.1159 | time: 12648.54ms | tok/sec: 41450.47\r\n",
      "step:   801 | loss: 29.130932 | lr: 4.8750e-04 | norm: 9.0112 | time: 12648.47ms | tok/sec: 41450.71\r\n",
      "step:   802 | loss: 28.850294 | lr: 4.8507e-04 | norm: 8.2013 | time: 12626.05ms | tok/sec: 41524.32\r\n",
      "step:   803 | loss: 29.519508 | lr: 4.8262e-04 | norm: 9.0116 | time: 12651.00ms | tok/sec: 41442.41\r\n",
      "step:   804 | loss: 28.862339 | lr: 4.8016e-04 | norm: 9.4705 | time: 12647.63ms | tok/sec: 41453.47\r\n",
      "step:   805 | loss: 29.019413 | lr: 4.7768e-04 | norm: 9.0953 | time: 12650.29ms | tok/sec: 41444.73\r\n",
      "step:   806 | loss: 29.012455 | lr: 4.7518e-04 | norm: 9.0173 | time: 12634.05ms | tok/sec: 41498.02\r\n",
      "step:   807 | loss: 29.805176 | lr: 4.7266e-04 | norm: 9.8376 | time: 12658.30ms | tok/sec: 41418.53\r\n",
      "step:   808 | loss: 29.921432 | lr: 4.7012e-04 | norm: 9.4944 | time: 12651.14ms | tok/sec: 41441.97\r\n",
      "step:   809 | loss: 28.748379 | lr: 4.6757e-04 | norm: 9.0752 | time: 12635.93ms | tok/sec: 41491.86\r\n",
      "step:   810 | loss: 29.994312 | lr: 4.6500e-04 | norm: 9.3186 | time: 12651.36ms | tok/sec: 41441.24\r\n",
      "step:   811 | loss: 28.981739 | lr: 4.6241e-04 | norm: 10.3273 | time: 12639.10ms | tok/sec: 41481.43\r\n",
      "step:   812 | loss: 29.003586 | lr: 4.5981e-04 | norm: 10.2280 | time: 12658.01ms | tok/sec: 41419.46\r\n",
      "step:   813 | loss: 29.248495 | lr: 4.5720e-04 | norm: 9.6808 | time: 12659.55ms | tok/sec: 41414.44\r\n",
      "step:   814 | loss: 28.981056 | lr: 4.5456e-04 | norm: 9.4782 | time: 12642.71ms | tok/sec: 41469.58\r\n",
      "step:   815 | loss: 30.102320 | lr: 4.5191e-04 | norm: 8.4045 | time: 12641.72ms | tok/sec: 41472.84\r\n",
      "step:   816 | loss: 29.097427 | lr: 4.4925e-04 | norm: 10.1749 | time: 12660.33ms | tok/sec: 41411.87\r\n",
      "step:   817 | loss: 29.220354 | lr: 4.4657e-04 | norm: 10.0253 | time: 12656.47ms | tok/sec: 41424.50\r\n",
      "step:   818 | loss: 29.233719 | lr: 4.4388e-04 | norm: 9.6230 | time: 12659.30ms | tok/sec: 41415.23\r\n",
      "step:   819 | loss: 29.377052 | lr: 4.4118e-04 | norm: 9.5644 | time: 12664.43ms | tok/sec: 41398.47\r\n",
      "step:   820 | loss: 30.171307 | lr: 4.3846e-04 | norm: 9.4084 | time: 12648.31ms | tok/sec: 41451.22\r\n",
      "step:   821 | loss: 29.382574 | lr: 4.3573e-04 | norm: 8.9705 | time: 12655.38ms | tok/sec: 41428.08\r\n",
      "step:   822 | loss: 30.646507 | lr: 4.3298e-04 | norm: 9.9085 | time: 12668.90ms | tok/sec: 41383.85\r\n",
      "step:   823 | loss: 29.718576 | lr: 4.3022e-04 | norm: 9.9567 | time: 12660.87ms | tok/sec: 41410.12\r\n",
      "step:   824 | loss: 29.115561 | lr: 4.2745e-04 | norm: 8.8265 | time: 12653.86ms | tok/sec: 41433.04\r\n",
      "step:   825 | loss: 29.466320 | lr: 4.2467e-04 | norm: 9.8715 | time: 12658.00ms | tok/sec: 41419.50\r\n",
      "step:   826 | loss: 29.289230 | lr: 4.2188e-04 | norm: 9.8512 | time: 12657.87ms | tok/sec: 41419.93\r\n",
      "step:   827 | loss: 30.076818 | lr: 4.1908e-04 | norm: 9.3152 | time: 12645.34ms | tok/sec: 41460.95\r\n",
      "step:   828 | loss: 29.902277 | lr: 4.1626e-04 | norm: 8.9920 | time: 12649.51ms | tok/sec: 41447.29\r\n",
      "step:   829 | loss: 30.329260 | lr: 4.1343e-04 | norm: 9.2727 | time: 12655.26ms | tok/sec: 41428.45\r\n",
      "step:   830 | loss: 29.053215 | lr: 4.1060e-04 | norm: 9.2103 | time: 12662.89ms | tok/sec: 41403.49\r\n",
      "step:   831 | loss: 29.615406 | lr: 4.0775e-04 | norm: 9.7884 | time: 12646.05ms | tok/sec: 41458.65\r\n",
      "step:   832 | loss: 29.734180 | lr: 4.0490e-04 | norm: 9.3805 | time: 12639.97ms | tok/sec: 41478.59\r\n",
      "step:   833 | loss: 29.673134 | lr: 4.0203e-04 | norm: 8.8851 | time: 12627.77ms | tok/sec: 41518.65\r\n",
      "step:   834 | loss: 30.134544 | lr: 3.9916e-04 | norm: 9.2343 | time: 12633.37ms | tok/sec: 41500.24\r\n",
      "step:   835 | loss: 30.862936 | lr: 3.9628e-04 | norm: 9.2417 | time: 12630.63ms | tok/sec: 41509.24\r\n",
      "step:   836 | loss: 31.348785 | lr: 3.9339e-04 | norm: 9.1157 | time: 12615.77ms | tok/sec: 41558.15\r\n",
      "step:   837 | loss: 31.439999 | lr: 3.9050e-04 | norm: 8.4642 | time: 12611.50ms | tok/sec: 41572.22\r\n",
      "step:   838 | loss: 31.566772 | lr: 3.8759e-04 | norm: 9.8001 | time: 12633.72ms | tok/sec: 41499.10\r\n",
      "step:   839 | loss: 32.295216 | lr: 3.8468e-04 | norm: 9.7249 | time: 12638.79ms | tok/sec: 41482.44\r\n",
      "step:   840 | loss: 31.048832 | lr: 3.8176e-04 | norm: 9.7051 | time: 12619.20ms | tok/sec: 41546.85\r\n",
      "step:   841 | loss: 32.432236 | lr: 3.7884e-04 | norm: 8.9210 | time: 12617.11ms | tok/sec: 41553.74\r\n",
      "step:   842 | loss: 31.839743 | lr: 3.7591e-04 | norm: 9.6650 | time: 12620.06ms | tok/sec: 41544.01\r\n",
      "step:   843 | loss: 31.227245 | lr: 3.7297e-04 | norm: 9.5857 | time: 12633.33ms | tok/sec: 41500.37\r\n",
      "step:   844 | loss: 38.996552 | lr: 3.7003e-04 | norm: 9.4181 | time: 12616.03ms | tok/sec: 41557.27\r\n",
      "step:   845 | loss: 31.747826 | lr: 3.6709e-04 | norm: 8.9169 | time: 12617.48ms | tok/sec: 41552.53\r\n",
      "step:   846 | loss: 31.437840 | lr: 3.6414e-04 | norm: 8.6763 | time: 12611.33ms | tok/sec: 41572.78\r\n",
      "step:   847 | loss: 31.741657 | lr: 3.6118e-04 | norm: 9.8875 | time: 12625.60ms | tok/sec: 41525.78\r\n",
      "step:   848 | loss: 30.798851 | lr: 3.5822e-04 | norm: 9.4138 | time: 12599.61ms | tok/sec: 41611.46\r\n",
      "step:   849 | loss: 31.645889 | lr: 3.5526e-04 | norm: 8.5858 | time: 12573.49ms | tok/sec: 41697.88\r\n",
      "step:   850 | loss: 31.768635 | lr: 3.5230e-04 | norm: 9.5799 | time: 12619.57ms | tok/sec: 41545.63\r\n",
      "step:   851 | loss: 31.445728 | lr: 3.4933e-04 | norm: 9.4904 | time: 12606.42ms | tok/sec: 41588.96\r\n",
      "step:   852 | loss: 32.147263 | lr: 3.4636e-04 | norm: 9.7699 | time: 12615.55ms | tok/sec: 41558.87\r\n",
      "step:   853 | loss: 30.947723 | lr: 3.4339e-04 | norm: 9.7893 | time: 12631.87ms | tok/sec: 41505.18\r\n",
      "step:   854 | loss: 32.105278 | lr: 3.4041e-04 | norm: 9.6233 | time: 12614.13ms | tok/sec: 41563.55\r\n",
      "step:   855 | loss: 31.763597 | lr: 3.3744e-04 | norm: 9.4733 | time: 12606.03ms | tok/sec: 41590.25\r\n",
      "step:   856 | loss: 31.615070 | lr: 3.3446e-04 | norm: 9.6540 | time: 12623.57ms | tok/sec: 41532.47\r\n",
      "step:   857 | loss: 32.362709 | lr: 3.3149e-04 | norm: 9.7274 | time: 12632.60ms | tok/sec: 41502.79\r\n",
      "step:   858 | loss: 31.518579 | lr: 3.2851e-04 | norm: 8.8744 | time: 12603.44ms | tok/sec: 41598.82\r\n",
      "step:   859 | loss: 31.249929 | lr: 3.2554e-04 | norm: 8.8566 | time: 12614.79ms | tok/sec: 41561.37\r\n",
      "step:   860 | loss: 31.503269 | lr: 3.2256e-04 | norm: 9.1561 | time: 12624.14ms | tok/sec: 41530.59\r\n",
      "step:   861 | loss: 31.441755 | lr: 3.1959e-04 | norm: 8.4130 | time: 12602.70ms | tok/sec: 41601.24\r\n",
      "step:   862 | loss: 31.449265 | lr: 3.1661e-04 | norm: 9.3499 | time: 12607.52ms | tok/sec: 41585.34\r\n",
      "step:   863 | loss: 31.782986 | lr: 3.1364e-04 | norm: 9.7496 | time: 12642.38ms | tok/sec: 41470.66\r\n",
      "step:   864 | loss: 32.768478 | lr: 3.1067e-04 | norm: 9.4671 | time: 12619.46ms | tok/sec: 41545.98\r\n",
      "step:   865 | loss: 32.299171 | lr: 3.0770e-04 | norm: 9.2787 | time: 12621.04ms | tok/sec: 41540.79\r\n",
      "step:   866 | loss: 31.853552 | lr: 3.0474e-04 | norm: 9.5454 | time: 12604.46ms | tok/sec: 41595.45\r\n",
      "step:   867 | loss: 31.514053 | lr: 3.0178e-04 | norm: 8.9242 | time: 12598.73ms | tok/sec: 41614.36\r\n",
      "step:   868 | loss: 31.593716 | lr: 2.9882e-04 | norm: 9.7742 | time: 12594.47ms | tok/sec: 41628.42\r\n",
      "step:   869 | loss: 31.380789 | lr: 2.9586e-04 | norm: 9.3853 | time: 12587.25ms | tok/sec: 41652.31\r\n",
      "step:   870 | loss: 32.208847 | lr: 2.9291e-04 | norm: 9.3023 | time: 12587.09ms | tok/sec: 41652.85\r\n",
      "step:   871 | loss: 31.162752 | lr: 2.8997e-04 | norm: 9.3495 | time: 12589.42ms | tok/sec: 41645.14\r\n",
      "step:   872 | loss: 31.805346 | lr: 2.8703e-04 | norm: 9.3344 | time: 12577.46ms | tok/sec: 41684.73\r\n",
      "step:   873 | loss: 30.977062 | lr: 2.8409e-04 | norm: 9.6856 | time: 12596.52ms | tok/sec: 41621.67\r\n",
      "step:   874 | loss: 31.131870 | lr: 2.8116e-04 | norm: 9.9916 | time: 12602.92ms | tok/sec: 41600.51\r\n",
      "step:   875 | loss: 31.839153 | lr: 2.7824e-04 | norm: 9.4354 | time: 12606.61ms | tok/sec: 41588.34\r\n",
      "step:   876 | loss: 32.147579 | lr: 2.7532e-04 | norm: 9.8204 | time: 12601.02ms | tok/sec: 41606.79\r\n",
      "step:   877 | loss: 32.479248 | lr: 2.7241e-04 | norm: 9.8518 | time: 12606.39ms | tok/sec: 41589.06\r\n",
      "step:   878 | loss: 31.702484 | lr: 2.6950e-04 | norm: 10.0672 | time: 12604.36ms | tok/sec: 41595.77\r\n",
      "step:   879 | loss: 31.464653 | lr: 2.6661e-04 | norm: 7.7911 | time: 12597.35ms | tok/sec: 41618.93\r\n",
      "step:   880 | loss: 30.557335 | lr: 2.6372e-04 | norm: 9.8237 | time: 12600.68ms | tok/sec: 41607.90\r\n",
      "step:   881 | loss: 32.493488 | lr: 2.6084e-04 | norm: 10.0813 | time: 12603.50ms | tok/sec: 41598.61\r\n",
      "step:   882 | loss: 32.553368 | lr: 2.5797e-04 | norm: 8.0428 | time: 12594.79ms | tok/sec: 41627.38\r\n",
      "step:   883 | loss: 33.529831 | lr: 2.5510e-04 | norm: 9.6578 | time: 12607.75ms | tok/sec: 41584.59\r\n",
      "step:   884 | loss: 32.325851 | lr: 2.5225e-04 | norm: 8.3210 | time: 12600.01ms | tok/sec: 41610.12\r\n",
      "step:   885 | loss: 33.847160 | lr: 2.4940e-04 | norm: 9.2114 | time: 12615.61ms | tok/sec: 41558.66\r\n",
      "step:   886 | loss: 32.515488 | lr: 2.4657e-04 | norm: 8.3748 | time: 12607.71ms | tok/sec: 41584.73\r\n",
      "step:   887 | loss: 33.257355 | lr: 2.4374e-04 | norm: 9.0759 | time: 12615.86ms | tok/sec: 41557.86\r\n",
      "step:   888 | loss: 33.700203 | lr: 2.4092e-04 | norm: 9.7743 | time: 12610.39ms | tok/sec: 41575.87\r\n",
      "step:   889 | loss: 34.205143 | lr: 2.3812e-04 | norm: 9.0598 | time: 12594.23ms | tok/sec: 41629.22\r\n",
      "step:   890 | loss: 33.317265 | lr: 2.3533e-04 | norm: 9.4455 | time: 12610.26ms | tok/sec: 41576.29\r\n",
      "step:   891 | loss: 33.286930 | lr: 2.3255e-04 | norm: 9.8596 | time: 12615.91ms | tok/sec: 41557.70\r\n",
      "step:   892 | loss: 32.853516 | lr: 2.2978e-04 | norm: 9.7781 | time: 12602.16ms | tok/sec: 41603.02\r\n",
      "step:   893 | loss: 32.704460 | lr: 2.2702e-04 | norm: 9.4225 | time: 12591.67ms | tok/sec: 41637.67\r\n",
      "step:   894 | loss: 32.935417 | lr: 2.2427e-04 | norm: 9.9715 | time: 12610.30ms | tok/sec: 41576.16\r\n",
      "step:   895 | loss: 34.121681 | lr: 2.2154e-04 | norm: 9.3721 | time: 12603.28ms | tok/sec: 41599.34\r\n",
      "step:   896 | loss: 33.489822 | lr: 2.1882e-04 | norm: 8.7025 | time: 12597.12ms | tok/sec: 41619.68\r\n",
      "step:   897 | loss: 33.019539 | lr: 2.1612e-04 | norm: 9.9960 | time: 12595.90ms | tok/sec: 41623.72\r\n",
      "step:   898 | loss: 33.851582 | lr: 2.1343e-04 | norm: 9.9182 | time: 12593.32ms | tok/sec: 41632.23\r\n",
      "step:   899 | loss: 32.670143 | lr: 2.1075e-04 | norm: 10.2015 | time: 12603.50ms | tok/sec: 41598.61\r\n",
      "step:   900 | loss: 32.350246 | lr: 2.0809e-04 | norm: 10.1244 | time: 12604.81ms | tok/sec: 41594.28\r\n",
      "step:   901 | loss: 31.816788 | lr: 2.0544e-04 | norm: 8.4838 | time: 12595.28ms | tok/sec: 41625.74\r\n",
      "step:   902 | loss: 33.976871 | lr: 2.0280e-04 | norm: 10.0071 | time: 12599.08ms | tok/sec: 41613.20\r\n",
      "step:   903 | loss: 32.744308 | lr: 2.0019e-04 | norm: 10.2267 | time: 12593.47ms | tok/sec: 41631.75\r\n",
      "step:   904 | loss: 32.786659 | lr: 1.9759e-04 | norm: 9.5797 | time: 12579.00ms | tok/sec: 41679.62\r\n",
      "step:   905 | loss: 32.770683 | lr: 1.9500e-04 | norm: 9.0708 | time: 12599.06ms | tok/sec: 41613.28\r\n",
      "step:   906 | loss: 32.554138 | lr: 1.9243e-04 | norm: 10.2587 | time: 12588.31ms | tok/sec: 41648.79\r\n",
      "step:   907 | loss: 34.132080 | lr: 1.8988e-04 | norm: 9.8953 | time: 12585.03ms | tok/sec: 41659.66\r\n",
      "step:   908 | loss: 33.487782 | lr: 1.8734e-04 | norm: 9.3795 | time: 12585.42ms | tok/sec: 41658.36\r\n",
      "step:   909 | loss: 32.436203 | lr: 1.8482e-04 | norm: 9.0363 | time: 12599.84ms | tok/sec: 41610.69\r\n",
      "step:   910 | loss: 32.425423 | lr: 1.8232e-04 | norm: 8.3939 | time: 12597.17ms | tok/sec: 41619.50\r\n",
      "step:   911 | loss: 32.535988 | lr: 1.7984e-04 | norm: 10.1864 | time: 12602.09ms | tok/sec: 41603.26\r\n",
      "step:   912 | loss: 32.601955 | lr: 1.7738e-04 | norm: 9.7227 | time: 12595.75ms | tok/sec: 41624.20\r\n",
      "step:   913 | loss: 32.649727 | lr: 1.7493e-04 | norm: 9.6223 | time: 12592.05ms | tok/sec: 41636.44\r\n",
      "step:   914 | loss: 33.158241 | lr: 1.7250e-04 | norm: 9.4362 | time: 12593.06ms | tok/sec: 41633.08\r\n",
      "step:   915 | loss: 31.530598 | lr: 1.7010e-04 | norm: 9.2402 | time: 12595.46ms | tok/sec: 41625.15\r\n",
      "step:   916 | loss: 32.801033 | lr: 1.6771e-04 | norm: 8.8867 | time: 12601.34ms | tok/sec: 41605.74\r\n",
      "step:   917 | loss: 32.636379 | lr: 1.6534e-04 | norm: 9.5116 | time: 12596.12ms | tok/sec: 41622.97\r\n",
      "step:   918 | loss: 32.510323 | lr: 1.6299e-04 | norm: 8.7999 | time: 12610.75ms | tok/sec: 41574.69\r\n",
      "step:   919 | loss: 32.954674 | lr: 1.6066e-04 | norm: 9.0301 | time: 12599.13ms | tok/sec: 41613.04\r\n",
      "step:   920 | loss: 31.858862 | lr: 1.5835e-04 | norm: 9.2676 | time: 12600.94ms | tok/sec: 41607.04\r\n",
      "step:   921 | loss: 31.636023 | lr: 1.5607e-04 | norm: 9.8349 | time: 12603.43ms | tok/sec: 41598.84\r\n",
      "step:   922 | loss: 31.829163 | lr: 1.5380e-04 | norm: 9.9357 | time: 12609.98ms | tok/sec: 41577.21\r\n",
      "step:   923 | loss: 32.530457 | lr: 1.5156e-04 | norm: 10.3023 | time: 12599.76ms | tok/sec: 41610.97\r\n",
      "step:   924 | loss: 32.488716 | lr: 1.4933e-04 | norm: 10.1838 | time: 12610.00ms | tok/sec: 41577.17\r\n",
      "step:   925 | loss: 32.001312 | lr: 1.4713e-04 | norm: 10.3157 | time: 12598.94ms | tok/sec: 41613.67\r\n",
      "step:   926 | loss: 33.141670 | lr: 1.4496e-04 | norm: 10.3858 | time: 12595.50ms | tok/sec: 41625.01\r\n",
      "step:   927 | loss: 34.198753 | lr: 1.4280e-04 | norm: 10.3386 | time: 12592.51ms | tok/sec: 41634.90\r\n",
      "step:   928 | loss: 34.783859 | lr: 1.4067e-04 | norm: 10.2961 | time: 12575.95ms | tok/sec: 41689.74\r\n",
      "step:   929 | loss: 33.794445 | lr: 1.3856e-04 | norm: 9.6748 | time: 12572.19ms | tok/sec: 41702.19\r\n",
      "step:   930 | loss: 33.958557 | lr: 1.3647e-04 | norm: 9.7283 | time: 12583.44ms | tok/sec: 41664.92\r\n",
      "step:   931 | loss: 34.383095 | lr: 1.3441e-04 | norm: 9.9530 | time: 12605.98ms | tok/sec: 41590.41\r\n",
      "step:   932 | loss: 34.132690 | lr: 1.3237e-04 | norm: 10.3597 | time: 12595.65ms | tok/sec: 41624.52\r\n",
      "step:   933 | loss: 34.654427 | lr: 1.3035e-04 | norm: 10.3661 | time: 12594.57ms | tok/sec: 41628.10\r\n",
      "step:   934 | loss: 34.887497 | lr: 1.2836e-04 | norm: 8.4681 | time: 12587.23ms | tok/sec: 41652.37\r\n",
      "step:   935 | loss: 34.338657 | lr: 1.2639e-04 | norm: 9.8112 | time: 12586.94ms | tok/sec: 41653.35\r\n",
      "step:   936 | loss: 34.583782 | lr: 1.2445e-04 | norm: 9.6725 | time: 12600.63ms | tok/sec: 41608.09\r\n",
      "step:   937 | loss: 33.883911 | lr: 1.2253e-04 | norm: 10.0123 | time: 12606.59ms | tok/sec: 41588.42\r\n",
      "step:   938 | loss: 32.942314 | lr: 1.2064e-04 | norm: 10.4437 | time: 12596.44ms | tok/sec: 41621.92\r\n",
      "step:   939 | loss: 33.435532 | lr: 1.1877e-04 | norm: 9.1326 | time: 12587.07ms | tok/sec: 41652.91\r\n",
      "step:   940 | loss: 34.686363 | lr: 1.1693e-04 | norm: 10.1675 | time: 12586.86ms | tok/sec: 41653.60\r\n",
      "step:   941 | loss: 32.768375 | lr: 1.1512e-04 | norm: 9.7612 | time: 12598.76ms | tok/sec: 41614.26\r\n",
      "step:   942 | loss: 34.225758 | lr: 1.1333e-04 | norm: 10.3838 | time: 12594.25ms | tok/sec: 41629.17\r\n",
      "step:   943 | loss: 33.336296 | lr: 1.1157e-04 | norm: 9.5987 | time: 12608.54ms | tok/sec: 41581.96\r\n",
      "step:   944 | loss: 33.222023 | lr: 1.0983e-04 | norm: 9.5682 | time: 12610.58ms | tok/sec: 41575.25\r\n",
      "step:   945 | loss: 34.591476 | lr: 1.0812e-04 | norm: 9.8162 | time: 12599.77ms | tok/sec: 41610.93\r\n",
      "step:   946 | loss: 33.257702 | lr: 1.0644e-04 | norm: 9.1640 | time: 12605.24ms | tok/sec: 41592.86\r\n",
      "step:   947 | loss: 34.901459 | lr: 1.0478e-04 | norm: 9.8809 | time: 12601.92ms | tok/sec: 41603.82\r\n",
      "step:   948 | loss: 33.008797 | lr: 1.0315e-04 | norm: 10.0552 | time: 12606.80ms | tok/sec: 41587.72\r\n",
      "step:   949 | loss: 34.585358 | lr: 1.0155e-04 | norm: 8.8004 | time: 12611.55ms | tok/sec: 41572.05\r\n",
      "step:   950 | loss: 32.916351 | lr: 9.9982e-05 | norm: 10.2798 | time: 12619.29ms | tok/sec: 41546.55\r\n",
      "step:   951 | loss: 34.466354 | lr: 9.8437e-05 | norm: 9.9931 | time: 12607.59ms | tok/sec: 41585.11\r\n",
      "step:   952 | loss: 33.551384 | lr: 9.6921e-05 | norm: 9.1557 | time: 12602.14ms | tok/sec: 41603.09\r\n",
      "step:   953 | loss: 32.960072 | lr: 9.5433e-05 | norm: 10.3991 | time: 13985.71ms | tok/sec: 37487.41\r\n",
      "step:   954 | loss: 33.521511 | lr: 9.3973e-05 | norm: 10.4417 | time: 12604.15ms | tok/sec: 41596.47\r\n",
      "step:   955 | loss: 33.330006 | lr: 9.2542e-05 | norm: 10.0848 | time: 12621.28ms | tok/sec: 41540.00\r\n",
      "step:   956 | loss: 32.820305 | lr: 9.1140e-05 | norm: 9.8220 | time: 12615.58ms | tok/sec: 41558.77\r\n",
      "step:   957 | loss: 33.816620 | lr: 8.9767e-05 | norm: 10.3478 | time: 12627.97ms | tok/sec: 41518.00\r\n",
      "step:   958 | loss: 34.865288 | lr: 8.8423e-05 | norm: 9.7419 | time: 12612.21ms | tok/sec: 41569.88\r\n",
      "step:   959 | loss: 33.600517 | lr: 8.7109e-05 | norm: 10.4149 | time: 12642.65ms | tok/sec: 41469.78\r\n",
      "step:   960 | loss: 31.935364 | lr: 8.5824e-05 | norm: 10.5344 | time: 12626.86ms | tok/sec: 41521.64\r\n",
      "step:   961 | loss: 34.236076 | lr: 8.4568e-05 | norm: 10.0123 | time: 12633.55ms | tok/sec: 41499.66\r\n",
      "step:   962 | loss: 33.247330 | lr: 8.3343e-05 | norm: 9.4442 | time: 12626.42ms | tok/sec: 41523.08\r\n",
      "step:   963 | loss: 32.622589 | lr: 8.2147e-05 | norm: 9.8497 | time: 12621.40ms | tok/sec: 41539.62\r\n",
      "step:   964 | loss: 31.348347 | lr: 8.0982e-05 | norm: 9.6068 | time: 12615.31ms | tok/sec: 41559.65\r\n",
      "step:   965 | loss: 33.385540 | lr: 7.9847e-05 | norm: 9.8127 | time: 12622.88ms | tok/sec: 41534.74\r\n",
      "step:   966 | loss: 32.955368 | lr: 7.8742e-05 | norm: 9.6445 | time: 12617.34ms | tok/sec: 41552.97\r\n",
      "step:   967 | loss: 33.306591 | lr: 7.7668e-05 | norm: 9.8358 | time: 12602.60ms | tok/sec: 41601.56\r\n",
      "step:   968 | loss: 32.402336 | lr: 7.6624e-05 | norm: 9.5486 | time: 12598.28ms | tok/sec: 41615.85\r\n",
      "step:   969 | loss: 33.416592 | lr: 7.5611e-05 | norm: 9.8412 | time: 12587.75ms | tok/sec: 41650.65\r\n",
      "step:   970 | loss: 32.438942 | lr: 7.4629e-05 | norm: 9.5723 | time: 12581.94ms | tok/sec: 41669.90\r\n",
      "step:   971 | loss: 32.312218 | lr: 7.3678e-05 | norm: 10.1424 | time: 12563.42ms | tok/sec: 41731.30\r\n",
      "step:   972 | loss: 33.678360 | lr: 7.2759e-05 | norm: 9.6355 | time: 12569.29ms | tok/sec: 41711.82\r\n",
      "step:   973 | loss: 33.623020 | lr: 7.1870e-05 | norm: 10.2118 | time: 12597.22ms | tok/sec: 41619.35\r\n",
      "step:   974 | loss: 34.924683 | lr: 7.1013e-05 | norm: 9.8379 | time: 12582.64ms | tok/sec: 41667.56\r\n",
      "step:   975 | loss: 34.339867 | lr: 7.0188e-05 | norm: 10.4938 | time: 12584.36ms | tok/sec: 41661.86\r\n",
      "step:   976 | loss: 35.258537 | lr: 6.9394e-05 | norm: 10.4753 | time: 12595.08ms | tok/sec: 41626.40\r\n",
      "step:   977 | loss: 34.330864 | lr: 6.8631e-05 | norm: 10.5704 | time: 12601.98ms | tok/sec: 41603.61\r\n",
      "step:   978 | loss: 34.808262 | lr: 6.7901e-05 | norm: 10.5051 | time: 12599.15ms | tok/sec: 41612.97\r\n",
      "step:   979 | loss: 35.251648 | lr: 6.7202e-05 | norm: 10.4962 | time: 12608.62ms | tok/sec: 41581.73\r\n",
      "step:   980 | loss: 34.164780 | lr: 6.6535e-05 | norm: 10.5326 | time: 12610.71ms | tok/sec: 41574.81\r\n",
      "step:   981 | loss: 33.982437 | lr: 6.5900e-05 | norm: 10.5291 | time: 12599.17ms | tok/sec: 41612.88\r\n",
      "step:   982 | loss: 34.324562 | lr: 6.5297e-05 | norm: 9.9708 | time: 12620.66ms | tok/sec: 41542.04\r\n",
      "step:   983 | loss: 33.985462 | lr: 6.4727e-05 | norm: 9.8182 | time: 12612.66ms | tok/sec: 41568.40\r\n",
      "step:   984 | loss: 34.575714 | lr: 6.4188e-05 | norm: 10.3178 | time: 12612.17ms | tok/sec: 41570.00\r\n",
      "step:   985 | loss: 34.030609 | lr: 6.3682e-05 | norm: 10.4926 | time: 12616.50ms | tok/sec: 41555.73\r\n",
      "step:   986 | loss: 34.529510 | lr: 6.3209e-05 | norm: 10.5249 | time: 12606.93ms | tok/sec: 41587.30\r\n",
      "step:   987 | loss: 33.986740 | lr: 6.2767e-05 | norm: 10.5499 | time: 12613.85ms | tok/sec: 41564.46\r\n",
      "step:   988 | loss: 34.254440 | lr: 6.2359e-05 | norm: 10.5108 | time: 12613.16ms | tok/sec: 41566.73\r\n",
      "step:   989 | loss: 35.255173 | lr: 6.1982e-05 | norm: 9.8139 | time: 12610.41ms | tok/sec: 41575.82\r\n",
      "step:   990 | loss: 33.453186 | lr: 6.1639e-05 | norm: 9.7446 | time: 12603.76ms | tok/sec: 41597.76\r\n",
      "step:   991 | loss: 34.324936 | lr: 6.1328e-05 | norm: 10.5198 | time: 12588.94ms | tok/sec: 41646.73\r\n",
      "step:   992 | loss: 34.471039 | lr: 6.1049e-05 | norm: 10.4659 | time: 12614.70ms | tok/sec: 41561.65\r\n",
      "step:   993 | loss: 33.616840 | lr: 6.0803e-05 | norm: 10.5754 | time: 12602.32ms | tok/sec: 41602.51\r\n",
      "step:   994 | loss: 36.834816 | lr: 6.0590e-05 | norm: 9.7767 | time: 12603.94ms | tok/sec: 41597.15\r\n",
      "step:   995 | loss: 34.375725 | lr: 6.0410e-05 | norm: 9.4634 | time: 12604.83ms | tok/sec: 41594.22\r\n",
      "step:   996 | loss: 33.041756 | lr: 6.0262e-05 | norm: 10.4092 | time: 12611.55ms | tok/sec: 41572.04\r\n",
      "step:   997 | loss: 33.760616 | lr: 6.0148e-05 | norm: 10.3731 | time: 12600.57ms | tok/sec: 41608.27\r\n",
      "step:   998 | loss: 33.193405 | lr: 6.0066e-05 | norm: 10.5967 | time: 12596.04ms | tok/sec: 41623.24\r\n",
      "validation loss: 34.8619\r\n",
      "step:   999 | loss: 33.947060 | lr: 6.0016e-05 | norm: 9.8840 | time: 16612.81ms | tok/sec: 31559.26\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263aa1a2",
   "metadata": {
    "papermill": {
     "duration": 3.282157,
     "end_time": "2024-07-07T20:34:49.321659",
     "exception": false,
     "start_time": "2024-07-07T20:34:46.039502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## If you want to run on a single device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e02c88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T20:34:55.827726Z",
     "iopub.status.busy": "2024-07-07T20:34:55.827297Z",
     "iopub.status.idle": "2024-07-07T20:34:55.832160Z",
     "shell.execute_reply": "2024-07-07T20:34:55.831221Z"
    },
    "papermill": {
     "duration": 3.257698,
     "end_time": "2024-07-07T20:34:55.834338",
     "exception": false,
     "start_time": "2024-07-07T20:34:52.576640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2373bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T20:35:02.502306Z",
     "iopub.status.busy": "2024-07-07T20:35:02.501940Z",
     "iopub.status.idle": "2024-07-07T20:35:02.506173Z",
     "shell.execute_reply": "2024-07-07T20:35:02.505235Z"
    },
    "papermill": {
     "duration": 3.283772,
     "end_time": "2024-07-07T20:35:02.508294",
     "exception": false,
     "start_time": "2024-07-07T20:34:59.224522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# num_return_sequences = 5\n",
    "# # this is not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b296c61e",
   "metadata": {
    "_cell_guid": "ba7816d1-4ccd-474c-ae38-5cb99ac6e7fc",
    "_uuid": "b44fbc42-eec7-44ab-917a-5d383e443954",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-07T20:35:09.089645Z",
     "iopub.status.busy": "2024-07-07T20:35:09.089216Z",
     "iopub.status.idle": "2024-07-07T20:35:09.094515Z",
     "shell.execute_reply": "2024-07-07T20:35:09.093622Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.309834,
     "end_time": "2024-07-07T20:35:09.096553",
     "exception": false,
     "start_time": "2024-07-07T20:35:05.786719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # generate! tokens is (B, T) where B = 5, T = 8\n",
    "# # set the seed to 42\n",
    "# torch.manual_seed(42)\n",
    "# # torch.cuda.manual_seed(42)\n",
    "# while x.size(1) < max_length:\n",
    "#     # forward the model to get the logits\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(x) # (B, T, vocab_size)\n",
    "#         # take the logits at the last position\n",
    "#         logits = logits[:, -1, :] # (B, vocab_size)\n",
    "#         # get the probabilities\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "#         # do top-k sampling of 50 (huggingface pipeline default)\n",
    "#         # topk_probs here become (5, 50), topk_indices is (5, 50)\n",
    "#         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "#         # select a token from the top-k probabilities\n",
    "#         ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "#         # gather the corresponding indices\n",
    "#         xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "#         # append to the sequence\n",
    "#         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# for i in range(num_return_sequences):\n",
    "#     tokens = x[i, :max_length].tolist()\n",
    "#     decoded = enc.decode(tokens)\n",
    "#     print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17540.889059,
   "end_time": "2024-07-07T20:35:12.852689",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-07T15:42:51.963630",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
