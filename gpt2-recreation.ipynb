{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b68e9f",
   "metadata": {
    "_cell_guid": "4c394a99-f5a3-4b23-95cd-477128ee993e",
    "_uuid": "1670d5c9-6e4f-477a-a5d1-356672775fbc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-05T13:00:15.316876Z",
     "iopub.status.busy": "2024-07-05T13:00:15.316201Z",
     "iopub.status.idle": "2024-07-05T13:02:41.302528Z",
     "shell.execute_reply": "2024-07-05T13:02:41.300938Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 145.994671,
     "end_time": "2024-07-05T13:02:41.305202",
     "exception": false,
     "start_time": "2024-07-05T13:00:15.310531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Collecting torch==2.3.1\r\n",
      "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2024.3.1)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting triton==2.3.1 (from torch==2.3.1)\r\n",
      "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\r\n",
      "Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m607.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.1.2\r\n",
      "    Uninstalling torch-2.1.2:\r\n",
      "      Successfully uninstalled torch-2.1.2\r\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 tiktoken-0.7.0 torch-2.3.1 triton-2.3.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken torch==2.3.1\n",
    "# by default (at the moment 18/06/2024) kaggle installs pytorch 2.1.something,\n",
    "# which for some reason breaks with the latest version of triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ed6eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T13:02:41.466404Z",
     "iopub.status.busy": "2024-07-05T13:02:41.465315Z",
     "iopub.status.idle": "2024-07-05T13:02:42.762863Z",
     "shell.execute_reply": "2024-07-05T13:02:42.761728Z"
    },
    "papermill": {
     "duration": 1.382278,
     "end_time": "2024-07-05T13:02:42.765057",
     "exception": false,
     "start_time": "2024-07-05T13:02:41.382779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-05 13:02:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: 'input.txt'\r\n",
      "\r\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \r\n",
      "\r\n",
      "2024-07-05 13:02:42 (42.9 MB/s) - 'input.txt' saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927f53a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T13:02:42.915102Z",
     "iopub.status.busy": "2024-07-05T13:02:42.914770Z",
     "iopub.status.idle": "2024-07-05T13:02:42.922255Z",
     "shell.execute_reply": "2024-07-05T13:02:42.921412Z"
    },
    "papermill": {
     "duration": 0.084455,
     "end_time": "2024-07-05T13:02:42.924262",
     "exception": false,
     "start_time": "2024-07-05T13:02:42.839807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_loader.py\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, process_rank, num_processes):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.is_master = process_rank == 0\n",
    "        \n",
    "        # load tokens from disk and store them in memory\n",
    "        with open('/kaggle/working/input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        if self.is_master:\n",
    "            print(f\"loaded {len(self.tokens)} tokens\")\n",
    "            print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "        \n",
    "        # state\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f275816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T13:02:43.119116Z",
     "iopub.status.busy": "2024-07-05T13:02:43.118755Z",
     "iopub.status.idle": "2024-07-05T13:02:43.131737Z",
     "shell.execute_reply": "2024-07-05T13:02:43.130851Z"
    },
    "papermill": {
     "duration": 0.092606,
     "end_time": "2024-07-05T13:02:43.133778",
     "exception": false,
     "start_time": "2024-07-05T13:02:43.041172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import inspect\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    path: str = \"gpt2\"\n",
    "    max_lr: float = 6e-4\n",
    "    min_lr: float = field(init=False)\n",
    "    min_lr_factor = 0.1\n",
    "    warmup_steps = 10\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # TODO maybe make it so this is done only if min_lr is not defined\n",
    "        self.min_lr = self.max_lr * self.min_lr_factor\n",
    "        \n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to the next batch\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T, T) matrix for all the queries and keys)\n",
    "        \n",
    "#         att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "#         att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "#         att = F.softmax(att, dim=-1)\n",
    "#         y = att @ v # (B , nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output project\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd) # generally, this should be initialized too but pytorch does it correctly\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig, process_rank: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.is_master = process_rank == 0\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        # calculate loss if targets are defined\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained GPT: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head, and n_embd are determined from medel_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),    # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),   # 350M params \n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),   # 774M params \n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),   # 1558M params \n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257   # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024    # always 1024 for GPT model checkpoints\n",
    "        \n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard mask / buffer\n",
    "\n",
    "        # init a higgingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # also ignore\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically, the OpenAI checkpoints use a \"Conv1D\" module, but we only want to use a vanila\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_lr(self, it, max_steps): ## TODO maybe move\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < self.config.warmup_steps:\n",
    "            return self.config.max_lr * (it + 1) / self.config.warmup_steps\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > max_steps:\n",
    "            return self.config.min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - self.config.warmup_steps) / (max_steps - self.config.warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "        return self.config.min_lr + coeff * (self.config.max_lr - self.config.min_lr)\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all of the candidate parameters (that require gradients)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameter that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. weight tensors in matmuls + embeddings decay, all biases and layernorms don't\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused) # fused is slower on kaggle for some reason TODO figure out why\n",
    "        if self.is_master:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters in total\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters in total\")\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37791794",
   "metadata": {
    "_cell_guid": "35edd86f-4e81-4576-a17b-187e0c29f353",
    "_uuid": "26d76cd9-11aa-4fc3-94bb-724b5d0da166",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-05T13:02:43.288903Z",
     "iopub.status.busy": "2024-07-05T13:02:43.288495Z",
     "iopub.status.idle": "2024-07-05T13:02:43.297473Z",
     "shell.execute_reply": "2024-07-05T13:02:43.296515Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.088546,
     "end_time": "2024-07-05T13:02:43.299448",
     "exception": false,
     "start_time": "2024-07-05T13:02:43.210902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from data_loader import DataLoaderLite\n",
    "from model import GPT, GPTConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ___________________________________Set up DDP_____________________________________\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), \"for now we need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla process\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process= True\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "\n",
    "print(f\"using device: {device}\")\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "    \n",
    "total_batch_size = 524288\n",
    "B = 8\n",
    "T = 1024\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f\"total desired batch size: {total_batch_size}\")\n",
    "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig(), ddp_rank)\n",
    "model.to(device)\n",
    "model = torch.compile(model) # TODO explain this better ?\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    \n",
    "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "max_steps = 50\n",
    "\n",
    "# optimize! the model\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), eps=1e-8) # TODO read about AdamW\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "for step in range(max_steps):\n",
    "    start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    \n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.float16): \n",
    "            # TODO look into how to get a gradient scaler working\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        \n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1) # switch to the DDP context manager\n",
    "        loss.backward()\n",
    "        \n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    lr = model.module.get_lr(step, max_steps) # try out the scheduler provided by pytorch\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    miliseconds = total_time*1000\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / total_time\n",
    "    \n",
    "    if master_process:\n",
    "        print(f\"step: {step}, loss: {loss_accum.item():.6f}, lr: {lr:.4e}, norm: {norm:.4f}, time: {miliseconds:.2f}ms, tok/sec: {tokens_per_sec:.2f}\")\n",
    "    \n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e04f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T13:02:43.453607Z",
     "iopub.status.busy": "2024-07-05T13:02:43.453156Z",
     "iopub.status.idle": "2024-07-05T13:14:42.775225Z",
     "shell.execute_reply": "2024-07-05T13:14:42.774262Z"
    },
    "papermill": {
     "duration": 719.401796,
     "end_time": "2024-07-05T13:14:42.777605",
     "exception": false,
     "start_time": "2024-07-05T13:02:43.375809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0705 13:02:46.383000 134795766028096 torch/distributed/run.py:757] \r\n",
      "W0705 13:02:46.383000 134795766028096 torch/distributed/run.py:757] *****************************************\r\n",
      "W0705 13:02:46.383000 134795766028096 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0705 13:02:46.383000 134795766028096 torch/distributed/run.py:757] *****************************************\r\n",
      "using device: cuda:0\r\n",
      "total desired batch size: 524288\r\n",
      "=> calculated gradient accumulation steps: 32\r\n",
      "using device: cuda:1\r\n",
      "loaded 338025 tokens\r\n",
      "1 epoch = 41 batches\r\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters in total\r\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters in total\r\n",
      "using fused AdamW: True\r\n",
      "step: 0, loss: 10.938564, lr: 6.0000e-05, norm: 21.1924, time: 74131.04ms, tok/sec: 7072.45\r\n",
      "step: 1, loss: 9.604090, lr: 1.2000e-04, norm: 8.2276, time: 12645.10ms, tok/sec: 41461.74\r\n",
      "step: 2, loss: 9.179706, lr: 1.8000e-04, norm: 3.8822, time: 12751.43ms, tok/sec: 41116.02\r\n",
      "step: 3, loss: 9.120297, lr: 2.4000e-04, norm: 3.9298, time: 12962.99ms, tok/sec: 40444.98\r\n",
      "step: 4, loss: 9.272064, lr: 3.0000e-04, norm: 4.8506, time: 13210.19ms, tok/sec: 39688.14\r\n",
      "step: 5, loss: 9.028257, lr: 3.6000e-04, norm: 4.3758, time: 13259.94ms, tok/sec: 39539.25\r\n",
      "step: 6, loss: 8.775995, lr: 4.2000e-04, norm: 3.4135, time: 12996.82ms, tok/sec: 40339.72\r\n",
      "step: 7, loss: 8.655592, lr: 4.8000e-04, norm: 2.4345, time: 12803.30ms, tok/sec: 40949.45\r\n",
      "step: 8, loss: 8.511133, lr: 5.4000e-04, norm: 2.3418, time: 12727.89ms, tok/sec: 41192.05\r\n",
      "step: 9, loss: 8.375751, lr: 6.0000e-04, norm: 2.4934, time: 12755.67ms, tok/sec: 41102.35\r\n",
      "step: 10, loss: 8.257563, lr: 6.0000e-04, norm: 2.5444, time: 12788.04ms, tok/sec: 40998.30\r\n",
      "step: 11, loss: 8.128592, lr: 5.9917e-04, norm: 2.5826, time: 12771.93ms, tok/sec: 41050.03\r\n",
      "step: 12, loss: 8.052392, lr: 5.9668e-04, norm: 2.7309, time: 12747.97ms, tok/sec: 41127.18\r\n",
      "step: 13, loss: 7.963954, lr: 5.9254e-04, norm: 2.8309, time: 12745.25ms, tok/sec: 41135.95\r\n",
      "step: 14, loss: 7.895386, lr: 5.8679e-04, norm: 2.7577, time: 12722.30ms, tok/sec: 41210.16\r\n",
      "step: 15, loss: 7.855097, lr: 5.7945e-04, norm: 2.7535, time: 12722.49ms, tok/sec: 41209.55\r\n",
      "step: 16, loss: 7.801450, lr: 5.7057e-04, norm: 2.7396, time: 12713.39ms, tok/sec: 41239.05\r\n",
      "step: 17, loss: 7.790470, lr: 5.6021e-04, norm: 2.5940, time: 12708.30ms, tok/sec: 41255.54\r\n",
      "step: 18, loss: 7.760345, lr: 5.4843e-04, norm: 2.6280, time: 12692.45ms, tok/sec: 41307.07\r\n",
      "step: 19, loss: 7.738233, lr: 5.3531e-04, norm: 2.6197, time: 12691.67ms, tok/sec: 41309.60\r\n",
      "step: 20, loss: 7.734582, lr: 5.2092e-04, norm: 2.6167, time: 12673.93ms, tok/sec: 41367.45\r\n",
      "step: 21, loss: 7.708962, lr: 5.0535e-04, norm: 2.7588, time: 12664.90ms, tok/sec: 41396.93\r\n",
      "step: 22, loss: 7.718742, lr: 4.8870e-04, norm: 2.7005, time: 12664.03ms, tok/sec: 41399.78\r\n",
      "step: 23, loss: 7.703188, lr: 4.7107e-04, norm: 2.7773, time: 12648.43ms, tok/sec: 41450.85\r\n",
      "step: 24, loss: 7.692013, lr: 4.5258e-04, norm: 2.8264, time: 12660.19ms, tok/sec: 41412.33\r\n",
      "step: 25, loss: 7.694778, lr: 4.3332e-04, norm: 2.8989, time: 12671.44ms, tok/sec: 41375.58\r\n",
      "step: 26, loss: 7.674911, lr: 4.1343e-04, norm: 2.9386, time: 12660.74ms, tok/sec: 41410.54\r\n",
      "step: 27, loss: 7.687523, lr: 3.9303e-04, norm: 2.8854, time: 12648.67ms, tok/sec: 41450.03\r\n",
      "step: 28, loss: 7.675362, lr: 3.7224e-04, norm: 2.9325, time: 12672.65ms, tok/sec: 41371.60\r\n",
      "step: 29, loss: 7.665369, lr: 3.5118e-04, norm: 2.9276, time: 12674.87ms, tok/sec: 41364.37\r\n",
      "step: 30, loss: 7.669182, lr: 3.3000e-04, norm: 2.9195, time: 12668.52ms, tok/sec: 41385.10\r\n",
      "step: 31, loss: 7.649899, lr: 3.0882e-04, norm: 2.9584, time: 12676.61ms, tok/sec: 41358.68\r\n",
      "step: 32, loss: 7.663164, lr: 2.8776e-04, norm: 2.8107, time: 12674.95ms, tok/sec: 41364.10\r\n",
      "step: 33, loss: 7.653246, lr: 2.6697e-04, norm: 2.8539, time: 12662.21ms, tok/sec: 41405.73\r\n",
      "step: 34, loss: 7.643810, lr: 2.4657e-04, norm: 2.8120, time: 12676.50ms, tok/sec: 41359.04\r\n",
      "step: 35, loss: 7.651387, lr: 2.2668e-04, norm: 2.7971, time: 12658.98ms, tok/sec: 41416.31\r\n",
      "step: 36, loss: 7.632481, lr: 2.0742e-04, norm: 2.8263, time: 12634.52ms, tok/sec: 41496.48\r\n",
      "step: 37, loss: 7.648511, lr: 1.8893e-04, norm: 2.7723, time: 12642.84ms, tok/sec: 41469.16\r\n",
      "step: 38, loss: 7.637937, lr: 1.7130e-04, norm: 2.8215, time: 12629.19ms, tok/sec: 41514.00\r\n",
      "step: 39, loss: 7.631171, lr: 1.5465e-04, norm: 2.8137, time: 12634.66ms, tok/sec: 41496.00\r\n",
      "step: 40, loss: 7.638825, lr: 1.3908e-04, norm: 2.8149, time: 12636.09ms, tok/sec: 41491.30\r\n",
      "step: 41, loss: 7.622380, lr: 1.2469e-04, norm: 2.9050, time: 12648.86ms, tok/sec: 41449.43\r\n",
      "step: 42, loss: 7.639106, lr: 1.1157e-04, norm: 2.7903, time: 12662.23ms, tok/sec: 41405.67\r\n",
      "step: 43, loss: 7.631849, lr: 9.9787e-05, norm: 2.9298, time: 12642.34ms, tok/sec: 41470.79\r\n",
      "step: 44, loss: 7.625073, lr: 8.9428e-05, norm: 2.9466, time: 12650.59ms, tok/sec: 41443.76\r\n",
      "step: 45, loss: 7.631262, lr: 8.0553e-05, norm: 2.8197, time: 12647.91ms, tok/sec: 41452.53\r\n",
      "step: 46, loss: 7.615940, lr: 7.3215e-05, norm: 2.8325, time: 12649.63ms, tok/sec: 41446.89\r\n",
      "step: 47, loss: 7.635174, lr: 6.7460e-05, norm: 2.8090, time: 12658.63ms, tok/sec: 41417.42\r\n",
      "step: 48, loss: 7.627457, lr: 6.3324e-05, norm: 2.8593, time: 12678.35ms, tok/sec: 41353.01\r\n",
      "step: 49, loss: 7.619213, lr: 6.0832e-05, norm: 2.7345, time: 12641.14ms, tok/sec: 41474.75\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35bc226d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T13:14:42.941983Z",
     "iopub.status.busy": "2024-07-05T13:14:42.941231Z",
     "iopub.status.idle": "2024-07-05T13:15:57.055236Z",
     "shell.execute_reply": "2024-07-05T13:15:57.054249Z"
    },
    "papermill": {
     "duration": 74.197716,
     "end_time": "2024-07-05T13:15:57.057703",
     "exception": false,
     "start_time": "2024-07-05T13:14:42.859987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\r\n",
      "total desired batch size: 524288\r\n",
      "=> calculated gradient accumulation steps: 64\r\n",
      "loaded 338025 tokens\r\n",
      "1 epoch = 41 batches\r\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters in total\r\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters in total\r\n",
      "using fused AdamW: True\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/main.py\", line 95, in <module>\r\n",
      "    lr = model.module.get_lr(step, max_steps) # try out the scheduler provided by pytorch\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 222, in __getattr__\r\n",
      "    return getattr(self._orig_mod, name)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1709, in __getattr__\r\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\r\n",
      "AttributeError: 'GPT' object has no attribute 'module'. Did you mean: 'modules'?\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25a634e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T13:15:57.221658Z",
     "iopub.status.busy": "2024-07-05T13:15:57.220966Z",
     "iopub.status.idle": "2024-07-05T13:15:57.225246Z",
     "shell.execute_reply": "2024-07-05T13:15:57.224420Z"
    },
    "papermill": {
     "duration": 0.088081,
     "end_time": "2024-07-05T13:15:57.227299",
     "exception": false,
     "start_time": "2024-07-05T13:15:57.139218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# num_return_sequences = 5\n",
    "# # this is not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b16ed53",
   "metadata": {
    "_cell_guid": "ba7816d1-4ccd-474c-ae38-5cb99ac6e7fc",
    "_uuid": "b44fbc42-eec7-44ab-917a-5d383e443954",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-05T13:15:57.391347Z",
     "iopub.status.busy": "2024-07-05T13:15:57.391000Z",
     "iopub.status.idle": "2024-07-05T13:15:57.396035Z",
     "shell.execute_reply": "2024-07-05T13:15:57.395133Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.089316,
     "end_time": "2024-07-05T13:15:57.397986",
     "exception": false,
     "start_time": "2024-07-05T13:15:57.308670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # generate! tokens is (B, T) where B = 5, T = 8\n",
    "# # set the seed to 42\n",
    "# torch.manual_seed(42)\n",
    "# # torch.cuda.manual_seed(42)\n",
    "# while x.size(1) < max_length:\n",
    "#     # forward the model to get the logits\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(x) # (B, T, vocab_size)\n",
    "#         # take the logits at the last position\n",
    "#         logits = logits[:, -1, :] # (B, vocab_size)\n",
    "#         # get the probabilities\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "#         # do top-k sampling of 50 (huggingface pipeline default)\n",
    "#         # topk_probs here become (5, 50), topk_indices is (5, 50)\n",
    "#         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "#         # select a token from the top-k probabilities\n",
    "#         ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "#         # gather the corresponding indices\n",
    "#         xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "#         # append to the sequence\n",
    "#         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# for i in range(num_return_sequences):\n",
    "#     tokens = x[i, :max_length].tolist()\n",
    "#     decoded = enc.decode(tokens)\n",
    "#     print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 945.412044,
   "end_time": "2024-07-05T13:15:57.799320",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-05T13:00:12.387276",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
