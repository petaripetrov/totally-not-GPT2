{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239a3b0e",
   "metadata": {
    "_cell_guid": "4c394a99-f5a3-4b23-95cd-477128ee993e",
    "_uuid": "1670d5c9-6e4f-477a-a5d1-356672775fbc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-04T14:45:54.598794Z",
     "iopub.status.busy": "2024-07-04T14:45:54.598469Z",
     "iopub.status.idle": "2024-07-04T14:52:36.383054Z",
     "shell.execute_reply": "2024-07-04T14:52:36.381469Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 401.792822,
     "end_time": "2024-07-04T14:52:36.385680",
     "exception": false,
     "start_time": "2024-07-04T14:45:54.592858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Collecting torch==2.3.1\r\n",
      "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2024.3.1)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting triton==2.3.1 (from torch==2.3.1)\r\n",
      "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\r\n",
      "Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.1.2\r\n",
      "    Uninstalling torch-2.1.2:\r\n",
      "      Successfully uninstalled torch-2.1.2\r\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 tiktoken-0.7.0 torch-2.3.1 triton-2.3.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken torch==2.3.1\n",
    "# by default (at the moment 18/06/2024) kaggle installs pytorch 2.1.something,\n",
    "# which for some reason breaks with the latest version of triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8c3050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T14:52:37.373186Z",
     "iopub.status.busy": "2024-07-04T14:52:37.372808Z",
     "iopub.status.idle": "2024-07-04T14:52:38.774584Z",
     "shell.execute_reply": "2024-07-04T14:52:38.773443Z"
    },
    "papermill": {
     "duration": 1.890013,
     "end_time": "2024-07-04T14:52:38.777284",
     "exception": false,
     "start_time": "2024-07-04T14:52:36.887271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-04 14:52:38--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: 'input.txt'\r\n",
      "\r\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \r\n",
      "\r\n",
      "2024-07-04 14:52:38 (19.0 MB/s) - 'input.txt' saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4862741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T14:52:39.866603Z",
     "iopub.status.busy": "2024-07-04T14:52:39.866196Z",
     "iopub.status.idle": "2024-07-04T14:52:39.873943Z",
     "shell.execute_reply": "2024-07-04T14:52:39.873074Z"
    },
    "papermill": {
     "duration": 0.541217,
     "end_time": "2024-07-04T14:52:39.876125",
     "exception": false,
     "start_time": "2024-07-04T14:52:39.334908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_loader.py\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, process_rank, num_processes):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.is_master = process_rank == 0\n",
    "        \n",
    "        # load tokens from disk and store them in memory\n",
    "        with open('/kaggle/working/input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        if self.is_master:\n",
    "            print(f\"loaded {len(self.tokens)} tokens\")\n",
    "            print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "        \n",
    "        # state\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea5fd1ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T14:52:40.858709Z",
     "iopub.status.busy": "2024-07-04T14:52:40.858344Z",
     "iopub.status.idle": "2024-07-04T14:52:40.871194Z",
     "shell.execute_reply": "2024-07-04T14:52:40.870125Z"
    },
    "papermill": {
     "duration": 0.516015,
     "end_time": "2024-07-04T14:52:40.873245",
     "exception": false,
     "start_time": "2024-07-04T14:52:40.357230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import inspect\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    path: str = \"gpt2\"\n",
    "        \n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to the next batch\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T, T) matrix for all the queries and keys)\n",
    "        \n",
    "#         att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "#         att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "#         att = F.softmax(att, dim=-1)\n",
    "#         y = att @ v # (B , nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output project\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd) # generally, this should be initialized too but pytorch does it correctly\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig, process_rank: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.is_master = process_rank == 0\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        # calculate loss if targets are defined\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained GPT: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head, and n_embd are determined from medel_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),    # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),   # 350M params \n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),   # 774M params \n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),   # 1558M params \n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257   # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024    # always 1024 for GPT model checkpoints\n",
    "        \n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard mask / buffer\n",
    "\n",
    "        # init a higgingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # also ignore\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically, the OpenAI checkpoints use a \"Conv1D\" module, but we only want to use a vanila\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all of the candidate parameters (that require gradients)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameter that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. weight tensors in matmuls + embeddings decay, all biases and layernorms don't\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused) # fused is slower on kaggle for some reason TODO figure out why\n",
    "        if self.is_master:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters in total\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters in total\")\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7302b05d",
   "metadata": {
    "_cell_guid": "35edd86f-4e81-4576-a17b-187e0c29f353",
    "_uuid": "26d76cd9-11aa-4fc3-94bb-724b5d0da166",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-04T14:52:41.968383Z",
     "iopub.status.busy": "2024-07-04T14:52:41.968029Z",
     "iopub.status.idle": "2024-07-04T14:52:41.976384Z",
     "shell.execute_reply": "2024-07-04T14:52:41.975538Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.595783,
     "end_time": "2024-07-04T14:52:41.978671",
     "exception": false,
     "start_time": "2024-07-04T14:52:41.382888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from data_loader import DataLoaderLite\n",
    "from model import GPT, GPTConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ___________________________________Set up DDP_____________________________________\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), \"for now we need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla process\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process= True\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "\n",
    "print(f\"using device: {device}\")\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "    \n",
    "total_batch_size = 524288\n",
    "B = 8\n",
    "T = 1024\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f\"total desired batch size: {total_batch_size}\")\n",
    "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig(), ddp_rank)\n",
    "model.to(device)\n",
    "model = torch.compile(model) # TODO explain this better ?\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    \n",
    "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "# ___________________________________LR Scaling______________________________________\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "\n",
    "def get_lr(it): ## TODO maybe move\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "\n",
    "# optimize! the model\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), eps=1e-8) # TODO read about AdamW\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "for step in range(max_steps):\n",
    "    start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    \n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.float16): \n",
    "            # TODO look into how to get a gradient scaler working\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        \n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1) # switch to the DDP context manager\n",
    "        loss.backward()\n",
    "        \n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    lr = get_lr(step) # try out the scheduler provided by pytorch\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    miliseconds = total_time*1000\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / total_time\n",
    "    \n",
    "    if master_process:\n",
    "        print(f\"step: {step}, loss: {loss_accum.item():.6f}, lr: {lr:.4e}, norm: {norm:.4f}, time: {miliseconds:.2f}ms, tok/sec: {tokens_per_sec:.2f}\")\n",
    "    \n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409267af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T14:52:42.991430Z",
     "iopub.status.busy": "2024-07-04T14:52:42.991025Z",
     "iopub.status.idle": "2024-07-04T15:04:17.078685Z",
     "shell.execute_reply": "2024-07-04T15:04:17.077535Z"
    },
    "papermill": {
     "duration": 694.584738,
     "end_time": "2024-07-04T15:04:17.081262",
     "exception": false,
     "start_time": "2024-07-04T14:52:42.496524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0704 14:52:45.879000 138301006960448 torch/distributed/run.py:757] \r\n",
      "W0704 14:52:45.879000 138301006960448 torch/distributed/run.py:757] *****************************************\r\n",
      "W0704 14:52:45.879000 138301006960448 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0704 14:52:45.879000 138301006960448 torch/distributed/run.py:757] *****************************************\r\n",
      "using device: cuda:0\r\n",
      "total desired batch size: 524288\r\n",
      "=> calculated gradient accumulation steps: 32\r\n",
      "using device: cuda:1\r\n",
      "loaded 338025 tokens\r\n",
      "1 epoch = 41 batches\r\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters in total\r\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters in total\r\n",
      "using fused AdamW: True\r\n",
      "step: 0, loss: 10.938564, lr: 6.0000e-05, norm: 21.1924, time: 75581.19ms, tok/sec: 6936.75\r\n",
      "step: 1, loss: 9.604090, lr: 1.2000e-04, norm: 8.2282, time: 11955.94ms, tok/sec: 43851.68\r\n",
      "step: 2, loss: 9.179728, lr: 1.8000e-04, norm: 3.8812, time: 11999.65ms, tok/sec: 43691.93\r\n",
      "step: 3, loss: 9.120567, lr: 2.4000e-04, norm: 3.9659, time: 12093.74ms, tok/sec: 43352.03\r\n",
      "step: 4, loss: 9.273706, lr: 3.0000e-04, norm: 4.8592, time: 12266.36ms, tok/sec: 42741.94\r\n",
      "step: 5, loss: 9.029781, lr: 3.6000e-04, norm: 4.3885, time: 12404.12ms, tok/sec: 42267.25\r\n",
      "step: 6, loss: 8.776279, lr: 4.2000e-04, norm: 3.4417, time: 12620.93ms, tok/sec: 41541.14\r\n",
      "step: 7, loss: 8.654945, lr: 4.8000e-04, norm: 2.4659, time: 12718.27ms, tok/sec: 41223.21\r\n",
      "step: 8, loss: 8.510195, lr: 5.4000e-04, norm: 2.3262, time: 12573.98ms, tok/sec: 41696.26\r\n",
      "step: 9, loss: 8.374390, lr: 6.0000e-04, norm: 2.3725, time: 12419.01ms, tok/sec: 42216.59\r\n",
      "step: 10, loss: 8.255682, lr: 6.0000e-04, norm: 2.5196, time: 12307.16ms, tok/sec: 42600.24\r\n",
      "step: 11, loss: 8.126420, lr: 5.9917e-04, norm: 2.5820, time: 12246.66ms, tok/sec: 42810.68\r\n",
      "step: 12, loss: 8.049919, lr: 5.9668e-04, norm: 2.5330, time: 12270.06ms, tok/sec: 42729.06\r\n",
      "step: 13, loss: 7.961172, lr: 5.9254e-04, norm: 2.5758, time: 12301.65ms, tok/sec: 42619.34\r\n",
      "step: 14, loss: 7.892665, lr: 5.8679e-04, norm: 2.6303, time: 12367.95ms, tok/sec: 42390.87\r\n",
      "step: 15, loss: 7.852398, lr: 5.7945e-04, norm: 2.6975, time: 12388.43ms, tok/sec: 42320.79\r\n",
      "step: 16, loss: 7.798972, lr: 5.7057e-04, norm: 2.7834, time: 12379.52ms, tok/sec: 42351.23\r\n",
      "step: 17, loss: 7.788121, lr: 5.6021e-04, norm: 2.6022, time: 12350.00ms, tok/sec: 42452.46\r\n",
      "step: 18, loss: 7.758025, lr: 5.4843e-04, norm: 2.6322, time: 12327.85ms, tok/sec: 42528.74\r\n",
      "step: 19, loss: 7.735364, lr: 5.3531e-04, norm: 2.6198, time: 12289.18ms, tok/sec: 42662.57\r\n",
      "step: 20, loss: 7.732350, lr: 5.2092e-04, norm: 2.7161, time: 12267.26ms, tok/sec: 42738.81\r\n",
      "step: 21, loss: 7.706664, lr: 5.0535e-04, norm: 2.7590, time: 12301.16ms, tok/sec: 42621.03\r\n",
      "step: 22, loss: 7.715747, lr: 4.8870e-04, norm: 2.7201, time: 12344.15ms, tok/sec: 42472.60\r\n",
      "step: 23, loss: 7.700105, lr: 4.7107e-04, norm: 2.8607, time: 12353.96ms, tok/sec: 42438.86\r\n",
      "step: 24, loss: 7.688993, lr: 4.5258e-04, norm: 2.9056, time: 12300.63ms, tok/sec: 42622.85\r\n",
      "step: 25, loss: 7.691799, lr: 4.3332e-04, norm: 2.8990, time: 12291.99ms, tok/sec: 42652.80\r\n",
      "step: 26, loss: 7.672293, lr: 4.1343e-04, norm: 2.9387, time: 12288.24ms, tok/sec: 42665.85\r\n",
      "step: 27, loss: 7.683615, lr: 3.9303e-04, norm: 2.8854, time: 12285.27ms, tok/sec: 42676.16\r\n",
      "step: 28, loss: 7.672028, lr: 3.7224e-04, norm: 2.9326, time: 12313.34ms, tok/sec: 42578.86\r\n",
      "step: 29, loss: 7.662294, lr: 3.5118e-04, norm: 2.9277, time: 12324.08ms, tok/sec: 42541.74\r\n",
      "step: 30, loss: 7.666278, lr: 3.3000e-04, norm: 2.9000, time: 12337.63ms, tok/sec: 42495.04\r\n",
      "step: 31, loss: 7.646640, lr: 3.0882e-04, norm: 2.8580, time: 12326.72ms, tok/sec: 42532.63\r\n",
      "step: 32, loss: 7.660641, lr: 2.8776e-04, norm: 2.7658, time: 12316.96ms, tok/sec: 42566.34\r\n",
      "step: 33, loss: 7.648832, lr: 2.6697e-04, norm: 2.8251, time: 12323.37ms, tok/sec: 42544.20\r\n",
      "step: 34, loss: 7.641005, lr: 2.4657e-04, norm: 2.8071, time: 12325.23ms, tok/sec: 42537.80\r\n",
      "step: 35, loss: 7.647401, lr: 2.2668e-04, norm: 2.8030, time: 12321.10ms, tok/sec: 42552.05\r\n",
      "step: 36, loss: 7.630472, lr: 2.0742e-04, norm: 2.8692, time: 12311.41ms, tok/sec: 42585.54\r\n",
      "step: 37, loss: 7.646842, lr: 1.8893e-04, norm: 2.8578, time: 12304.07ms, tok/sec: 42610.95\r\n",
      "step: 38, loss: 7.636381, lr: 1.7130e-04, norm: 2.8450, time: 12298.10ms, tok/sec: 42631.64\r\n",
      "step: 39, loss: 7.629974, lr: 1.5465e-04, norm: 2.8676, time: 12298.96ms, tok/sec: 42628.66\r\n",
      "step: 40, loss: 7.635878, lr: 1.3908e-04, norm: 2.7386, time: 12301.31ms, tok/sec: 42620.50\r\n",
      "step: 41, loss: 7.619149, lr: 1.2469e-04, norm: 2.6717, time: 12301.43ms, tok/sec: 42620.08\r\n",
      "step: 42, loss: 7.636761, lr: 1.1157e-04, norm: 2.4402, time: 12285.89ms, tok/sec: 42673.98\r\n",
      "step: 43, loss: 7.628724, lr: 9.9787e-05, norm: 2.5577, time: 12282.80ms, tok/sec: 42684.73\r\n",
      "step: 44, loss: 7.619269, lr: 8.9428e-05, norm: 2.6278, time: 12290.49ms, tok/sec: 42658.02\r\n",
      "step: 45, loss: 7.627955, lr: 8.0553e-05, norm: 2.5003, time: 12279.39ms, tok/sec: 42696.59\r\n",
      "step: 46, loss: 7.612133, lr: 7.3215e-05, norm: 2.5597, time: 12289.81ms, tok/sec: 42660.40\r\n",
      "step: 47, loss: 7.631717, lr: 6.7460e-05, norm: 2.4050, time: 12284.26ms, tok/sec: 42679.64\r\n",
      "step: 48, loss: 7.623558, lr: 6.3324e-05, norm: 2.4561, time: 12284.25ms, tok/sec: 42679.68\r\n",
      "step: 49, loss: 7.615779, lr: 6.0832e-05, norm: 2.5965, time: 12296.42ms, tok/sec: 42637.43\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714fd873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T15:04:18.097771Z",
     "iopub.status.busy": "2024-07-04T15:04:18.096594Z",
     "iopub.status.idle": "2024-07-04T15:25:33.893002Z",
     "shell.execute_reply": "2024-07-04T15:25:33.891725Z"
    },
    "papermill": {
     "duration": 1276.312249,
     "end_time": "2024-07-04T15:25:33.895723",
     "exception": false,
     "start_time": "2024-07-04T15:04:17.583474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\r\n",
      "total desired batch size: 524288\r\n",
      "=> calculated gradient accumulation steps: 64\r\n",
      "loaded 338025 tokens\r\n",
      "1 epoch = 41 batches\r\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters in total\r\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters in total\r\n",
      "using fused AdamW: True\r\n",
      "step: 0, loss: 10.939035, lr: 6.0000e-05, norm: 17.0043, time: 64669.53ms, tok/sec: 8107.19\r\n",
      "step: 1, loss: 9.594664, lr: 1.2000e-04, norm: 8.4415, time: 25685.47ms, tok/sec: 20411.85\r\n",
      "step: 2, loss: 9.370935, lr: 1.8000e-04, norm: 5.0638, time: 25235.50ms, tok/sec: 20775.81\r\n",
      "step: 3, loss: 9.244095, lr: 2.4000e-04, norm: 4.1942, time: 24896.64ms, tok/sec: 21058.58\r\n",
      "step: 4, loss: 9.259415, lr: 3.0000e-04, norm: 4.2382, time: 24852.33ms, tok/sec: 21096.13\r\n",
      "step: 5, loss: 9.184933, lr: 3.6000e-04, norm: 3.8872, time: 24886.01ms, tok/sec: 21067.58\r\n",
      "step: 6, loss: 9.122185, lr: 4.2000e-04, norm: 4.0055, time: 24649.76ms, tok/sec: 21269.50\r\n",
      "step: 7, loss: 9.039521, lr: 4.8000e-04, norm: 4.0151, time: 24582.55ms, tok/sec: 21327.65\r\n",
      "step: 8, loss: 8.927726, lr: 5.4000e-04, norm: 4.0281, time: 24605.90ms, tok/sec: 21307.41\r\n",
      "step: 9, loss: 8.817530, lr: 6.0000e-04, norm: 4.0068, time: 24633.47ms, tok/sec: 21283.56\r\n",
      "step: 10, loss: 8.676395, lr: 6.0000e-04, norm: 4.0508, time: 24552.80ms, tok/sec: 21353.49\r\n",
      "step: 11, loss: 8.581113, lr: 5.9917e-04, norm: 4.0034, time: 24588.99ms, tok/sec: 21322.06\r\n",
      "step: 12, loss: 8.466866, lr: 5.9668e-04, norm: 4.0642, time: 24583.62ms, tok/sec: 21326.72\r\n",
      "step: 13, loss: 8.401512, lr: 5.9254e-04, norm: 4.0179, time: 24522.99ms, tok/sec: 21379.45\r\n",
      "step: 14, loss: 8.332117, lr: 5.8679e-04, norm: 4.0476, time: 24509.09ms, tok/sec: 21391.57\r\n",
      "step: 15, loss: 8.276114, lr: 5.7945e-04, norm: 4.0534, time: 24462.80ms, tok/sec: 21432.06\r\n",
      "step: 16, loss: 8.244382, lr: 5.7057e-04, norm: 4.0495, time: 24446.56ms, tok/sec: 21446.29\r\n",
      "step: 17, loss: 8.201396, lr: 5.6021e-04, norm: 4.0820, time: 24427.64ms, tok/sec: 21462.90\r\n",
      "step: 18, loss: 8.192754, lr: 5.4843e-04, norm: 4.0378, time: 24426.12ms, tok/sec: 21464.24\r\n",
      "step: 19, loss: 8.152429, lr: 5.3531e-04, norm: 4.1046, time: 24453.83ms, tok/sec: 21439.91\r\n",
      "step: 20, loss: 8.154623, lr: 5.2092e-04, norm: 4.0477, time: 24463.78ms, tok/sec: 21431.19\r\n",
      "step: 21, loss: 8.128088, lr: 5.0535e-04, norm: 4.0959, time: 24436.47ms, tok/sec: 21455.14\r\n",
      "step: 22, loss: 8.120255, lr: 4.8870e-04, norm: 4.0742, time: 24472.84ms, tok/sec: 21423.26\r\n",
      "step: 23, loss: 8.109875, lr: 4.7107e-04, norm: 4.0943, time: 24441.65ms, tok/sec: 21450.59\r\n",
      "step: 24, loss: 8.093443, lr: 4.5258e-04, norm: 4.1063, time: 24414.08ms, tok/sec: 21474.82\r\n",
      "step: 25, loss: 8.096293, lr: 4.3332e-04, norm: 4.0868, time: 24422.96ms, tok/sec: 21467.02\r\n",
      "step: 26, loss: 8.071841, lr: 4.1343e-04, norm: 4.1347, time: 24400.29ms, tok/sec: 21486.96\r\n",
      "step: 27, loss: 8.084352, lr: 3.9303e-04, norm: 4.0816, time: 24390.87ms, tok/sec: 21495.25\r\n",
      "step: 28, loss: 8.056682, lr: 3.7224e-04, norm: 4.1503, time: 24423.92ms, tok/sec: 21466.17\r\n",
      "step: 29, loss: 8.067462, lr: 3.5118e-04, norm: 4.0957, time: 24420.41ms, tok/sec: 21469.26\r\n",
      "step: 30, loss: 8.055740, lr: 3.3000e-04, norm: 4.1264, time: 24413.64ms, tok/sec: 21475.21\r\n",
      "step: 31, loss: 8.045362, lr: 3.0882e-04, norm: 4.1298, time: 24422.66ms, tok/sec: 21467.27\r\n",
      "step: 32, loss: 8.046277, lr: 2.8776e-04, norm: 4.1293, time: 24407.87ms, tok/sec: 21480.28\r\n",
      "step: 33, loss: 8.032660, lr: 2.6697e-04, norm: 4.1547, time: 24395.97ms, tok/sec: 21490.77\r\n",
      "step: 34, loss: 8.043195, lr: 2.4657e-04, norm: 4.1168, time: 24419.45ms, tok/sec: 21470.10\r\n",
      "step: 35, loss: 8.018941, lr: 2.2668e-04, norm: 4.1722, time: 24404.65ms, tok/sec: 21483.12\r\n",
      "step: 36, loss: 8.035789, lr: 2.0742e-04, norm: 4.1174, time: 24403.90ms, tok/sec: 21483.78\r\n",
      "step: 37, loss: 8.015342, lr: 1.8893e-04, norm: 4.1714, time: 24410.88ms, tok/sec: 21477.63\r\n",
      "step: 38, loss: 8.023417, lr: 1.7130e-04, norm: 4.1339, time: 24377.80ms, tok/sec: 21506.78\r\n",
      "step: 39, loss: 8.017237, lr: 1.5465e-04, norm: 4.1551, time: 24372.34ms, tok/sec: 21511.60\r\n",
      "step: 40, loss: 8.008861, lr: 1.3908e-04, norm: 4.1620, time: 24372.63ms, tok/sec: 21511.35\r\n",
      "step: 41, loss: 8.019560, lr: 1.2469e-04, norm: 4.1438, time: 24380.49ms, tok/sec: 21504.40\r\n",
      "step: 42, loss: 7.999118, lr: 1.1157e-04, norm: 4.1874, time: 24396.31ms, tok/sec: 21490.46\r\n",
      "step: 43, loss: 8.017305, lr: 9.9787e-05, norm: 4.1320, time: 24404.92ms, tok/sec: 21482.88\r\n",
      "step: 44, loss: 7.997322, lr: 8.9428e-05, norm: 4.1939, time: 24390.32ms, tok/sec: 21495.74\r\n",
      "step: 45, loss: 8.013108, lr: 8.0553e-05, norm: 4.1365, time: 24392.58ms, tok/sec: 21493.75\r\n",
      "step: 46, loss: 8.002995, lr: 7.3215e-05, norm: 4.1687, time: 24398.03ms, tok/sec: 21488.95\r\n",
      "step: 47, loss: 8.000794, lr: 6.7460e-05, norm: 4.1573, time: 24405.81ms, tok/sec: 21482.10\r\n",
      "step: 48, loss: 8.004696, lr: 6.3324e-05, norm: 4.1668, time: 24392.43ms, tok/sec: 21493.88\r\n",
      "step: 49, loss: 7.994596, lr: 6.0832e-05, norm: 4.1773, time: 24392.74ms, tok/sec: 21493.61\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65746a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T15:25:34.944023Z",
     "iopub.status.busy": "2024-07-04T15:25:34.943624Z",
     "iopub.status.idle": "2024-07-04T15:25:34.948097Z",
     "shell.execute_reply": "2024-07-04T15:25:34.947127Z"
    },
    "papermill": {
     "duration": 0.520551,
     "end_time": "2024-07-04T15:25:34.950134",
     "exception": false,
     "start_time": "2024-07-04T15:25:34.429583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# num_return_sequences = 5\n",
    "# # this is not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100b8c8d",
   "metadata": {
    "_cell_guid": "ba7816d1-4ccd-474c-ae38-5cb99ac6e7fc",
    "_uuid": "b44fbc42-eec7-44ab-917a-5d383e443954",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-04T15:25:36.027452Z",
     "iopub.status.busy": "2024-07-04T15:25:36.027133Z",
     "iopub.status.idle": "2024-07-04T15:25:36.031898Z",
     "shell.execute_reply": "2024-07-04T15:25:36.031048Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.524554,
     "end_time": "2024-07-04T15:25:36.033808",
     "exception": false,
     "start_time": "2024-07-04T15:25:35.509254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # generate! tokens is (B, T) where B = 5, T = 8\n",
    "# # set the seed to 42\n",
    "# torch.manual_seed(42)\n",
    "# # torch.cuda.manual_seed(42)\n",
    "# while x.size(1) < max_length:\n",
    "#     # forward the model to get the logits\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(x) # (B, T, vocab_size)\n",
    "#         # take the logits at the last position\n",
    "#         logits = logits[:, -1, :] # (B, vocab_size)\n",
    "#         # get the probabilities\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "#         # do top-k sampling of 50 (huggingface pipeline default)\n",
    "#         # topk_probs here become (5, 50), topk_indices is (5, 50)\n",
    "#         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "#         # select a token from the top-k probabilities\n",
    "#         ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "#         # gather the corresponding indices\n",
    "#         xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "#         # append to the sequence\n",
    "#         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# for i in range(num_return_sequences):\n",
    "#     tokens = x[i, :max_length].tolist()\n",
    "#     decoded = enc.decode(tokens)\n",
    "#     print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2384.892122,
   "end_time": "2024-07-04T15:25:36.866591",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-04T14:45:51.974469",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
