{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c21b04c",
   "metadata": {
    "_cell_guid": "4c394a99-f5a3-4b23-95cd-477128ee993e",
    "_uuid": "1670d5c9-6e4f-477a-a5d1-356672775fbc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-08T12:45:36.069252Z",
     "iopub.status.busy": "2024-07-08T12:45:36.068826Z",
     "iopub.status.idle": "2024-07-08T12:47:54.704945Z",
     "shell.execute_reply": "2024-07-08T12:47:54.703488Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 138.644759,
     "end_time": "2024-07-08T12:47:54.707073",
     "exception": false,
     "start_time": "2024-07-08T12:45:36.062314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\r\n",
      "Collecting torch==2.3.1\r\n",
      "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2024.3.1)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting triton==2.3.1 (from torch==2.3.1)\r\n",
      "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n",
      "Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.1.2\r\n",
      "    Uninstalling torch-2.1.2:\r\n",
      "      Successfully uninstalled torch-2.1.2\r\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 tiktoken-0.7.0 torch-2.3.1 triton-2.3.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken tqdm datasets torch==2.3.1\n",
    "# by default (at the moment 18/06/2024) kaggle installs pytorch 2.1.something,\n",
    "# which for some reason breaks with the latest version of triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5847ec19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:47:54.861529Z",
     "iopub.status.busy": "2024-07-08T12:47:54.861192Z",
     "iopub.status.idle": "2024-07-08T12:47:56.073727Z",
     "shell.execute_reply": "2024-07-08T12:47:56.072876Z"
    },
    "papermill": {
     "duration": 1.291494,
     "end_time": "2024-07-08T12:47:56.075807",
     "exception": false,
     "start_time": "2024-07-08T12:47:54.784313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-08 12:47:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: 'input.txt'\r\n",
      "\r\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \r\n",
      "\r\n",
      "2024-07-08 12:47:55 (18.4 MB/s) - 'input.txt' saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20145136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:47:56.223998Z",
     "iopub.status.busy": "2024-07-08T12:47:56.223197Z",
     "iopub.status.idle": "2024-07-08T12:47:56.232241Z",
     "shell.execute_reply": "2024-07-08T12:47:56.231330Z"
    },
    "papermill": {
     "duration": 0.086389,
     "end_time": "2024-07-08T12:47:56.234100",
     "exception": false,
     "start_time": "2024-07-08T12:47:56.147711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fineweb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fineweb.py\n",
    "\n",
    "\"\"\"\n",
    "FineWeb-Edu dataset (for srs pretraining)\n",
    "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
    "Downloads and tokenizes the data and saves data shards to disk.\n",
    "Run simply as:\n",
    "$ python fineweb.py\n",
    "Will save shards to the local directory \"edu_fineweb10B\".\n",
    "\"\"\"\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------\n",
    "local_dir = \"edu_fineweb10B\"\n",
    "remote_name = \"sample-10BT\"\n",
    "shard_size = int(1e8) # 100M tokens per shard, total of 100 shards\n",
    "\n",
    "# create the cache if it doesn't exist yet\n",
    "DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), local_dir)\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# download the dataset\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n",
    "\n",
    "# init the tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
    "\n",
    "def tokenize(doc):\n",
    "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
    "    tokens = [eot]\n",
    "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "    return tokens_np_uint16\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "    \n",
    "# tokenize all documents and write output shards, each of shard_size tokens (last shard has remainder)\n",
    "max_shards = 50\n",
    "nprocs = max(1, os.cpu_count()//2)\n",
    "with mp.Pool(nprocs) as pool:\n",
    "    shard_index = 0\n",
    "    # preallocate buffer to hold current shard\n",
    "    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "    token_count = 0\n",
    "    progress_bar = None\n",
    "    for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
    "        \n",
    "        if shard_index > max_shards:\n",
    "            break\n",
    "\n",
    "        # is there enough space in the current shard for the new tokens?\n",
    "        if token_count + len(tokens) < shard_size:\n",
    "            # simply append tokens to current shard\n",
    "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "            token_count += len(tokens)\n",
    "            # update progress bar\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "            progress_bar.update(len(tokens))\n",
    "        else:\n",
    "            # write the current shard and start a new one\n",
    "            split = \"val\" if shard_index == 0 else \"train\"\n",
    "            filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
    "            remainder = shard_size - token_count\n",
    "            progress_bar.update(remainder)\n",
    "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
    "            write_datafile(filename, all_tokens_np)\n",
    "            shard_index += 1\n",
    "            progress_bar = None\n",
    "            # populate the next shard with the leftovers of the current doc\n",
    "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
    "            token_count = len(tokens)-remainder\n",
    "\n",
    "    # write any remaining tokens as the last shard\n",
    "    if token_count != 0 and not shard_index > max_shards:\n",
    "        split = \"val\" if shard_index == 0 else \"train\"\n",
    "        filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "        write_datafile(filename, all_tokens_np[:token_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461fc87d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:47:56.425218Z",
     "iopub.status.busy": "2024-07-08T12:47:56.424851Z",
     "iopub.status.idle": "2024-07-08T12:47:56.428854Z",
     "shell.execute_reply": "2024-07-08T12:47:56.428052Z"
    },
    "papermill": {
     "duration": 0.079319,
     "end_time": "2024-07-08T12:47:56.430881",
     "exception": false,
     "start_time": "2024-07-08T12:47:56.351562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python fineweb.py # run this if you have not loaded the data, else just use the already loaded location TODO check if the data is already loaded or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803054a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:47:56.578788Z",
     "iopub.status.busy": "2024-07-08T12:47:56.578171Z",
     "iopub.status.idle": "2024-07-08T12:47:56.584692Z",
     "shell.execute_reply": "2024-07-08T12:47:56.583886Z"
    },
    "papermill": {
     "duration": 0.083505,
     "end_time": "2024-07-08T12:47:56.587139",
     "exception": false,
     "start_time": "2024-07-08T12:47:56.503634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_loader.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    \n",
    "    return ptt\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T, process_rank, num_processes, split, data_root=\"edu_fineweb10B\"):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.is_master = process_rank == 0\n",
    "        assert split in {'train', 'val'}, f\"Invalid split {split}, must be train or val\"\n",
    "        \n",
    "        # load tokens from disk and store them in memory\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        \n",
    "        assert len(shards) > 0, f\"no shards found for split: {split}\"\n",
    "        if self.is_master:\n",
    "            print(f\"found {len(shards)} shards for split: {split}\")\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds, advance to the next shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4a008f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:47:56.733594Z",
     "iopub.status.busy": "2024-07-08T12:47:56.733227Z",
     "iopub.status.idle": "2024-07-08T12:47:56.746639Z",
     "shell.execute_reply": "2024-07-08T12:47:56.745678Z"
    },
    "papermill": {
     "duration": 0.089113,
     "end_time": "2024-07-08T12:47:56.748626",
     "exception": false,
     "start_time": "2024-07-08T12:47:56.659513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import inspect\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    path: str = \"gpt2\"\n",
    "    min_lr_factor: float = 0.1\n",
    "    warmup_steps: int = 10\n",
    "    max_lr: float = 6e-4\n",
    "    min_lr: float = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # TODO maybe make it so this is done only if min_lr is not defined\n",
    "        self.min_lr = self.max_lr * self.min_lr_factor\n",
    "        \n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to the next batch\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T, T) matrix for all the queries and keys)\n",
    "        \n",
    "#         att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "#         att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "#         att = F.softmax(att, dim=-1)\n",
    "#         y = att @ v # (B , nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output project\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd) # generally, this should be initialized too but pytorch does it correctly\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig, process_rank: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.is_master = process_rank == 0\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        # calculate loss if targets are defined\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained GPT: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head, and n_embd are determined from medel_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),    # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),   # 350M params \n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),   # 774M params \n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),   # 1558M params \n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257   # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024    # always 1024 for GPT model checkpoints\n",
    "        \n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard mask / buffer\n",
    "\n",
    "        # init a higgingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # also ignore\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically, the OpenAI checkpoints use a \"Conv1D\" module, but we only want to use a vanila\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_lr(self, it, max_steps): ## TODO maybe move\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < self.config.warmup_steps:\n",
    "            return self.config.max_lr * (it + 1) / self.config.warmup_steps\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > max_steps:\n",
    "            return self.config.min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - self.config.warmup_steps) / (max_steps - self.config.warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "        return self.config.min_lr + coeff * (self.config.max_lr - self.config.min_lr)\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all of the candidate parameters (that require gradients)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameter that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. weight tensors in matmuls + embeddings decay, all biases and layernorms don't\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused) # fused is slower on kaggle for some reason TODO figure out why\n",
    "        if self.is_master:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters in total\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters in total\")\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195bf23d",
   "metadata": {
    "_cell_guid": "35edd86f-4e81-4576-a17b-187e0c29f353",
    "_uuid": "26d76cd9-11aa-4fc3-94bb-724b5d0da166",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-08T12:47:56.896038Z",
     "iopub.status.busy": "2024-07-08T12:47:56.895689Z",
     "iopub.status.idle": "2024-07-08T12:47:56.905275Z",
     "shell.execute_reply": "2024-07-08T12:47:56.904434Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.085705,
     "end_time": "2024-07-08T12:47:56.907215",
     "exception": false,
     "start_time": "2024-07-08T12:47:56.821510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from data_loader import DataLoader\n",
    "from model import GPT, GPTConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ___________________________________Set up DDP_____________________________________\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), \"for now we need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla process\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process= True\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "\n",
    "print(f\"using device: {device}\")\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "    \n",
    "total_batch_size = 524288 # we see (roughly) half a shard per batch\n",
    "B = 8\n",
    "T = 1024\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f\"total desired batch size: {total_batch_size}\")\n",
    "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "train_loader = DataLoader(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\", data_root=\"/kaggle/input/finewebedu-5b\")\n",
    "val_loader = DataLoader(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\", data_root=\"/kaggle/input/finewebedu-5b\")\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig(warmup_steps = 715), ddp_rank)\n",
    "model.to(device)\n",
    "model = torch.compile(model) # TODO explain this better ?\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    \n",
    "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "# max_steps = 9536 # We only load half of Fineweb-EDU due to drive constraints\n",
    "max_steps = 1000 # defo not enough but cant do more due to GPU compute constraints\n",
    "\n",
    "# optimize! the model\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), eps=1e-8) # TODO read about AdamW\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
    "    pass\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "val_step = 25\n",
    "\n",
    "for step in range(max_steps):\n",
    "    start = time.time()\n",
    "    last_step = (step == max_steps -1)\n",
    "    \n",
    "    if step % val_step == 0 or last_step: #abstract this into a property on the model config or other constant\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            \n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                with torch.autocast(device_type=device_type, dtype=torch.float16):\n",
    "                    logits, loss = model(x, y)\n",
    "                \n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "                \n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "                if step > 0 and (step % 500 == 0 or last_step):\n",
    "                    # optionally write model checkpoints\n",
    "                    checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
    "                    checkpoint = {\n",
    "                        'model': raw_model.state_dict(),\n",
    "                        'config': raw_model.config,\n",
    "                        'step': step,\n",
    "                        'val_loss': val_loss_accum.item()\n",
    "                    }\n",
    "                    # you might also want to add optimizer.state_dict() and\n",
    "                    # rng seeds etc., if you want to resume training\n",
    "                    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    \n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1) # switch to the DDP context manager\n",
    "            \n",
    "        with torch.autocast(device_type=device_type, dtype=torch.float16): \n",
    "            # TODO look into how to get a gradient scaler working\n",
    "            logits, loss = model(x, y)\n",
    "        \n",
    "        # we have to scale the loss to account for gradient accumulation\n",
    "        # because the gradients just add on each successive backward()\n",
    "        # addition of gradients corresponds to a SUM in the objective, but\n",
    "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    scaler.unscale_(optimizer)\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = model.module.get_lr(step, max_steps) # try out the scheduler provided by pytorch (if there are any)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    miliseconds = total_time*1000\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / total_time\n",
    "    \n",
    "    if master_process:\n",
    "        log_str = f\"step: {step:5d} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | time: {miliseconds:.2f}ms | tok/sec: {tokens_per_sec:.2f}\"\n",
    "        print(log_str)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{log_str}\\n\")\n",
    "    \n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd70006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:47:57.055108Z",
     "iopub.status.busy": "2024-07-08T12:47:57.054090Z",
     "iopub.status.idle": "2024-07-08T16:58:50.326365Z",
     "shell.execute_reply": "2024-07-08T16:58:50.324807Z"
    },
    "papermill": {
     "duration": 15053.349358,
     "end_time": "2024-07-08T16:58:50.329116",
     "exception": false,
     "start_time": "2024-07-08T12:47:56.979758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0708 12:47:59.853000 135380890093376 torch/distributed/run.py:757] \r\n",
      "W0708 12:47:59.853000 135380890093376 torch/distributed/run.py:757] *****************************************\r\n",
      "W0708 12:47:59.853000 135380890093376 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0708 12:47:59.853000 135380890093376 torch/distributed/run.py:757] *****************************************\r\n",
      "using device: cuda:0\r\n",
      "total desired batch size: 524288\r\n",
      "=> calculated gradient accumulation steps: 32\r\n",
      "found 50 shards for split: train\r\n",
      "using device: cuda:1\r\n",
      "found 1 shards for split: val\r\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters in total\r\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters in total\r\n",
      "using fused AdamW: True\r\n",
      "validation loss: 10.9499\r\n",
      "step:     0 | loss: 10.954966 | lr: 8.3916e-07 | norm: 15.3500 | time: 93235.62ms | tok/sec: 5623.26\r\n",
      "step:     1 | loss: 10.902684 | lr: 1.6783e-06 | norm: 14.8739 | time: 14234.85ms | tok/sec: 36831.30\r\n",
      "step:     2 | loss: 10.804056 | lr: 2.5175e-06 | norm: 14.4742 | time: 14919.86ms | tok/sec: 35140.29\r\n",
      "step:     3 | loss: 10.662260 | lr: 3.3566e-06 | norm: 12.9423 | time: 15487.93ms | tok/sec: 33851.39\r\n",
      "step:     4 | loss: 10.518734 | lr: 4.1958e-06 | norm: 10.5806 | time: 15079.82ms | tok/sec: 34767.53\r\n",
      "step:     5 | loss: 10.377098 | lr: 5.0350e-06 | norm: 8.8725 | time: 14914.83ms | tok/sec: 35152.12\r\n",
      "step:     6 | loss: 10.257944 | lr: 5.8741e-06 | norm: 7.5769 | time: 14902.58ms | tok/sec: 35181.01\r\n",
      "step:     7 | loss: 10.147922 | lr: 6.7133e-06 | norm: 6.4484 | time: 14728.84ms | tok/sec: 35596.00\r\n",
      "step:     8 | loss: 10.037144 | lr: 7.5524e-06 | norm: 5.5088 | time: 14933.18ms | tok/sec: 35108.94\r\n",
      "step:     9 | loss: 9.962893 | lr: 8.3916e-06 | norm: 4.5983 | time: 14971.57ms | tok/sec: 35018.90\r\n",
      "step:    10 | loss: 9.877531 | lr: 9.2308e-06 | norm: 3.8993 | time: 14833.56ms | tok/sec: 35344.71\r\n",
      "step:    11 | loss: 9.830208 | lr: 1.0070e-05 | norm: 3.3752 | time: 14682.93ms | tok/sec: 35707.32\r\n",
      "step:    12 | loss: 9.783779 | lr: 1.0909e-05 | norm: 2.9928 | time: 14665.11ms | tok/sec: 35750.69\r\n",
      "step:    13 | loss: 9.746510 | lr: 1.1748e-05 | norm: 2.7035 | time: 14700.35ms | tok/sec: 35664.99\r\n",
      "step:    14 | loss: 9.686831 | lr: 1.2587e-05 | norm: 2.5526 | time: 14731.84ms | tok/sec: 35588.77\r\n",
      "step:    15 | loss: 9.664825 | lr: 1.3427e-05 | norm: 2.4613 | time: 14778.72ms | tok/sec: 35475.87\r\n",
      "step:    16 | loss: 9.650267 | lr: 1.4266e-05 | norm: 2.3388 | time: 14744.87ms | tok/sec: 35557.30\r\n",
      "step:    17 | loss: 9.627310 | lr: 1.5105e-05 | norm: 2.2904 | time: 14741.80ms | tok/sec: 35564.72\r\n",
      "step:    18 | loss: 9.676426 | lr: 1.5944e-05 | norm: 2.2974 | time: 14712.90ms | tok/sec: 35634.59\r\n",
      "step:    19 | loss: 9.669887 | lr: 1.6783e-05 | norm: 2.4032 | time: 14688.60ms | tok/sec: 35693.53\r\n",
      "step:    20 | loss: 9.573683 | lr: 1.7622e-05 | norm: 2.2400 | time: 14636.80ms | tok/sec: 35819.86\r\n",
      "step:    21 | loss: 9.538756 | lr: 1.8462e-05 | norm: 2.2891 | time: 14609.56ms | tok/sec: 35886.65\r\n",
      "step:    22 | loss: 9.523468 | lr: 1.9301e-05 | norm: 2.2454 | time: 14617.37ms | tok/sec: 35867.47\r\n",
      "step:    23 | loss: 9.509946 | lr: 2.0140e-05 | norm: 2.2008 | time: 14630.45ms | tok/sec: 35835.40\r\n",
      "step:    24 | loss: 9.447868 | lr: 2.0979e-05 | norm: 2.2087 | time: 14611.44ms | tok/sec: 35882.03\r\n",
      "validation loss: 9.4212\r\n",
      "step:    25 | loss: 9.412796 | lr: 2.1818e-05 | norm: 2.1772 | time: 18343.92ms | tok/sec: 28581.02\r\n",
      "step:    26 | loss: 9.396200 | lr: 2.2657e-05 | norm: 2.0770 | time: 14634.43ms | tok/sec: 35825.66\r\n",
      "step:    27 | loss: 9.351007 | lr: 2.3497e-05 | norm: 2.1157 | time: 14658.68ms | tok/sec: 35766.39\r\n",
      "step:    28 | loss: 9.324081 | lr: 2.4336e-05 | norm: 2.1153 | time: 14685.29ms | tok/sec: 35701.59\r\n",
      "step:    29 | loss: 9.272934 | lr: 2.5175e-05 | norm: 2.1103 | time: 14719.30ms | tok/sec: 35619.08\r\n",
      "step:    30 | loss: 9.308473 | lr: 2.6014e-05 | norm: 2.0511 | time: 14713.88ms | tok/sec: 35632.20\r\n",
      "step:    31 | loss: 9.232485 | lr: 2.6853e-05 | norm: 2.0055 | time: 14703.39ms | tok/sec: 35657.62\r\n",
      "step:    32 | loss: 9.208766 | lr: 2.7692e-05 | norm: 1.9189 | time: 14711.86ms | tok/sec: 35637.10\r\n",
      "step:    33 | loss: 9.165339 | lr: 2.8531e-05 | norm: 1.9994 | time: 14689.15ms | tok/sec: 35692.19\r\n",
      "step:    34 | loss: 9.061462 | lr: 2.9371e-05 | norm: 2.1884 | time: 14673.94ms | tok/sec: 35729.18\r\n",
      "step:    35 | loss: 9.071215 | lr: 3.0210e-05 | norm: 2.6443 | time: 14701.82ms | tok/sec: 35661.43\r\n",
      "step:    36 | loss: 9.037258 | lr: 3.1049e-05 | norm: 2.1204 | time: 14685.44ms | tok/sec: 35701.22\r\n",
      "step:    37 | loss: 8.962355 | lr: 3.1888e-05 | norm: 1.9391 | time: 14684.58ms | tok/sec: 35703.30\r\n",
      "step:    38 | loss: 8.944944 | lr: 3.2727e-05 | norm: 2.2144 | time: 14697.73ms | tok/sec: 35671.36\r\n",
      "step:    39 | loss: 8.922291 | lr: 3.3566e-05 | norm: 1.9489 | time: 14728.23ms | tok/sec: 35597.48\r\n",
      "step:    40 | loss: 8.889414 | lr: 3.4406e-05 | norm: 1.8095 | time: 14731.16ms | tok/sec: 35590.40\r\n",
      "step:    41 | loss: 8.815395 | lr: 3.5245e-05 | norm: 1.9008 | time: 14730.80ms | tok/sec: 35591.29\r\n",
      "step:    42 | loss: 8.769211 | lr: 3.6084e-05 | norm: 2.0287 | time: 14739.11ms | tok/sec: 35571.21\r\n",
      "step:    43 | loss: 8.780380 | lr: 3.6923e-05 | norm: 1.8367 | time: 14729.96ms | tok/sec: 35593.30\r\n",
      "step:    44 | loss: 8.730776 | lr: 3.7762e-05 | norm: 1.7506 | time: 14722.76ms | tok/sec: 35610.72\r\n",
      "step:    45 | loss: 8.717600 | lr: 3.8601e-05 | norm: 1.8616 | time: 14750.04ms | tok/sec: 35544.86\r\n",
      "step:    46 | loss: 8.661514 | lr: 3.9441e-05 | norm: 1.7744 | time: 14716.82ms | tok/sec: 35625.10\r\n",
      "step:    47 | loss: 8.670359 | lr: 4.0280e-05 | norm: 1.7325 | time: 14742.69ms | tok/sec: 35562.58\r\n",
      "step:    48 | loss: 8.661343 | lr: 4.1119e-05 | norm: 1.6660 | time: 14722.12ms | tok/sec: 35612.26\r\n",
      "step:    49 | loss: 8.579149 | lr: 4.1958e-05 | norm: 1.6995 | time: 14734.62ms | tok/sec: 35582.04\r\n",
      "validation loss: 8.5740\r\n",
      "step:    50 | loss: 8.583591 | lr: 4.2797e-05 | norm: 1.6188 | time: 18180.17ms | tok/sec: 28838.45\r\n",
      "step:    51 | loss: 8.543600 | lr: 4.3636e-05 | norm: 1.6125 | time: 14685.95ms | tok/sec: 35699.98\r\n",
      "step:    52 | loss: 8.500718 | lr: 4.4476e-05 | norm: 1.6130 | time: 14727.13ms | tok/sec: 35600.15\r\n",
      "step:    53 | loss: 8.451038 | lr: 4.5315e-05 | norm: 1.7792 | time: 14745.04ms | tok/sec: 35556.91\r\n",
      "step:    54 | loss: 8.460245 | lr: 4.6154e-05 | norm: 1.9618 | time: 14719.85ms | tok/sec: 35617.75\r\n",
      "step:    55 | loss: 8.424748 | lr: 4.6993e-05 | norm: 1.6571 | time: 14706.15ms | tok/sec: 35650.93\r\n",
      "step:    56 | loss: 8.374099 | lr: 4.7832e-05 | norm: 1.5058 | time: 14720.66ms | tok/sec: 35615.80\r\n",
      "step:    57 | loss: 8.357931 | lr: 4.8671e-05 | norm: 1.6398 | time: 14700.44ms | tok/sec: 35664.78\r\n",
      "step:    58 | loss: 8.281681 | lr: 4.9510e-05 | norm: 1.8605 | time: 14695.72ms | tok/sec: 35676.24\r\n",
      "step:    59 | loss: 8.245053 | lr: 5.0350e-05 | norm: 1.4734 | time: 14691.38ms | tok/sec: 35686.77\r\n",
      "step:    60 | loss: 8.229914 | lr: 5.1189e-05 | norm: 1.3857 | time: 14685.27ms | tok/sec: 35701.63\r\n",
      "step:    61 | loss: 8.230469 | lr: 5.2028e-05 | norm: 1.4615 | time: 14698.85ms | tok/sec: 35668.64\r\n",
      "step:    62 | loss: 8.128563 | lr: 5.2867e-05 | norm: 1.5025 | time: 14716.32ms | tok/sec: 35626.29\r\n",
      "step:    63 | loss: 8.091398 | lr: 5.3706e-05 | norm: 1.4169 | time: 14713.23ms | tok/sec: 35633.77\r\n",
      "step:    64 | loss: 8.060192 | lr: 5.4545e-05 | norm: 1.3169 | time: 14700.36ms | tok/sec: 35664.97\r\n",
      "step:    65 | loss: 8.049734 | lr: 5.5385e-05 | norm: 1.3873 | time: 14695.83ms | tok/sec: 35675.96\r\n",
      "step:    66 | loss: 7.988317 | lr: 5.6224e-05 | norm: 1.3327 | time: 14717.44ms | tok/sec: 35623.59\r\n",
      "step:    67 | loss: 7.985086 | lr: 5.7063e-05 | norm: 1.3195 | time: 14714.66ms | tok/sec: 35630.31\r\n",
      "step:    68 | loss: 7.946204 | lr: 5.7902e-05 | norm: 1.3644 | time: 14715.59ms | tok/sec: 35628.05\r\n",
      "step:    69 | loss: 7.855059 | lr: 5.8741e-05 | norm: 1.4002 | time: 14730.98ms | tok/sec: 35590.83\r\n",
      "step:    70 | loss: 7.809804 | lr: 5.9580e-05 | norm: 1.3269 | time: 14740.87ms | tok/sec: 35566.96\r\n",
      "step:    71 | loss: 7.823190 | lr: 6.0420e-05 | norm: 1.3391 | time: 14736.57ms | tok/sec: 35577.35\r\n",
      "step:    72 | loss: 7.753149 | lr: 6.1259e-05 | norm: 1.1041 | time: 14721.71ms | tok/sec: 35613.26\r\n",
      "step:    73 | loss: 7.726448 | lr: 6.2098e-05 | norm: 1.4380 | time: 14755.01ms | tok/sec: 35532.87\r\n",
      "step:    74 | loss: 7.668313 | lr: 6.2937e-05 | norm: 1.6142 | time: 14738.45ms | tok/sec: 35572.80\r\n",
      "validation loss: 7.7298\r\n",
      "step:    75 | loss: 7.683303 | lr: 6.3776e-05 | norm: 1.3780 | time: 18260.15ms | tok/sec: 28712.14\r\n",
      "step:    76 | loss: 7.614807 | lr: 6.4615e-05 | norm: 1.4580 | time: 14702.02ms | tok/sec: 35660.95\r\n",
      "step:    77 | loss: 7.624469 | lr: 6.5455e-05 | norm: 1.1919 | time: 14689.17ms | tok/sec: 35692.16\r\n",
      "step:    78 | loss: 7.587448 | lr: 6.6294e-05 | norm: 1.7233 | time: 14737.89ms | tok/sec: 35574.15\r\n",
      "step:    79 | loss: 7.522645 | lr: 6.7133e-05 | norm: 1.0268 | time: 14741.16ms | tok/sec: 35566.27\r\n",
      "step:    80 | loss: 7.502213 | lr: 6.7972e-05 | norm: 1.3643 | time: 14733.87ms | tok/sec: 35583.85\r\n",
      "step:    81 | loss: 7.504260 | lr: 6.8811e-05 | norm: 1.2166 | time: 14769.71ms | tok/sec: 35497.50\r\n",
      "step:    82 | loss: 7.436335 | lr: 6.9650e-05 | norm: 1.1283 | time: 14770.51ms | tok/sec: 35495.59\r\n",
      "step:    83 | loss: 7.383090 | lr: 7.0490e-05 | norm: 1.0514 | time: 14762.06ms | tok/sec: 35515.91\r\n",
      "step:    84 | loss: 7.381988 | lr: 7.1329e-05 | norm: 1.0656 | time: 14738.66ms | tok/sec: 35572.29\r\n",
      "step:    85 | loss: 7.335545 | lr: 7.2168e-05 | norm: 1.1386 | time: 14692.14ms | tok/sec: 35684.93\r\n",
      "step:    86 | loss: 7.317106 | lr: 7.3007e-05 | norm: 0.8765 | time: 14649.29ms | tok/sec: 35789.31\r\n",
      "step:    87 | loss: 7.323621 | lr: 7.3846e-05 | norm: 0.8754 | time: 14680.99ms | tok/sec: 35712.03\r\n",
      "step:    88 | loss: 7.294918 | lr: 7.4685e-05 | norm: 0.8250 | time: 14709.08ms | tok/sec: 35643.84\r\n",
      "step:    89 | loss: 7.334115 | lr: 7.5524e-05 | norm: 1.2540 | time: 14705.08ms | tok/sec: 35653.53\r\n",
      "step:    90 | loss: 7.247079 | lr: 7.6364e-05 | norm: 1.2045 | time: 14720.90ms | tok/sec: 35615.22\r\n",
      "step:    91 | loss: 7.233954 | lr: 7.7203e-05 | norm: 1.4828 | time: 14734.03ms | tok/sec: 35583.48\r\n",
      "step:    92 | loss: 7.258459 | lr: 7.8042e-05 | norm: 0.8203 | time: 14757.60ms | tok/sec: 35526.64\r\n",
      "step:    93 | loss: 7.317090 | lr: 7.8881e-05 | norm: 0.7430 | time: 14761.07ms | tok/sec: 35518.30\r\n",
      "step:    94 | loss: 7.321705 | lr: 7.9720e-05 | norm: 0.7645 | time: 14778.12ms | tok/sec: 35477.32\r\n",
      "step:    95 | loss: 7.304366 | lr: 8.0559e-05 | norm: 0.8265 | time: 14769.82ms | tok/sec: 35497.26\r\n",
      "step:    96 | loss: 7.259231 | lr: 8.1399e-05 | norm: 0.9580 | time: 14756.99ms | tok/sec: 35528.12\r\n",
      "step:    97 | loss: 7.217330 | lr: 8.2238e-05 | norm: 1.2148 | time: 14786.87ms | tok/sec: 35456.32\r\n",
      "step:    98 | loss: 7.239300 | lr: 8.3077e-05 | norm: 0.6421 | time: 14762.86ms | tok/sec: 35513.99\r\n",
      "step:    99 | loss: 7.189620 | lr: 8.3916e-05 | norm: 1.3610 | time: 14759.03ms | tok/sec: 35523.21\r\n",
      "validation loss: 7.1874\r\n",
      "step:   100 | loss: 7.242380 | lr: 8.4755e-05 | norm: 1.1989 | time: 18301.34ms | tok/sec: 28647.51\r\n",
      "step:   101 | loss: 7.160427 | lr: 8.5594e-05 | norm: 1.2976 | time: 14765.26ms | tok/sec: 35508.22\r\n",
      "step:   102 | loss: 7.143790 | lr: 8.6434e-05 | norm: 0.7927 | time: 14735.80ms | tok/sec: 35579.20\r\n",
      "step:   103 | loss: 7.031531 | lr: 8.7273e-05 | norm: 0.8738 | time: 14799.44ms | tok/sec: 35426.20\r\n",
      "step:   104 | loss: 7.117088 | lr: 8.8112e-05 | norm: 0.8043 | time: 14709.01ms | tok/sec: 35644.00\r\n",
      "step:   105 | loss: 7.133275 | lr: 8.8951e-05 | norm: 1.6739 | time: 14741.97ms | tok/sec: 35564.31\r\n",
      "step:   106 | loss: 7.078355 | lr: 8.9790e-05 | norm: 1.0178 | time: 14717.30ms | tok/sec: 35623.92\r\n",
      "step:   107 | loss: 7.019475 | lr: 9.0629e-05 | norm: 1.3270 | time: 14706.15ms | tok/sec: 35650.93\r\n",
      "step:   108 | loss: 7.080756 | lr: 9.1469e-05 | norm: 1.0084 | time: 14721.49ms | tok/sec: 35613.79\r\n",
      "step:   109 | loss: 7.095032 | lr: 9.2308e-05 | norm: 0.8214 | time: 14698.57ms | tok/sec: 35669.31\r\n",
      "step:   110 | loss: 7.026657 | lr: 9.3147e-05 | norm: 0.8653 | time: 14712.64ms | tok/sec: 35635.22\r\n",
      "step:   111 | loss: 6.999876 | lr: 9.3986e-05 | norm: 0.8334 | time: 14699.43ms | tok/sec: 35667.23\r\n",
      "step:   112 | loss: 6.969057 | lr: 9.4825e-05 | norm: 0.9291 | time: 14765.64ms | tok/sec: 35507.30\r\n",
      "step:   113 | loss: 7.023218 | lr: 9.5664e-05 | norm: 0.7110 | time: 14818.85ms | tok/sec: 35379.81\r\n",
      "step:   114 | loss: 7.019284 | lr: 9.6503e-05 | norm: 0.8654 | time: 14795.84ms | tok/sec: 35434.83\r\n",
      "step:   115 | loss: 6.986910 | lr: 9.7343e-05 | norm: 1.0720 | time: 14854.39ms | tok/sec: 35295.16\r\n",
      "step:   116 | loss: 6.906488 | lr: 9.8182e-05 | norm: 1.1465 | time: 14845.03ms | tok/sec: 35317.40\r\n",
      "step:   117 | loss: 6.832927 | lr: 9.9021e-05 | norm: 1.5391 | time: 14837.51ms | tok/sec: 35335.30\r\n",
      "step:   118 | loss: 6.846679 | lr: 9.9860e-05 | norm: 0.5792 | time: 14795.51ms | tok/sec: 35435.61\r\n",
      "step:   119 | loss: 6.902101 | lr: 1.0070e-04 | norm: 1.4018 | time: 14779.53ms | tok/sec: 35473.93\r\n",
      "step:   120 | loss: 6.849763 | lr: 1.0154e-04 | norm: 0.9547 | time: 14758.68ms | tok/sec: 35524.05\r\n",
      "step:   121 | loss: 6.833216 | lr: 1.0238e-04 | norm: 0.9177 | time: 14736.70ms | tok/sec: 35577.03\r\n",
      "step:   122 | loss: 6.823363 | lr: 1.0322e-04 | norm: 0.9826 | time: 14713.38ms | tok/sec: 35633.42\r\n",
      "step:   123 | loss: 6.802236 | lr: 1.0406e-04 | norm: 0.5939 | time: 14721.49ms | tok/sec: 35613.77\r\n",
      "step:   124 | loss: 6.821525 | lr: 1.0490e-04 | norm: 0.6799 | time: 14719.67ms | tok/sec: 35618.20\r\n",
      "validation loss: 6.8782\r\n",
      "step:   125 | loss: 6.807854 | lr: 1.0573e-04 | norm: 0.6937 | time: 18292.13ms | tok/sec: 28661.94\r\n",
      "step:   126 | loss: 6.802768 | lr: 1.0657e-04 | norm: 0.8772 | time: 14800.45ms | tok/sec: 35423.78\r\n",
      "step:   127 | loss: 6.757718 | lr: 1.0741e-04 | norm: 1.4620 | time: 14870.39ms | tok/sec: 35257.19\r\n",
      "step:   128 | loss: 6.717148 | lr: 1.0825e-04 | norm: 0.7466 | time: 14844.19ms | tok/sec: 35319.41\r\n",
      "step:   129 | loss: 6.694173 | lr: 1.0909e-04 | norm: 0.8767 | time: 14837.46ms | tok/sec: 35335.42\r\n",
      "step:   130 | loss: 6.758954 | lr: 1.0993e-04 | norm: 0.9247 | time: 14820.66ms | tok/sec: 35375.48\r\n",
      "step:   131 | loss: 6.709847 | lr: 1.1077e-04 | norm: 0.9267 | time: 14815.13ms | tok/sec: 35388.70\r\n",
      "step:   132 | loss: 6.718422 | lr: 1.1161e-04 | norm: 1.1894 | time: 14793.20ms | tok/sec: 35441.14\r\n",
      "step:   133 | loss: 6.621114 | lr: 1.1245e-04 | norm: 0.9627 | time: 14791.12ms | tok/sec: 35446.12\r\n",
      "step:   134 | loss: 6.615801 | lr: 1.1329e-04 | norm: 0.9851 | time: 14796.78ms | tok/sec: 35432.57\r\n",
      "step:   135 | loss: 6.695484 | lr: 1.1413e-04 | norm: 1.1689 | time: 14787.95ms | tok/sec: 35453.73\r\n",
      "step:   136 | loss: 6.663959 | lr: 1.1497e-04 | norm: 1.1651 | time: 14812.21ms | tok/sec: 35395.66\r\n",
      "step:   137 | loss: 6.682221 | lr: 1.1580e-04 | norm: 1.0098 | time: 14801.22ms | tok/sec: 35421.95\r\n",
      "step:   138 | loss: 6.612821 | lr: 1.1664e-04 | norm: 1.3298 | time: 14766.58ms | tok/sec: 35505.03\r\n",
      "step:   139 | loss: 6.702150 | lr: 1.1748e-04 | norm: 0.9295 | time: 14773.35ms | tok/sec: 35488.78\r\n",
      "step:   140 | loss: 6.823023 | lr: 1.1832e-04 | norm: 1.3272 | time: 14752.30ms | tok/sec: 35539.40\r\n",
      "step:   141 | loss: 6.699667 | lr: 1.1916e-04 | norm: 1.1222 | time: 14762.00ms | tok/sec: 35516.05\r\n",
      "step:   142 | loss: 6.755795 | lr: 1.2000e-04 | norm: 0.9106 | time: 14737.81ms | tok/sec: 35574.35\r\n",
      "step:   143 | loss: 6.692546 | lr: 1.2084e-04 | norm: 1.2472 | time: 14736.32ms | tok/sec: 35577.95\r\n",
      "step:   144 | loss: 6.690932 | lr: 1.2168e-04 | norm: 0.7753 | time: 14702.91ms | tok/sec: 35658.80\r\n",
      "step:   145 | loss: 6.685322 | lr: 1.2252e-04 | norm: 0.6888 | time: 14694.76ms | tok/sec: 35678.56\r\n",
      "step:   146 | loss: 6.689749 | lr: 1.2336e-04 | norm: 0.7176 | time: 14734.79ms | tok/sec: 35581.63\r\n",
      "step:   147 | loss: 6.686793 | lr: 1.2420e-04 | norm: 0.6460 | time: 14739.99ms | tok/sec: 35569.09\r\n",
      "step:   148 | loss: 6.682903 | lr: 1.2503e-04 | norm: 0.6462 | time: 14757.50ms | tok/sec: 35526.89\r\n",
      "step:   149 | loss: 6.669599 | lr: 1.2587e-04 | norm: 0.7082 | time: 14736.25ms | tok/sec: 35578.12\r\n",
      "validation loss: 6.6556\r\n",
      "step:   150 | loss: 6.666016 | lr: 1.2671e-04 | norm: 1.2793 | time: 18298.85ms | tok/sec: 28651.43\r\n",
      "step:   151 | loss: 6.582060 | lr: 1.2755e-04 | norm: 1.3429 | time: 14816.33ms | tok/sec: 35385.82\r\n",
      "step:   152 | loss: 6.596015 | lr: 1.2839e-04 | norm: 0.6809 | time: 14803.48ms | tok/sec: 35416.53\r\n",
      "step:   153 | loss: 6.664003 | lr: 1.2923e-04 | norm: 0.9001 | time: 14843.22ms | tok/sec: 35321.73\r\n",
      "step:   154 | loss: 6.572103 | lr: 1.3007e-04 | norm: 1.2200 | time: 14846.53ms | tok/sec: 35313.85\r\n",
      "step:   155 | loss: 6.538552 | lr: 1.3091e-04 | norm: 1.2103 | time: 14843.30ms | tok/sec: 35321.53\r\n",
      "step:   156 | loss: 6.603547 | lr: 1.3175e-04 | norm: 0.8999 | time: 14796.33ms | tok/sec: 35433.65\r\n",
      "step:   157 | loss: 6.655909 | lr: 1.3259e-04 | norm: 1.2301 | time: 14819.77ms | tok/sec: 35377.60\r\n",
      "step:   158 | loss: 6.636684 | lr: 1.3343e-04 | norm: 0.7896 | time: 14791.28ms | tok/sec: 35445.76\r\n",
      "step:   159 | loss: 6.556610 | lr: 1.3427e-04 | norm: 1.0067 | time: 14788.85ms | tok/sec: 35451.57\r\n",
      "step:   160 | loss: 6.506057 | lr: 1.3510e-04 | norm: 1.3768 | time: 14834.73ms | tok/sec: 35341.94\r\n",
      "step:   161 | loss: 6.579947 | lr: 1.3594e-04 | norm: 0.5966 | time: 14819.08ms | tok/sec: 35379.25\r\n",
      "step:   162 | loss: 6.478987 | lr: 1.3678e-04 | norm: 0.9391 | time: 14841.63ms | tok/sec: 35325.51\r\n",
      "step:   163 | loss: 6.441220 | lr: 1.3762e-04 | norm: 0.9606 | time: 14793.31ms | tok/sec: 35440.87\r\n",
      "step:   164 | loss: 6.466389 | lr: 1.3846e-04 | norm: 0.7123 | time: 14804.98ms | tok/sec: 35412.94\r\n",
      "step:   165 | loss: 6.508305 | lr: 1.3930e-04 | norm: 1.2133 | time: 14800.17ms | tok/sec: 35424.46\r\n",
      "step:   166 | loss: 6.350541 | lr: 1.4014e-04 | norm: 0.9148 | time: 14777.91ms | tok/sec: 35477.82\r\n",
      "step:   167 | loss: 6.455278 | lr: 1.4098e-04 | norm: 1.1805 | time: 14773.66ms | tok/sec: 35488.02\r\n",
      "step:   168 | loss: 6.427141 | lr: 1.4182e-04 | norm: 1.5245 | time: 14771.94ms | tok/sec: 35492.16\r\n",
      "step:   169 | loss: 6.452315 | lr: 1.4266e-04 | norm: 1.0345 | time: 14760.76ms | tok/sec: 35519.03\r\n",
      "step:   170 | loss: 6.421737 | lr: 1.4350e-04 | norm: 0.9573 | time: 14777.09ms | tok/sec: 35479.79\r\n",
      "step:   171 | loss: 6.453283 | lr: 1.4434e-04 | norm: 1.0759 | time: 14805.59ms | tok/sec: 35411.48\r\n",
      "step:   172 | loss: 6.438627 | lr: 1.4517e-04 | norm: 1.1773 | time: 14787.66ms | tok/sec: 35454.44\r\n",
      "step:   173 | loss: 6.384998 | lr: 1.4601e-04 | norm: 1.0543 | time: 14824.43ms | tok/sec: 35366.49\r\n",
      "step:   174 | loss: 6.351953 | lr: 1.4685e-04 | norm: 0.8149 | time: 14819.49ms | tok/sec: 35378.27\r\n",
      "validation loss: 6.4942\r\n",
      "step:   175 | loss: 6.428369 | lr: 1.4769e-04 | norm: 1.2436 | time: 18392.45ms | tok/sec: 28505.60\r\n",
      "step:   176 | loss: 6.361930 | lr: 1.4853e-04 | norm: 0.9794 | time: 14830.36ms | tok/sec: 35352.34\r\n",
      "step:   177 | loss: 6.395126 | lr: 1.4937e-04 | norm: 0.9215 | time: 14852.07ms | tok/sec: 35300.67\r\n",
      "step:   178 | loss: 6.330128 | lr: 1.5021e-04 | norm: 0.8570 | time: 14861.19ms | tok/sec: 35279.00\r\n",
      "step:   179 | loss: 6.312511 | lr: 1.5105e-04 | norm: 1.1195 | time: 14902.12ms | tok/sec: 35182.12\r\n",
      "step:   180 | loss: 6.312122 | lr: 1.5189e-04 | norm: 1.3082 | time: 14862.97ms | tok/sec: 35274.77\r\n",
      "step:   181 | loss: 6.339897 | lr: 1.5273e-04 | norm: 1.0185 | time: 14870.20ms | tok/sec: 35257.62\r\n",
      "step:   182 | loss: 6.332840 | lr: 1.5357e-04 | norm: 1.0256 | time: 14885.28ms | tok/sec: 35221.92\r\n",
      "step:   183 | loss: 6.320779 | lr: 1.5441e-04 | norm: 0.8583 | time: 14899.10ms | tok/sec: 35189.23\r\n",
      "step:   184 | loss: 6.350300 | lr: 1.5524e-04 | norm: 1.0071 | time: 14828.55ms | tok/sec: 35356.66\r\n",
      "step:   185 | loss: 6.401705 | lr: 1.5608e-04 | norm: 1.1584 | time: 14843.44ms | tok/sec: 35321.19\r\n",
      "step:   186 | loss: 6.423823 | lr: 1.5692e-04 | norm: 1.2111 | time: 14773.93ms | tok/sec: 35487.37\r\n",
      "step:   187 | loss: 6.484317 | lr: 1.5776e-04 | norm: 1.0349 | time: 14775.66ms | tok/sec: 35483.23\r\n",
      "step:   188 | loss: 6.435096 | lr: 1.5860e-04 | norm: 1.0285 | time: 14757.11ms | tok/sec: 35527.83\r\n",
      "step:   189 | loss: 6.406527 | lr: 1.5944e-04 | norm: 0.9439 | time: 14779.75ms | tok/sec: 35473.40\r\n",
      "step:   190 | loss: 6.447931 | lr: 1.6028e-04 | norm: 0.7177 | time: 17323.55ms | tok/sec: 30264.46\r\n",
      "step:   191 | loss: 6.393440 | lr: 1.6112e-04 | norm: 0.7566 | time: 14820.73ms | tok/sec: 35375.31\r\n",
      "step:   192 | loss: 6.400276 | lr: 1.6196e-04 | norm: 0.8367 | time: 14941.36ms | tok/sec: 35089.72\r\n",
      "step:   193 | loss: 6.413674 | lr: 1.6280e-04 | norm: 0.6131 | time: 14873.05ms | tok/sec: 35250.88\r\n",
      "step:   194 | loss: 6.323364 | lr: 1.6364e-04 | norm: 0.7466 | time: 14713.94ms | tok/sec: 35632.05\r\n",
      "step:   195 | loss: 6.336142 | lr: 1.6448e-04 | norm: 0.7244 | time: 14675.35ms | tok/sec: 35725.75\r\n",
      "step:   196 | loss: 6.319429 | lr: 1.6531e-04 | norm: 0.7758 | time: 14760.30ms | tok/sec: 35520.16\r\n",
      "step:   197 | loss: 6.362393 | lr: 1.6615e-04 | norm: 1.2102 | time: 14820.68ms | tok/sec: 35375.44\r\n",
      "step:   198 | loss: 6.366535 | lr: 1.6699e-04 | norm: 1.0403 | time: 14881.80ms | tok/sec: 35230.14\r\n",
      "step:   199 | loss: 6.325089 | lr: 1.6783e-04 | norm: 0.9070 | time: 14897.35ms | tok/sec: 35193.37\r\n",
      "validation loss: 6.3576\r\n",
      "step:   200 | loss: 6.327371 | lr: 1.6867e-04 | norm: 0.8759 | time: 19756.44ms | tok/sec: 26537.57\r\n",
      "step:   201 | loss: 6.304954 | lr: 1.6951e-04 | norm: 0.9775 | time: 14916.48ms | tok/sec: 35148.24\r\n",
      "step:   202 | loss: 6.317629 | lr: 1.7035e-04 | norm: 0.9965 | time: 14860.21ms | tok/sec: 35281.32\r\n",
      "step:   203 | loss: 6.314726 | lr: 1.7119e-04 | norm: 1.2181 | time: 14778.06ms | tok/sec: 35477.46\r\n",
      "step:   204 | loss: 6.297376 | lr: 1.7203e-04 | norm: 1.0368 | time: 14764.94ms | tok/sec: 35508.98\r\n",
      "step:   205 | loss: 6.358269 | lr: 1.7287e-04 | norm: 1.1180 | time: 14754.43ms | tok/sec: 35534.28\r\n",
      "step:   206 | loss: 6.342318 | lr: 1.7371e-04 | norm: 0.7649 | time: 14741.58ms | tok/sec: 35565.26\r\n",
      "step:   207 | loss: 6.308594 | lr: 1.7455e-04 | norm: 0.8530 | time: 14806.96ms | tok/sec: 35408.21\r\n",
      "step:   208 | loss: 6.241212 | lr: 1.7538e-04 | norm: 0.7289 | time: 14788.21ms | tok/sec: 35453.11\r\n",
      "step:   209 | loss: 6.203013 | lr: 1.7622e-04 | norm: 1.0861 | time: 14801.92ms | tok/sec: 35420.28\r\n",
      "step:   210 | loss: 6.227142 | lr: 1.7706e-04 | norm: 0.9775 | time: 14852.73ms | tok/sec: 35299.11\r\n",
      "step:   211 | loss: 6.266840 | lr: 1.7790e-04 | norm: 1.1158 | time: 14858.47ms | tok/sec: 35285.46\r\n",
      "step:   212 | loss: 6.285524 | lr: 1.7874e-04 | norm: 0.8242 | time: 14826.77ms | tok/sec: 35360.91\r\n",
      "step:   213 | loss: 6.266730 | lr: 1.7958e-04 | norm: 0.9986 | time: 14818.45ms | tok/sec: 35380.77\r\n",
      "step:   214 | loss: 6.260281 | lr: 1.8042e-04 | norm: 0.9774 | time: 14779.29ms | tok/sec: 35474.51\r\n",
      "step:   215 | loss: 6.271234 | lr: 1.8126e-04 | norm: 0.7760 | time: 14820.07ms | tok/sec: 35376.90\r\n",
      "step:   216 | loss: 6.210320 | lr: 1.8210e-04 | norm: 0.7526 | time: 14799.24ms | tok/sec: 35426.69\r\n",
      "step:   217 | loss: 6.215556 | lr: 1.8294e-04 | norm: 0.9562 | time: 14796.25ms | tok/sec: 35433.83\r\n",
      "step:   218 | loss: 6.199250 | lr: 1.8378e-04 | norm: 1.0650 | time: 14820.66ms | tok/sec: 35375.49\r\n",
      "step:   219 | loss: 6.180565 | lr: 1.8462e-04 | norm: 0.7952 | time: 14840.45ms | tok/sec: 35328.30\r\n",
      "step:   220 | loss: 6.145347 | lr: 1.8545e-04 | norm: 0.7109 | time: 14857.63ms | tok/sec: 35287.46\r\n",
      "step:   221 | loss: 6.144562 | lr: 1.8629e-04 | norm: 0.7402 | time: 14806.02ms | tok/sec: 35410.46\r\n",
      "step:   222 | loss: 6.179752 | lr: 1.8713e-04 | norm: 0.8614 | time: 14794.77ms | tok/sec: 35437.39\r\n",
      "step:   223 | loss: 6.112716 | lr: 1.8797e-04 | norm: 1.1813 | time: 14831.78ms | tok/sec: 35348.95\r\n",
      "step:   224 | loss: 6.121406 | lr: 1.8881e-04 | norm: 1.1220 | time: 14837.35ms | tok/sec: 35335.70\r\n",
      "validation loss: 6.2524\r\n",
      "step:   225 | loss: 6.112418 | lr: 1.8965e-04 | norm: 0.9258 | time: 18400.73ms | tok/sec: 28492.78\r\n",
      "step:   226 | loss: 6.078623 | lr: 1.9049e-04 | norm: 0.7666 | time: 14833.95ms | tok/sec: 35343.80\r\n",
      "step:   227 | loss: 6.192739 | lr: 1.9133e-04 | norm: 1.0187 | time: 14842.19ms | tok/sec: 35324.17\r\n",
      "step:   228 | loss: 6.118485 | lr: 1.9217e-04 | norm: 1.1440 | time: 14858.38ms | tok/sec: 35285.68\r\n",
      "step:   229 | loss: 6.099004 | lr: 1.9301e-04 | norm: 0.9067 | time: 14858.28ms | tok/sec: 35285.91\r\n",
      "step:   230 | loss: 6.129680 | lr: 1.9385e-04 | norm: 0.9104 | time: 14868.40ms | tok/sec: 35261.90\r\n",
      "step:   231 | loss: 6.075581 | lr: 1.9469e-04 | norm: 0.7661 | time: 14832.19ms | tok/sec: 35347.99\r\n",
      "step:   232 | loss: 6.258144 | lr: 1.9552e-04 | norm: 0.8940 | time: 14856.01ms | tok/sec: 35291.30\r\n",
      "step:   233 | loss: 6.258100 | lr: 1.9636e-04 | norm: 1.0295 | time: 14844.32ms | tok/sec: 35319.11\r\n",
      "step:   234 | loss: 6.247172 | lr: 1.9720e-04 | norm: 0.9247 | time: 14828.54ms | tok/sec: 35356.67\r\n",
      "step:   235 | loss: 6.260140 | lr: 1.9804e-04 | norm: 1.2673 | time: 14849.37ms | tok/sec: 35307.08\r\n",
      "step:   236 | loss: 6.276026 | lr: 1.9888e-04 | norm: 0.9993 | time: 14846.27ms | tok/sec: 35314.46\r\n",
      "step:   237 | loss: 6.283792 | lr: 1.9972e-04 | norm: 1.5873 | time: 14822.70ms | tok/sec: 35370.60\r\n",
      "step:   238 | loss: 6.221379 | lr: 2.0056e-04 | norm: 1.0732 | time: 14853.71ms | tok/sec: 35296.78\r\n",
      "step:   239 | loss: 6.207392 | lr: 2.0140e-04 | norm: 1.0793 | time: 14854.39ms | tok/sec: 35295.17\r\n",
      "step:   240 | loss: 6.228806 | lr: 2.0224e-04 | norm: 1.0553 | time: 14895.79ms | tok/sec: 35197.06\r\n",
      "step:   241 | loss: 6.236420 | lr: 2.0308e-04 | norm: 1.1956 | time: 14858.10ms | tok/sec: 35286.33\r\n",
      "step:   242 | loss: 6.218489 | lr: 2.0392e-04 | norm: 0.8147 | time: 14864.83ms | tok/sec: 35270.36\r\n",
      "step:   243 | loss: 6.216204 | lr: 2.0476e-04 | norm: 0.7046 | time: 14884.97ms | tok/sec: 35222.63\r\n",
      "step:   244 | loss: 6.144171 | lr: 2.0559e-04 | norm: 0.8565 | time: 14876.77ms | tok/sec: 35242.06\r\n",
      "step:   245 | loss: 6.106843 | lr: 2.0643e-04 | norm: 0.8201 | time: 14877.60ms | tok/sec: 35240.10\r\n",
      "step:   246 | loss: 6.132854 | lr: 2.0727e-04 | norm: 0.6404 | time: 14902.47ms | tok/sec: 35181.28\r\n",
      "step:   247 | loss: 6.190666 | lr: 2.0811e-04 | norm: 0.7151 | time: 14901.46ms | tok/sec: 35183.67\r\n",
      "step:   248 | loss: 6.165257 | lr: 2.0895e-04 | norm: 0.6903 | time: 14907.56ms | tok/sec: 35169.26\r\n",
      "step:   249 | loss: 6.129848 | lr: 2.0979e-04 | norm: 0.6982 | time: 14871.38ms | tok/sec: 35254.83\r\n",
      "validation loss: 6.1445\r\n",
      "step:   250 | loss: 6.155723 | lr: 2.1063e-04 | norm: 0.7783 | time: 18411.17ms | tok/sec: 28476.63\r\n",
      "step:   251 | loss: 6.104743 | lr: 2.1147e-04 | norm: 1.0703 | time: 14840.81ms | tok/sec: 35327.46\r\n",
      "step:   252 | loss: 6.185016 | lr: 2.1231e-04 | norm: 1.6474 | time: 14842.84ms | tok/sec: 35322.63\r\n",
      "step:   253 | loss: 6.130242 | lr: 2.1315e-04 | norm: 0.5474 | time: 14870.92ms | tok/sec: 35255.92\r\n",
      "step:   254 | loss: 6.152289 | lr: 2.1399e-04 | norm: 0.9487 | time: 14868.80ms | tok/sec: 35260.94\r\n",
      "step:   255 | loss: 6.062513 | lr: 2.1483e-04 | norm: 1.1658 | time: 14879.43ms | tok/sec: 35235.76\r\n",
      "step:   256 | loss: 6.097939 | lr: 2.1566e-04 | norm: 0.7360 | time: 14898.49ms | tok/sec: 35190.68\r\n",
      "step:   257 | loss: 6.084764 | lr: 2.1650e-04 | norm: 1.0068 | time: 14869.75ms | tok/sec: 35258.71\r\n",
      "step:   258 | loss: 6.059202 | lr: 2.1734e-04 | norm: 1.0823 | time: 14875.40ms | tok/sec: 35245.30\r\n",
      "step:   259 | loss: 6.079679 | lr: 2.1818e-04 | norm: 0.9555 | time: 14897.25ms | tok/sec: 35193.60\r\n",
      "step:   260 | loss: 6.004777 | lr: 2.1902e-04 | norm: 1.0316 | time: 14879.44ms | tok/sec: 35235.72\r\n",
      "step:   261 | loss: 6.055381 | lr: 2.1986e-04 | norm: 0.8846 | time: 14845.32ms | tok/sec: 35316.71\r\n",
      "step:   262 | loss: 6.020236 | lr: 2.2070e-04 | norm: 0.7939 | time: 14870.88ms | tok/sec: 35256.02\r\n",
      "step:   263 | loss: 6.007952 | lr: 2.2154e-04 | norm: 0.8357 | time: 14848.07ms | tok/sec: 35310.18\r\n",
      "step:   264 | loss: 6.018699 | lr: 2.2238e-04 | norm: 0.7700 | time: 14868.37ms | tok/sec: 35261.96\r\n",
      "step:   265 | loss: 6.032841 | lr: 2.2322e-04 | norm: 0.7166 | time: 14853.46ms | tok/sec: 35297.37\r\n",
      "step:   266 | loss: 6.070931 | lr: 2.2406e-04 | norm: 0.8819 | time: 14824.84ms | tok/sec: 35365.51\r\n",
      "step:   267 | loss: 5.973832 | lr: 2.2490e-04 | norm: 0.9482 | time: 14850.96ms | tok/sec: 35303.31\r\n",
      "step:   268 | loss: 5.981979 | lr: 2.2573e-04 | norm: 0.8185 | time: 14840.19ms | tok/sec: 35328.93\r\n",
      "step:   269 | loss: 6.021669 | lr: 2.2657e-04 | norm: 0.9368 | time: 14866.37ms | tok/sec: 35266.70\r\n",
      "step:   270 | loss: 6.020608 | lr: 2.2741e-04 | norm: 1.2205 | time: 14843.86ms | tok/sec: 35320.19\r\n",
      "step:   271 | loss: 5.971270 | lr: 2.2825e-04 | norm: 1.4588 | time: 14832.10ms | tok/sec: 35348.19\r\n",
      "step:   272 | loss: 5.956232 | lr: 2.2909e-04 | norm: 0.7825 | time: 14848.42ms | tok/sec: 35309.35\r\n",
      "step:   273 | loss: 5.947302 | lr: 2.2993e-04 | norm: 0.7485 | time: 14857.46ms | tok/sec: 35287.87\r\n",
      "step:   274 | loss: 5.921646 | lr: 2.3077e-04 | norm: 0.7527 | time: 14861.72ms | tok/sec: 35277.74\r\n",
      "validation loss: 6.0641\r\n",
      "step:   275 | loss: 5.991587 | lr: 2.3161e-04 | norm: 0.7705 | time: 18416.91ms | tok/sec: 28467.76\r\n",
      "step:   276 | loss: 6.017283 | lr: 2.3245e-04 | norm: 0.6603 | time: 14831.49ms | tok/sec: 35349.64\r\n",
      "step:   277 | loss: 5.930738 | lr: 2.3329e-04 | norm: 0.7355 | time: 14826.31ms | tok/sec: 35362.00\r\n",
      "step:   278 | loss: 6.088025 | lr: 2.3413e-04 | norm: 0.7829 | time: 14833.83ms | tok/sec: 35344.07\r\n",
      "step:   279 | loss: 6.045879 | lr: 2.3497e-04 | norm: 0.9056 | time: 14873.93ms | tok/sec: 35248.79\r\n",
      "step:   280 | loss: 6.098006 | lr: 2.3580e-04 | norm: 1.3076 | time: 14835.11ms | tok/sec: 35341.02\r\n",
      "step:   281 | loss: 6.069864 | lr: 2.3664e-04 | norm: 1.1425 | time: 14864.58ms | tok/sec: 35270.96\r\n",
      "step:   282 | loss: 6.065293 | lr: 2.3748e-04 | norm: 1.3520 | time: 14857.09ms | tok/sec: 35288.73\r\n",
      "step:   283 | loss: 6.068041 | lr: 2.3832e-04 | norm: 0.9846 | time: 14857.89ms | tok/sec: 35286.84\r\n",
      "step:   284 | loss: 6.124226 | lr: 2.3916e-04 | norm: 1.3732 | time: 14848.23ms | tok/sec: 35309.80\r\n",
      "step:   285 | loss: 6.043617 | lr: 2.4000e-04 | norm: 0.9754 | time: 14833.34ms | tok/sec: 35345.24\r\n",
      "step:   286 | loss: 6.068676 | lr: 2.4084e-04 | norm: 0.9576 | time: 14840.08ms | tok/sec: 35329.18\r\n",
      "step:   287 | loss: 6.035678 | lr: 2.4168e-04 | norm: 0.7556 | time: 14865.11ms | tok/sec: 35269.71\r\n",
      "step:   288 | loss: 6.020618 | lr: 2.4252e-04 | norm: 1.0762 | time: 14801.18ms | tok/sec: 35422.03\r\n",
      "step:   289 | loss: 6.025035 | lr: 2.4336e-04 | norm: 0.7833 | time: 14796.54ms | tok/sec: 35433.15\r\n",
      "step:   290 | loss: 5.962501 | lr: 2.4420e-04 | norm: 0.7233 | time: 14836.59ms | tok/sec: 35337.51\r\n",
      "step:   291 | loss: 5.995883 | lr: 2.4503e-04 | norm: 0.9503 | time: 14881.11ms | tok/sec: 35231.77\r\n",
      "step:   292 | loss: 5.928214 | lr: 2.4587e-04 | norm: 1.0582 | time: 14842.44ms | tok/sec: 35323.58\r\n",
      "step:   293 | loss: 5.988434 | lr: 2.4671e-04 | norm: 1.1022 | time: 14870.09ms | tok/sec: 35257.88\r\n",
      "step:   294 | loss: 6.047527 | lr: 2.4755e-04 | norm: 0.8906 | time: 14850.29ms | tok/sec: 35304.91\r\n",
      "step:   295 | loss: 5.988472 | lr: 2.4839e-04 | norm: 0.8024 | time: 14902.84ms | tok/sec: 35180.40\r\n",
      "step:   296 | loss: 5.989484 | lr: 2.4923e-04 | norm: 0.8982 | time: 14860.46ms | tok/sec: 35280.74\r\n",
      "step:   297 | loss: 5.996671 | lr: 2.5007e-04 | norm: 1.0698 | time: 14849.11ms | tok/sec: 35307.70\r\n",
      "step:   298 | loss: 5.963829 | lr: 2.5091e-04 | norm: 0.9809 | time: 14895.08ms | tok/sec: 35198.73\r\n",
      "step:   299 | loss: 5.967655 | lr: 2.5175e-04 | norm: 0.9242 | time: 14879.85ms | tok/sec: 35234.76\r\n",
      "validation loss: 5.9762\r\n",
      "step:   300 | loss: 6.059898 | lr: 2.5259e-04 | norm: 0.8974 | time: 18415.16ms | tok/sec: 28470.45\r\n",
      "step:   301 | loss: 5.909271 | lr: 2.5343e-04 | norm: 0.8584 | time: 14884.24ms | tok/sec: 35224.38\r\n",
      "step:   302 | loss: 5.901811 | lr: 2.5427e-04 | norm: 0.7970 | time: 14885.05ms | tok/sec: 35222.45\r\n",
      "step:   303 | loss: 5.923997 | lr: 2.5510e-04 | norm: 0.7314 | time: 14903.22ms | tok/sec: 35179.51\r\n",
      "step:   304 | loss: 5.887560 | lr: 2.5594e-04 | norm: 0.8451 | time: 14903.36ms | tok/sec: 35179.18\r\n",
      "step:   305 | loss: 5.884224 | lr: 2.5678e-04 | norm: 0.9115 | time: 14842.79ms | tok/sec: 35322.75\r\n",
      "step:   306 | loss: 5.891723 | lr: 2.5762e-04 | norm: 1.0220 | time: 14845.82ms | tok/sec: 35315.53\r\n",
      "step:   307 | loss: 5.868920 | lr: 2.5846e-04 | norm: 0.7682 | time: 14849.55ms | tok/sec: 35306.65\r\n",
      "step:   308 | loss: 5.882993 | lr: 2.5930e-04 | norm: 0.6862 | time: 14799.77ms | tok/sec: 35425.42\r\n",
      "step:   309 | loss: 5.850325 | lr: 2.6014e-04 | norm: 0.6643 | time: 14839.92ms | tok/sec: 35329.58\r\n",
      "step:   310 | loss: 5.853864 | lr: 2.6098e-04 | norm: 0.7645 | time: 14805.41ms | tok/sec: 35411.93\r\n",
      "step:   311 | loss: 5.840396 | lr: 2.6182e-04 | norm: 0.7491 | time: 14850.15ms | tok/sec: 35305.24\r\n",
      "step:   312 | loss: 5.852149 | lr: 2.6266e-04 | norm: 0.6962 | time: 14808.31ms | tok/sec: 35405.00\r\n",
      "step:   313 | loss: 5.819872 | lr: 2.6350e-04 | norm: 0.6482 | time: 14800.05ms | tok/sec: 35424.74\r\n",
      "step:   314 | loss: 5.808896 | lr: 2.6434e-04 | norm: 0.7000 | time: 14821.55ms | tok/sec: 35373.36\r\n",
      "step:   315 | loss: 5.778819 | lr: 2.6517e-04 | norm: 0.7896 | time: 14806.56ms | tok/sec: 35409.17\r\n",
      "step:   316 | loss: 5.837732 | lr: 2.6601e-04 | norm: 0.8033 | time: 14807.62ms | tok/sec: 35406.62\r\n",
      "step:   317 | loss: 5.807896 | lr: 2.6685e-04 | norm: 0.9457 | time: 14846.71ms | tok/sec: 35313.42\r\n",
      "step:   318 | loss: 5.837942 | lr: 2.6769e-04 | norm: 1.0417 | time: 14823.08ms | tok/sec: 35369.70\r\n",
      "step:   319 | loss: 5.802693 | lr: 2.6853e-04 | norm: 0.9246 | time: 14859.75ms | tok/sec: 35282.42\r\n",
      "step:   320 | loss: 5.753695 | lr: 2.6937e-04 | norm: 0.9991 | time: 14843.80ms | tok/sec: 35320.33\r\n",
      "step:   321 | loss: 5.768747 | lr: 2.7021e-04 | norm: 0.7507 | time: 14816.03ms | tok/sec: 35386.55\r\n",
      "step:   322 | loss: 5.756010 | lr: 2.7105e-04 | norm: 0.9163 | time: 14799.93ms | tok/sec: 35425.04\r\n",
      "step:   323 | loss: 5.758698 | lr: 2.7189e-04 | norm: 0.9772 | time: 14774.03ms | tok/sec: 35487.14\r\n",
      "step:   324 | loss: 5.893078 | lr: 2.7273e-04 | norm: 1.0304 | time: 14840.00ms | tok/sec: 35329.39\r\n",
      "validation loss: 5.8964\r\n",
      "step:   325 | loss: 5.941169 | lr: 2.7357e-04 | norm: 1.0369 | time: 18328.27ms | tok/sec: 28605.42\r\n",
      "step:   326 | loss: 5.921373 | lr: 2.7441e-04 | norm: 1.0414 | time: 14801.80ms | tok/sec: 35420.56\r\n",
      "step:   327 | loss: 5.955693 | lr: 2.7524e-04 | norm: 1.3655 | time: 14818.98ms | tok/sec: 35379.48\r\n",
      "step:   328 | loss: 5.939952 | lr: 2.7608e-04 | norm: 1.1361 | time: 14880.00ms | tok/sec: 35234.40\r\n",
      "step:   329 | loss: 5.936954 | lr: 2.7692e-04 | norm: 1.6939 | time: 14848.45ms | tok/sec: 35309.28\r\n",
      "step:   330 | loss: 5.954053 | lr: 2.7776e-04 | norm: 0.9937 | time: 14817.29ms | tok/sec: 35383.53\r\n",
      "step:   331 | loss: 5.903392 | lr: 2.7860e-04 | norm: 1.0416 | time: 14894.95ms | tok/sec: 35199.05\r\n",
      "step:   332 | loss: 5.957129 | lr: 2.7944e-04 | norm: 1.4722 | time: 14907.00ms | tok/sec: 35170.59\r\n",
      "step:   333 | loss: 5.908837 | lr: 2.8028e-04 | norm: 1.0940 | time: 14841.89ms | tok/sec: 35324.87\r\n",
      "step:   334 | loss: 5.925208 | lr: 2.8112e-04 | norm: 0.8635 | time: 14854.18ms | tok/sec: 35295.66\r\n",
      "step:   335 | loss: 5.924551 | lr: 2.8196e-04 | norm: 0.9267 | time: 14845.34ms | tok/sec: 35316.66\r\n",
      "step:   336 | loss: 5.835503 | lr: 2.8280e-04 | norm: 1.0394 | time: 14857.27ms | tok/sec: 35288.32\r\n",
      "step:   337 | loss: 5.867406 | lr: 2.8364e-04 | norm: 1.0389 | time: 14839.05ms | tok/sec: 35331.64\r\n",
      "step:   338 | loss: 5.890719 | lr: 2.8448e-04 | norm: 0.9738 | time: 14869.79ms | tok/sec: 35258.59\r\n",
      "step:   339 | loss: 5.863396 | lr: 2.8531e-04 | norm: 0.9060 | time: 14831.38ms | tok/sec: 35349.91\r\n",
      "step:   340 | loss: 5.854321 | lr: 2.8615e-04 | norm: 0.8067 | time: 14855.74ms | tok/sec: 35291.95\r\n",
      "step:   341 | loss: 5.840066 | lr: 2.8699e-04 | norm: 0.7397 | time: 14846.91ms | tok/sec: 35312.94\r\n",
      "step:   342 | loss: 5.874423 | lr: 2.8783e-04 | norm: 0.9097 | time: 14840.84ms | tok/sec: 35327.39\r\n",
      "step:   343 | loss: 5.794362 | lr: 2.8867e-04 | norm: 1.1170 | time: 14831.23ms | tok/sec: 35350.26\r\n",
      "step:   344 | loss: 5.815326 | lr: 2.8951e-04 | norm: 0.8102 | time: 14809.99ms | tok/sec: 35400.96\r\n",
      "step:   345 | loss: 5.770345 | lr: 2.9035e-04 | norm: 0.6908 | time: 14842.17ms | tok/sec: 35324.21\r\n",
      "step:   346 | loss: 5.762762 | lr: 2.9119e-04 | norm: 0.7326 | time: 14820.86ms | tok/sec: 35375.00\r\n",
      "step:   347 | loss: 5.788791 | lr: 2.9203e-04 | norm: 0.7787 | time: 14810.49ms | tok/sec: 35399.76\r\n",
      "step:   348 | loss: 5.724985 | lr: 2.9287e-04 | norm: 0.9992 | time: 14827.74ms | tok/sec: 35358.59\r\n",
      "step:   349 | loss: 5.763412 | lr: 2.9371e-04 | norm: 1.2173 | time: 14841.87ms | tok/sec: 35324.92\r\n",
      "validation loss: 5.7952\r\n",
      "step:   350 | loss: 5.739787 | lr: 2.9455e-04 | norm: 1.0222 | time: 18395.25ms | tok/sec: 28501.27\r\n",
      "step:   351 | loss: 5.769859 | lr: 2.9538e-04 | norm: 1.1146 | time: 14820.33ms | tok/sec: 35376.27\r\n",
      "step:   352 | loss: 5.710399 | lr: 2.9622e-04 | norm: 0.8610 | time: 14850.02ms | tok/sec: 35305.54\r\n",
      "step:   353 | loss: 5.711835 | lr: 2.9706e-04 | norm: 1.0713 | time: 14829.96ms | tok/sec: 35353.30\r\n",
      "step:   354 | loss: 5.779963 | lr: 2.9790e-04 | norm: 1.7672 | time: 14860.50ms | tok/sec: 35280.65\r\n",
      "step:   355 | loss: 5.741656 | lr: 2.9874e-04 | norm: 0.9504 | time: 14858.02ms | tok/sec: 35286.53\r\n",
      "step:   356 | loss: 5.696080 | lr: 2.9958e-04 | norm: 1.1290 | time: 14849.65ms | tok/sec: 35306.41\r\n",
      "step:   357 | loss: 5.707987 | lr: 3.0042e-04 | norm: 1.1844 | time: 14857.14ms | tok/sec: 35288.61\r\n",
      "step:   358 | loss: 5.695947 | lr: 3.0126e-04 | norm: 0.9199 | time: 14877.59ms | tok/sec: 35240.12\r\n",
      "step:   359 | loss: 5.634274 | lr: 3.0210e-04 | norm: 0.7090 | time: 14882.52ms | tok/sec: 35228.45\r\n",
      "step:   360 | loss: 5.655785 | lr: 3.0294e-04 | norm: 0.8245 | time: 14855.38ms | tok/sec: 35292.79\r\n",
      "step:   361 | loss: 5.678250 | lr: 3.0378e-04 | norm: 1.1059 | time: 14875.50ms | tok/sec: 35245.07\r\n",
      "step:   362 | loss: 5.708962 | lr: 3.0462e-04 | norm: 1.0473 | time: 14863.85ms | tok/sec: 35272.69\r\n",
      "step:   363 | loss: 5.626589 | lr: 3.0545e-04 | norm: 0.9547 | time: 14886.55ms | tok/sec: 35218.90\r\n",
      "step:   364 | loss: 5.669359 | lr: 3.0629e-04 | norm: 1.1258 | time: 14871.37ms | tok/sec: 35254.85\r\n",
      "step:   365 | loss: 5.602551 | lr: 3.0713e-04 | norm: 0.7751 | time: 14869.40ms | tok/sec: 35259.52\r\n",
      "step:   366 | loss: 5.586974 | lr: 3.0797e-04 | norm: 0.8834 | time: 14842.49ms | tok/sec: 35323.46\r\n",
      "step:   367 | loss: 5.631517 | lr: 3.0881e-04 | norm: 1.0050 | time: 14856.57ms | tok/sec: 35289.97\r\n",
      "step:   368 | loss: 5.653497 | lr: 3.0965e-04 | norm: 0.8647 | time: 14874.24ms | tok/sec: 35248.04\r\n",
      "step:   369 | loss: 5.581397 | lr: 3.1049e-04 | norm: 0.7831 | time: 14847.13ms | tok/sec: 35312.43\r\n",
      "step:   370 | loss: 5.622196 | lr: 3.1133e-04 | norm: 0.7813 | time: 14871.43ms | tok/sec: 35254.71\r\n",
      "step:   371 | loss: 5.718655 | lr: 3.1217e-04 | norm: 0.7967 | time: 14854.40ms | tok/sec: 35295.14\r\n",
      "step:   372 | loss: 5.806201 | lr: 3.1301e-04 | norm: 1.1532 | time: 14809.37ms | tok/sec: 35402.45\r\n",
      "step:   373 | loss: 5.724863 | lr: 3.1385e-04 | norm: 1.3272 | time: 14778.60ms | tok/sec: 35476.17\r\n",
      "step:   374 | loss: 5.729146 | lr: 3.1469e-04 | norm: 1.0007 | time: 14768.75ms | tok/sec: 35499.82\r\n",
      "validation loss: 5.7225\r\n",
      "step:   375 | loss: 5.763180 | lr: 3.1552e-04 | norm: 1.3299 | time: 18265.23ms | tok/sec: 28704.15\r\n",
      "step:   376 | loss: 5.781067 | lr: 3.1636e-04 | norm: 1.2591 | time: 14775.37ms | tok/sec: 35483.93\r\n",
      "step:   377 | loss: 5.784610 | lr: 3.1720e-04 | norm: 1.2476 | time: 14779.94ms | tok/sec: 35472.95\r\n",
      "step:   378 | loss: 5.788593 | lr: 3.1804e-04 | norm: 1.1019 | time: 14816.37ms | tok/sec: 35385.73\r\n",
      "step:   379 | loss: 5.704604 | lr: 3.1888e-04 | norm: 1.3010 | time: 14829.93ms | tok/sec: 35353.38\r\n",
      "step:   380 | loss: 5.720204 | lr: 3.1972e-04 | norm: 0.7699 | time: 14789.73ms | tok/sec: 35449.46\r\n",
      "step:   381 | loss: 5.731280 | lr: 3.2056e-04 | norm: 1.0340 | time: 17001.80ms | tok/sec: 30837.21\r\n",
      "step:   382 | loss: 5.672390 | lr: 3.2140e-04 | norm: 1.1636 | time: 14821.37ms | tok/sec: 35373.79\r\n",
      "step:   383 | loss: 5.687902 | lr: 3.2224e-04 | norm: 1.0948 | time: 14845.61ms | tok/sec: 35316.03\r\n",
      "step:   384 | loss: 5.658621 | lr: 3.2308e-04 | norm: 1.2064 | time: 14853.47ms | tok/sec: 35297.34\r\n",
      "step:   385 | loss: 5.742053 | lr: 3.2392e-04 | norm: 1.1086 | time: 14779.46ms | tok/sec: 35474.11\r\n",
      "step:   386 | loss: 5.750971 | lr: 3.2476e-04 | norm: 1.0526 | time: 14802.20ms | tok/sec: 35419.61\r\n",
      "step:   387 | loss: 5.703115 | lr: 3.2559e-04 | norm: 0.9189 | time: 14802.25ms | tok/sec: 35419.48\r\n",
      "step:   388 | loss: 5.625088 | lr: 3.2643e-04 | norm: 1.0131 | time: 14837.43ms | tok/sec: 35335.49\r\n",
      "step:   389 | loss: 5.653088 | lr: 3.2727e-04 | norm: 0.8490 | time: 14804.29ms | tok/sec: 35414.60\r\n",
      "step:   390 | loss: 5.653683 | lr: 3.2811e-04 | norm: 0.8729 | time: 14856.43ms | tok/sec: 35290.30\r\n",
      "step:   391 | loss: 5.651222 | lr: 3.2895e-04 | norm: 0.8287 | time: 14842.38ms | tok/sec: 35323.71\r\n",
      "step:   392 | loss: 5.601226 | lr: 3.2979e-04 | norm: 0.7627 | time: 14824.11ms | tok/sec: 35367.25\r\n",
      "step:   393 | loss: 5.627001 | lr: 3.3063e-04 | norm: 0.7906 | time: 14837.18ms | tok/sec: 35336.09\r\n",
      "step:   394 | loss: 5.545744 | lr: 3.3147e-04 | norm: 0.9396 | time: 14859.29ms | tok/sec: 35283.51\r\n",
      "step:   395 | loss: 5.538190 | lr: 3.3231e-04 | norm: 0.9759 | time: 14856.90ms | tok/sec: 35289.18\r\n",
      "step:   396 | loss: 5.559537 | lr: 3.3315e-04 | norm: 1.0827 | time: 14864.04ms | tok/sec: 35272.25\r\n",
      "step:   397 | loss: 5.570696 | lr: 3.3399e-04 | norm: 1.0273 | time: 14857.36ms | tok/sec: 35288.09\r\n",
      "step:   398 | loss: 5.555644 | lr: 3.3483e-04 | norm: 1.0661 | time: 14825.72ms | tok/sec: 35363.40\r\n",
      "step:   399 | loss: 5.580720 | lr: 3.3566e-04 | norm: 1.0685 | time: 14862.31ms | tok/sec: 35276.34\r\n",
      "validation loss: 5.6190\r\n",
      "step:   400 | loss: 5.560190 | lr: 3.3650e-04 | norm: 1.0884 | time: 18351.84ms | tok/sec: 28568.69\r\n",
      "step:   401 | loss: 5.568179 | lr: 3.3734e-04 | norm: 1.0317 | time: 14809.91ms | tok/sec: 35401.17\r\n",
      "step:   402 | loss: 5.541368 | lr: 3.3818e-04 | norm: 0.9377 | time: 14829.84ms | tok/sec: 35353.58\r\n",
      "step:   403 | loss: 5.555264 | lr: 3.3902e-04 | norm: 0.9585 | time: 14839.94ms | tok/sec: 35329.52\r\n",
      "step:   404 | loss: 5.534007 | lr: 3.3986e-04 | norm: 0.9346 | time: 14862.10ms | tok/sec: 35276.83\r\n",
      "step:   405 | loss: 5.508516 | lr: 3.4070e-04 | norm: 0.8237 | time: 14857.79ms | tok/sec: 35287.09\r\n",
      "step:   406 | loss: 5.457026 | lr: 3.4154e-04 | norm: 0.7412 | time: 14834.45ms | tok/sec: 35342.60\r\n",
      "step:   407 | loss: 5.428594 | lr: 3.4238e-04 | norm: 0.9121 | time: 14824.58ms | tok/sec: 35366.14\r\n",
      "step:   408 | loss: 5.462073 | lr: 3.4322e-04 | norm: 0.9904 | time: 14841.55ms | tok/sec: 35325.69\r\n",
      "step:   409 | loss: 5.473800 | lr: 3.4406e-04 | norm: 0.9917 | time: 14822.72ms | tok/sec: 35370.57\r\n",
      "step:   410 | loss: 5.463493 | lr: 3.4490e-04 | norm: 0.8717 | time: 14834.69ms | tok/sec: 35342.01\r\n",
      "step:   411 | loss: 5.476818 | lr: 3.4573e-04 | norm: 1.0089 | time: 14843.90ms | tok/sec: 35320.10\r\n",
      "step:   412 | loss: 5.488038 | lr: 3.4657e-04 | norm: 1.4127 | time: 14841.23ms | tok/sec: 35326.46\r\n",
      "step:   413 | loss: 5.449357 | lr: 3.4741e-04 | norm: 0.8039 | time: 14879.50ms | tok/sec: 35235.58\r\n",
      "step:   414 | loss: 5.462118 | lr: 3.4825e-04 | norm: 0.7722 | time: 14804.21ms | tok/sec: 35414.78\r\n",
      "step:   415 | loss: 5.448617 | lr: 3.4909e-04 | norm: 0.8909 | time: 14792.14ms | tok/sec: 35443.69\r\n",
      "step:   416 | loss: 5.502616 | lr: 3.4993e-04 | norm: 0.7841 | time: 14869.59ms | tok/sec: 35259.07\r\n",
      "step:   417 | loss: 5.630196 | lr: 3.5077e-04 | norm: 1.0624 | time: 14876.31ms | tok/sec: 35243.15\r\n",
      "step:   418 | loss: 5.633711 | lr: 3.5161e-04 | norm: 1.3294 | time: 14902.56ms | tok/sec: 35181.08\r\n",
      "step:   419 | loss: 5.621228 | lr: 3.5245e-04 | norm: 0.9347 | time: 14868.38ms | tok/sec: 35261.94\r\n",
      "step:   420 | loss: 5.586061 | lr: 3.5329e-04 | norm: 1.2012 | time: 14852.29ms | tok/sec: 35300.15\r\n",
      "step:   421 | loss: 5.610367 | lr: 3.5413e-04 | norm: 1.0605 | time: 14916.38ms | tok/sec: 35148.47\r\n",
      "step:   422 | loss: 5.583694 | lr: 3.5497e-04 | norm: 0.8903 | time: 14865.80ms | tok/sec: 35268.07\r\n",
      "step:   423 | loss: 5.633299 | lr: 3.5580e-04 | norm: 0.8475 | time: 14875.72ms | tok/sec: 35244.53\r\n",
      "step:   424 | loss: 5.577967 | lr: 3.5664e-04 | norm: 0.9326 | time: 14874.61ms | tok/sec: 35247.18\r\n",
      "validation loss: 5.5286\r\n",
      "step:   425 | loss: 5.570976 | lr: 3.5748e-04 | norm: 0.9902 | time: 18440.16ms | tok/sec: 28431.86\r\n",
      "step:   426 | loss: 5.547729 | lr: 3.5832e-04 | norm: 1.2010 | time: 14799.76ms | tok/sec: 35425.45\r\n",
      "step:   427 | loss: 5.540708 | lr: 3.5916e-04 | norm: 1.0328 | time: 14820.72ms | tok/sec: 35375.35\r\n",
      "step:   428 | loss: 5.509154 | lr: 3.6000e-04 | norm: 0.9367 | time: 14824.92ms | tok/sec: 35365.31\r\n",
      "step:   429 | loss: 5.604790 | lr: 3.6084e-04 | norm: 0.9048 | time: 14792.23ms | tok/sec: 35443.47\r\n",
      "step:   430 | loss: 5.565383 | lr: 3.6168e-04 | norm: 0.9455 | time: 14817.58ms | tok/sec: 35382.83\r\n",
      "step:   431 | loss: 5.507877 | lr: 3.6252e-04 | norm: 1.1419 | time: 14795.79ms | tok/sec: 35434.94\r\n",
      "step:   432 | loss: 5.590622 | lr: 3.6336e-04 | norm: 1.0158 | time: 14826.94ms | tok/sec: 35360.51\r\n",
      "step:   433 | loss: 5.605428 | lr: 3.6420e-04 | norm: 1.2715 | time: 14805.77ms | tok/sec: 35411.05\r\n",
      "step:   434 | loss: 5.481082 | lr: 3.6503e-04 | norm: 0.9243 | time: 14812.59ms | tok/sec: 35394.74\r\n",
      "step:   435 | loss: 5.501108 | lr: 3.6587e-04 | norm: 0.8060 | time: 14784.77ms | tok/sec: 35461.37\r\n",
      "step:   436 | loss: 5.449666 | lr: 3.6671e-04 | norm: 0.9215 | time: 14756.24ms | tok/sec: 35529.92\r\n",
      "step:   437 | loss: 5.575891 | lr: 3.6755e-04 | norm: 1.0242 | time: 14745.62ms | tok/sec: 35555.51\r\n",
      "step:   438 | loss: 5.560406 | lr: 3.6839e-04 | norm: 0.9822 | time: 14768.56ms | tok/sec: 35500.27\r\n",
      "step:   439 | loss: 5.530575 | lr: 3.6923e-04 | norm: 0.9103 | time: 14779.46ms | tok/sec: 35474.11\r\n",
      "step:   440 | loss: 5.373765 | lr: 3.7007e-04 | norm: 0.9443 | time: 14761.86ms | tok/sec: 35516.39\r\n",
      "step:   441 | loss: 5.424994 | lr: 3.7091e-04 | norm: 0.9345 | time: 14781.23ms | tok/sec: 35469.85\r\n",
      "step:   442 | loss: 5.423960 | lr: 3.7175e-04 | norm: 1.1823 | time: 14765.65ms | tok/sec: 35507.28\r\n",
      "step:   443 | loss: 5.396502 | lr: 3.7259e-04 | norm: 0.9016 | time: 14786.40ms | tok/sec: 35457.44\r\n",
      "step:   444 | loss: 5.432550 | lr: 3.7343e-04 | norm: 0.9968 | time: 14776.80ms | tok/sec: 35480.48\r\n",
      "step:   445 | loss: 5.426383 | lr: 3.7427e-04 | norm: 1.0827 | time: 14799.37ms | tok/sec: 35426.37\r\n",
      "step:   446 | loss: 5.421435 | lr: 3.7510e-04 | norm: 1.2099 | time: 14760.34ms | tok/sec: 35520.06\r\n",
      "step:   447 | loss: 5.440548 | lr: 3.7594e-04 | norm: 0.8977 | time: 14759.56ms | tok/sec: 35521.92\r\n",
      "step:   448 | loss: 5.456161 | lr: 3.7678e-04 | norm: 1.0443 | time: 14765.82ms | tok/sec: 35506.88\r\n",
      "step:   449 | loss: 5.411669 | lr: 3.7762e-04 | norm: 1.1652 | time: 14798.59ms | tok/sec: 35428.25\r\n",
      "validation loss: 5.4400\r\n",
      "step:   450 | loss: 5.401525 | lr: 3.7846e-04 | norm: 0.7948 | time: 18290.93ms | tok/sec: 28663.83\r\n",
      "step:   451 | loss: 5.409601 | lr: 3.7930e-04 | norm: 0.7764 | time: 14750.37ms | tok/sec: 35544.05\r\n",
      "step:   452 | loss: 5.310172 | lr: 3.8014e-04 | norm: 0.8841 | time: 14761.30ms | tok/sec: 35517.74\r\n",
      "step:   453 | loss: 5.339931 | lr: 3.8098e-04 | norm: 0.9982 | time: 14803.97ms | tok/sec: 35415.37\r\n",
      "step:   454 | loss: 5.364214 | lr: 3.8182e-04 | norm: 1.2044 | time: 14756.94ms | tok/sec: 35528.23\r\n",
      "step:   455 | loss: 5.276266 | lr: 3.8266e-04 | norm: 0.9998 | time: 14760.96ms | tok/sec: 35518.55\r\n",
      "step:   456 | loss: 5.350266 | lr: 3.8350e-04 | norm: 0.9386 | time: 14791.84ms | tok/sec: 35444.40\r\n",
      "step:   457 | loss: 5.324297 | lr: 3.8434e-04 | norm: 1.1151 | time: 14775.92ms | tok/sec: 35482.61\r\n",
      "step:   458 | loss: 5.336463 | lr: 3.8517e-04 | norm: 1.0425 | time: 14822.14ms | tok/sec: 35371.95\r\n",
      "step:   459 | loss: 5.350069 | lr: 3.8601e-04 | norm: 0.8656 | time: 14797.56ms | tok/sec: 35430.70\r\n",
      "step:   460 | loss: 5.302461 | lr: 3.8685e-04 | norm: 1.0961 | time: 14791.66ms | tok/sec: 35444.84\r\n",
      "step:   461 | loss: 5.302043 | lr: 3.8769e-04 | norm: 1.0784 | time: 14812.87ms | tok/sec: 35394.09\r\n",
      "step:   462 | loss: 5.225952 | lr: 3.8853e-04 | norm: 0.9100 | time: 14828.75ms | tok/sec: 35356.18\r\n",
      "step:   463 | loss: 5.344178 | lr: 3.8937e-04 | norm: 0.8600 | time: 14826.89ms | tok/sec: 35360.62\r\n",
      "step:   464 | loss: 5.368979 | lr: 3.9021e-04 | norm: 0.7690 | time: 14784.09ms | tok/sec: 35462.99\r\n",
      "step:   465 | loss: 5.413925 | lr: 3.9105e-04 | norm: 0.8910 | time: 14794.98ms | tok/sec: 35436.89\r\n",
      "step:   466 | loss: 5.479095 | lr: 3.9189e-04 | norm: 1.0943 | time: 14799.41ms | tok/sec: 35426.27\r\n",
      "step:   467 | loss: 5.409643 | lr: 3.9273e-04 | norm: 1.1179 | time: 14801.08ms | tok/sec: 35422.29\r\n",
      "step:   468 | loss: 5.432890 | lr: 3.9357e-04 | norm: 1.0860 | time: 14844.52ms | tok/sec: 35318.63\r\n",
      "step:   469 | loss: 5.416272 | lr: 3.9441e-04 | norm: 0.8579 | time: 14809.56ms | tok/sec: 35402.00\r\n",
      "step:   470 | loss: 5.395312 | lr: 3.9524e-04 | norm: 0.8537 | time: 14788.38ms | tok/sec: 35452.71\r\n",
      "step:   471 | loss: 5.337423 | lr: 3.9608e-04 | norm: 0.8662 | time: 14796.87ms | tok/sec: 35432.36\r\n",
      "step:   472 | loss: 5.426158 | lr: 3.9692e-04 | norm: 0.9371 | time: 14818.36ms | tok/sec: 35380.98\r\n",
      "step:   473 | loss: 5.405675 | lr: 3.9776e-04 | norm: 0.8751 | time: 14842.58ms | tok/sec: 35323.25\r\n",
      "step:   474 | loss: 5.431352 | lr: 3.9860e-04 | norm: 1.0226 | time: 14819.71ms | tok/sec: 35377.75\r\n",
      "validation loss: 5.3559\r\n",
      "step:   475 | loss: 5.412132 | lr: 3.9944e-04 | norm: 1.0306 | time: 18529.83ms | tok/sec: 28294.26\r\n",
      "step:   476 | loss: 5.310052 | lr: 4.0028e-04 | norm: 1.0143 | time: 14859.77ms | tok/sec: 35282.39\r\n",
      "step:   477 | loss: 5.414615 | lr: 4.0112e-04 | norm: 1.0297 | time: 14821.72ms | tok/sec: 35372.96\r\n",
      "step:   478 | loss: 5.365682 | lr: 4.0196e-04 | norm: 1.0562 | time: 14856.03ms | tok/sec: 35291.26\r\n",
      "step:   479 | loss: 5.418473 | lr: 4.0280e-04 | norm: 1.0373 | time: 14865.05ms | tok/sec: 35269.84\r\n",
      "step:   480 | loss: 5.363940 | lr: 4.0364e-04 | norm: 1.0410 | time: 14857.72ms | tok/sec: 35287.25\r\n",
      "step:   481 | loss: 5.389296 | lr: 4.0448e-04 | norm: 0.8945 | time: 14893.14ms | tok/sec: 35203.33\r\n",
      "step:   482 | loss: 5.335792 | lr: 4.0531e-04 | norm: 0.7253 | time: 14887.22ms | tok/sec: 35217.31\r\n",
      "step:   483 | loss: 5.379409 | lr: 4.0615e-04 | norm: 0.8060 | time: 14837.27ms | tok/sec: 35335.88\r\n",
      "step:   484 | loss: 5.317624 | lr: 4.0699e-04 | norm: 0.8778 | time: 14922.30ms | tok/sec: 35134.53\r\n",
      "step:   485 | loss: 5.358958 | lr: 4.0783e-04 | norm: 0.9551 | time: 14840.17ms | tok/sec: 35328.99\r\n",
      "step:   486 | loss: 5.324755 | lr: 4.0867e-04 | norm: 0.9018 | time: 14821.20ms | tok/sec: 35374.20\r\n",
      "step:   487 | loss: 5.313957 | lr: 4.0951e-04 | norm: 0.7665 | time: 14767.51ms | tok/sec: 35502.81\r\n",
      "step:   488 | loss: 5.264935 | lr: 4.1035e-04 | norm: 0.7179 | time: 14755.88ms | tok/sec: 35530.80\r\n",
      "step:   489 | loss: 5.219352 | lr: 4.1119e-04 | norm: 0.8366 | time: 14761.63ms | tok/sec: 35516.94\r\n",
      "step:   490 | loss: 5.300903 | lr: 4.1203e-04 | norm: 1.2739 | time: 14771.78ms | tok/sec: 35492.55\r\n",
      "step:   491 | loss: 5.246506 | lr: 4.1287e-04 | norm: 0.9926 | time: 14761.71ms | tok/sec: 35516.75\r\n",
      "step:   492 | loss: 5.275514 | lr: 4.1371e-04 | norm: 1.1116 | time: 14737.11ms | tok/sec: 35576.04\r\n",
      "step:   493 | loss: 5.245984 | lr: 4.1455e-04 | norm: 0.9703 | time: 14799.29ms | tok/sec: 35426.55\r\n",
      "step:   494 | loss: 5.285322 | lr: 4.1538e-04 | norm: 0.9959 | time: 14817.18ms | tok/sec: 35383.79\r\n",
      "step:   495 | loss: 5.235015 | lr: 4.1622e-04 | norm: 1.0914 | time: 14761.73ms | tok/sec: 35516.71\r\n",
      "step:   496 | loss: 5.267339 | lr: 4.1706e-04 | norm: 1.1521 | time: 14769.81ms | tok/sec: 35497.28\r\n",
      "step:   497 | loss: 5.238440 | lr: 4.1790e-04 | norm: 1.0446 | time: 14808.63ms | tok/sec: 35404.22\r\n",
      "step:   498 | loss: 5.198567 | lr: 4.1874e-04 | norm: 0.8570 | time: 14821.73ms | tok/sec: 35372.93\r\n",
      "step:   499 | loss: 5.136405 | lr: 4.1958e-04 | norm: 0.8909 | time: 14804.30ms | tok/sec: 35414.57\r\n",
      "validation loss: 5.2600\r\n",
      "step:   500 | loss: 5.214598 | lr: 4.2042e-04 | norm: 0.8074 | time: 19101.65ms | tok/sec: 27447.26\r\n",
      "step:   501 | loss: 5.154950 | lr: 4.2126e-04 | norm: 0.8104 | time: 14772.36ms | tok/sec: 35491.14\r\n",
      "step:   502 | loss: 5.160583 | lr: 4.2210e-04 | norm: 0.8693 | time: 14869.49ms | tok/sec: 35259.31\r\n",
      "step:   503 | loss: 5.113753 | lr: 4.2294e-04 | norm: 1.0393 | time: 14813.58ms | tok/sec: 35392.39\r\n",
      "step:   504 | loss: 5.206889 | lr: 4.2378e-04 | norm: 1.0301 | time: 14820.44ms | tok/sec: 35376.00\r\n",
      "step:   505 | loss: 5.141056 | lr: 4.2462e-04 | norm: 0.9123 | time: 14832.70ms | tok/sec: 35346.77\r\n",
      "step:   506 | loss: 5.174286 | lr: 4.2545e-04 | norm: 1.1875 | time: 14873.06ms | tok/sec: 35250.85\r\n",
      "step:   507 | loss: 5.178795 | lr: 4.2629e-04 | norm: 0.8756 | time: 14884.51ms | tok/sec: 35223.74\r\n",
      "step:   508 | loss: 5.075459 | lr: 4.2713e-04 | norm: 0.8518 | time: 14888.44ms | tok/sec: 35214.43\r\n",
      "step:   509 | loss: 5.132910 | lr: 4.2797e-04 | norm: 0.9969 | time: 14868.69ms | tok/sec: 35261.22\r\n",
      "step:   510 | loss: 5.285456 | lr: 4.2881e-04 | norm: 1.0703 | time: 14859.31ms | tok/sec: 35283.48\r\n",
      "step:   511 | loss: 5.311949 | lr: 4.2965e-04 | norm: 1.1274 | time: 14922.64ms | tok/sec: 35133.72\r\n",
      "step:   512 | loss: 5.311604 | lr: 4.3049e-04 | norm: 0.9471 | time: 14920.78ms | tok/sec: 35138.11\r\n",
      "step:   513 | loss: 5.280125 | lr: 4.3133e-04 | norm: 0.9648 | time: 14866.78ms | tok/sec: 35265.74\r\n",
      "step:   514 | loss: 5.316957 | lr: 4.3217e-04 | norm: 1.1847 | time: 14911.28ms | tok/sec: 35160.50\r\n",
      "step:   515 | loss: 5.280793 | lr: 4.3301e-04 | norm: 0.8904 | time: 14911.94ms | tok/sec: 35158.94\r\n",
      "step:   516 | loss: 5.419149 | lr: 4.3385e-04 | norm: 1.0616 | time: 14880.47ms | tok/sec: 35233.28\r\n",
      "step:   517 | loss: 5.365156 | lr: 4.3469e-04 | norm: 1.1024 | time: 14877.26ms | tok/sec: 35240.90\r\n",
      "step:   518 | loss: 5.282663 | lr: 4.3552e-04 | norm: 0.8719 | time: 14829.17ms | tok/sec: 35355.19\r\n",
      "step:   519 | loss: 5.295154 | lr: 4.3636e-04 | norm: 0.7246 | time: 14857.19ms | tok/sec: 35288.51\r\n",
      "step:   520 | loss: 5.273630 | lr: 4.3720e-04 | norm: 0.7688 | time: 14856.25ms | tok/sec: 35290.74\r\n",
      "step:   521 | loss: 5.262219 | lr: 4.3804e-04 | norm: 0.9379 | time: 14839.08ms | tok/sec: 35331.57\r\n",
      "step:   522 | loss: 5.286949 | lr: 4.3888e-04 | norm: 1.0877 | time: 14883.35ms | tok/sec: 35226.47\r\n",
      "step:   523 | loss: 5.198456 | lr: 4.3972e-04 | norm: 0.8998 | time: 14848.10ms | tok/sec: 35310.10\r\n",
      "step:   524 | loss: 5.224268 | lr: 4.4056e-04 | norm: 1.1472 | time: 14808.13ms | tok/sec: 35405.41\r\n",
      "validation loss: 5.1978\r\n",
      "step:   525 | loss: 5.195658 | lr: 4.4140e-04 | norm: 0.9389 | time: 18452.51ms | tok/sec: 28412.83\r\n",
      "step:   526 | loss: 5.230324 | lr: 4.4224e-04 | norm: 0.8132 | time: 14815.63ms | tok/sec: 35387.50\r\n",
      "step:   527 | loss: 5.195539 | lr: 4.4308e-04 | norm: 0.6615 | time: 14826.09ms | tok/sec: 35362.51\r\n",
      "step:   528 | loss: 5.193771 | lr: 4.4392e-04 | norm: 0.6382 | time: 14826.02ms | tok/sec: 35362.70\r\n",
      "step:   529 | loss: 5.188000 | lr: 4.4476e-04 | norm: 0.6257 | time: 14855.87ms | tok/sec: 35291.64\r\n",
      "step:   530 | loss: 5.188990 | lr: 4.4559e-04 | norm: 0.7993 | time: 14859.56ms | tok/sec: 35282.89\r\n",
      "step:   531 | loss: 5.179860 | lr: 4.4643e-04 | norm: 0.9329 | time: 14846.46ms | tok/sec: 35314.01\r\n",
      "step:   532 | loss: 5.169800 | lr: 4.4727e-04 | norm: 1.0455 | time: 14847.49ms | tok/sec: 35311.56\r\n",
      "step:   533 | loss: 5.183290 | lr: 4.4811e-04 | norm: 0.9484 | time: 14825.38ms | tok/sec: 35364.22\r\n",
      "step:   534 | loss: 5.130530 | lr: 4.4895e-04 | norm: 0.9218 | time: 14845.82ms | tok/sec: 35315.53\r\n",
      "step:   535 | loss: 5.160212 | lr: 4.4979e-04 | norm: 1.1516 | time: 14857.01ms | tok/sec: 35288.93\r\n",
      "step:   536 | loss: 5.202924 | lr: 4.5063e-04 | norm: 0.9406 | time: 14809.74ms | tok/sec: 35401.56\r\n",
      "step:   537 | loss: 5.094589 | lr: 4.5147e-04 | norm: 0.9883 | time: 14819.59ms | tok/sec: 35378.03\r\n",
      "step:   538 | loss: 5.133211 | lr: 4.5231e-04 | norm: 0.9883 | time: 14847.72ms | tok/sec: 35311.00\r\n",
      "step:   539 | loss: 5.140828 | lr: 4.5315e-04 | norm: 0.9892 | time: 14825.46ms | tok/sec: 35364.03\r\n",
      "step:   540 | loss: 5.132784 | lr: 4.5399e-04 | norm: 1.0129 | time: 14842.18ms | tok/sec: 35324.19\r\n",
      "step:   541 | loss: 5.071290 | lr: 4.5483e-04 | norm: 0.8443 | time: 14781.25ms | tok/sec: 35469.80\r\n",
      "step:   542 | loss: 5.142114 | lr: 4.5566e-04 | norm: 0.8113 | time: 14849.21ms | tok/sec: 35307.46\r\n",
      "step:   543 | loss: 5.108949 | lr: 4.5650e-04 | norm: 0.8029 | time: 14841.22ms | tok/sec: 35326.46\r\n",
      "step:   544 | loss: 5.109788 | lr: 4.5734e-04 | norm: 0.8374 | time: 14815.81ms | tok/sec: 35387.07\r\n",
      "step:   545 | loss: 5.056443 | lr: 4.5818e-04 | norm: 0.8376 | time: 14845.46ms | tok/sec: 35316.39\r\n",
      "step:   546 | loss: 4.995439 | lr: 4.5902e-04 | norm: 0.7020 | time: 14802.73ms | tok/sec: 35418.33\r\n",
      "step:   547 | loss: 4.969437 | lr: 4.5986e-04 | norm: 0.7850 | time: 14822.23ms | tok/sec: 35371.74\r\n",
      "step:   548 | loss: 5.034390 | lr: 4.6070e-04 | norm: 1.0012 | time: 14831.62ms | tok/sec: 35349.33\r\n",
      "step:   549 | loss: 5.032865 | lr: 4.6154e-04 | norm: 0.9536 | time: 14863.10ms | tok/sec: 35274.48\r\n",
      "validation loss: 5.1125\r\n",
      "step:   550 | loss: 5.031802 | lr: 4.6238e-04 | norm: 0.8297 | time: 18469.27ms | tok/sec: 28387.04\r\n",
      "step:   551 | loss: 5.020662 | lr: 4.6322e-04 | norm: 0.9660 | time: 14848.86ms | tok/sec: 35308.29\r\n",
      "step:   552 | loss: 4.957531 | lr: 4.6406e-04 | norm: 0.7971 | time: 14821.29ms | tok/sec: 35373.97\r\n",
      "step:   553 | loss: 5.074966 | lr: 4.6490e-04 | norm: 0.9094 | time: 14841.39ms | tok/sec: 35326.07\r\n",
      "step:   554 | loss: 4.997867 | lr: 4.6573e-04 | norm: 0.8971 | time: 14803.87ms | tok/sec: 35415.61\r\n",
      "step:   555 | loss: 4.975616 | lr: 4.6657e-04 | norm: 0.9009 | time: 14823.27ms | tok/sec: 35369.25\r\n",
      "step:   556 | loss: 5.011181 | lr: 4.6741e-04 | norm: 0.8214 | time: 14898.48ms | tok/sec: 35190.70\r\n",
      "step:   557 | loss: 5.137116 | lr: 4.6825e-04 | norm: 0.7925 | time: 14855.76ms | tok/sec: 35291.90\r\n",
      "step:   558 | loss: 5.157157 | lr: 4.6909e-04 | norm: 0.9067 | time: 14822.64ms | tok/sec: 35370.75\r\n",
      "step:   559 | loss: 5.109426 | lr: 4.6993e-04 | norm: 0.9723 | time: 14817.07ms | tok/sec: 35384.05\r\n",
      "step:   560 | loss: 5.160709 | lr: 4.7077e-04 | norm: 0.7541 | time: 14864.93ms | tok/sec: 35270.12\r\n",
      "step:   561 | loss: 5.100998 | lr: 4.7161e-04 | norm: 0.8356 | time: 14857.44ms | tok/sec: 35287.90\r\n",
      "step:   562 | loss: 5.113173 | lr: 4.7245e-04 | norm: 0.7357 | time: 14863.00ms | tok/sec: 35274.72\r\n",
      "step:   563 | loss: 5.082266 | lr: 4.7329e-04 | norm: 0.7580 | time: 14824.49ms | tok/sec: 35366.35\r\n",
      "step:   564 | loss: 5.132306 | lr: 4.7413e-04 | norm: 0.7291 | time: 14795.89ms | tok/sec: 35434.70\r\n",
      "step:   565 | loss: 5.108622 | lr: 4.7497e-04 | norm: 0.6928 | time: 14856.58ms | tok/sec: 35289.96\r\n",
      "step:   566 | loss: 5.077537 | lr: 4.7580e-04 | norm: 0.8570 | time: 14823.27ms | tok/sec: 35369.25\r\n",
      "step:   567 | loss: 5.155712 | lr: 4.7664e-04 | norm: 0.8980 | time: 14861.79ms | tok/sec: 35277.58\r\n",
      "step:   568 | loss: 5.097181 | lr: 4.7748e-04 | norm: 1.1962 | time: 14827.72ms | tok/sec: 35358.64\r\n",
      "step:   569 | loss: 5.116597 | lr: 4.7832e-04 | norm: 0.8700 | time: 14812.78ms | tok/sec: 35394.30\r\n",
      "step:   570 | loss: 5.057706 | lr: 4.7916e-04 | norm: 0.8909 | time: 14837.56ms | tok/sec: 35335.18\r\n",
      "step:   571 | loss: 5.047112 | lr: 4.8000e-04 | norm: 0.8706 | time: 14778.71ms | tok/sec: 35475.91\r\n",
      "step:   572 | loss: 5.051296 | lr: 4.8084e-04 | norm: 0.8964 | time: 17050.86ms | tok/sec: 30748.49\r\n",
      "step:   573 | loss: 5.077685 | lr: 4.8168e-04 | norm: 0.9475 | time: 14878.69ms | tok/sec: 35237.51\r\n",
      "step:   574 | loss: 5.050856 | lr: 4.8252e-04 | norm: 1.0231 | time: 14955.64ms | tok/sec: 35056.21\r\n",
      "validation loss: 5.0408\r\n",
      "step:   575 | loss: 5.057973 | lr: 4.8336e-04 | norm: 0.9913 | time: 18440.64ms | tok/sec: 28431.12\r\n",
      "step:   576 | loss: 5.069897 | lr: 4.8420e-04 | norm: 0.9872 | time: 14789.52ms | tok/sec: 35449.96\r\n",
      "step:   577 | loss: 5.080858 | lr: 4.8503e-04 | norm: 0.9304 | time: 14753.17ms | tok/sec: 35537.31\r\n",
      "step:   578 | loss: 5.057974 | lr: 4.8587e-04 | norm: 0.8070 | time: 14729.43ms | tok/sec: 35594.58\r\n",
      "step:   579 | loss: 5.040816 | lr: 4.8671e-04 | norm: 0.8555 | time: 14739.25ms | tok/sec: 35570.88\r\n",
      "step:   580 | loss: 4.978570 | lr: 4.8755e-04 | norm: 0.7550 | time: 14721.26ms | tok/sec: 35614.35\r\n",
      "step:   581 | loss: 5.005508 | lr: 4.8839e-04 | norm: 0.6595 | time: 14724.57ms | tok/sec: 35606.33\r\n",
      "step:   582 | loss: 4.867911 | lr: 4.8923e-04 | norm: 0.9726 | time: 14730.91ms | tok/sec: 35591.01\r\n",
      "step:   583 | loss: 4.934227 | lr: 4.9007e-04 | norm: 0.6475 | time: 14755.41ms | tok/sec: 35531.92\r\n",
      "step:   584 | loss: 4.972429 | lr: 4.9091e-04 | norm: 0.6028 | time: 14763.21ms | tok/sec: 35513.13\r\n",
      "step:   585 | loss: 4.938835 | lr: 4.9175e-04 | norm: 0.6578 | time: 14764.52ms | tok/sec: 35510.00\r\n",
      "step:   586 | loss: 4.961808 | lr: 4.9259e-04 | norm: 0.7987 | time: 14769.90ms | tok/sec: 35497.06\r\n",
      "step:   587 | loss: 5.017594 | lr: 4.9343e-04 | norm: 1.2329 | time: 14821.39ms | tok/sec: 35373.74\r\n",
      "step:   588 | loss: 4.996276 | lr: 4.9427e-04 | norm: 0.9147 | time: 14796.88ms | tok/sec: 35432.33\r\n",
      "step:   589 | loss: 5.015904 | lr: 4.9510e-04 | norm: 0.9068 | time: 14822.48ms | tok/sec: 35371.15\r\n",
      "step:   590 | loss: 4.993901 | lr: 4.9594e-04 | norm: 0.7643 | time: 14820.59ms | tok/sec: 35375.66\r\n",
      "step:   591 | loss: 4.977029 | lr: 4.9678e-04 | norm: 0.7347 | time: 14778.87ms | tok/sec: 35475.51\r\n",
      "step:   592 | loss: 4.844326 | lr: 4.9762e-04 | norm: 0.7319 | time: 14820.20ms | tok/sec: 35376.59\r\n",
      "step:   593 | loss: 4.842998 | lr: 4.9846e-04 | norm: 0.8420 | time: 14879.81ms | tok/sec: 35234.86\r\n",
      "step:   594 | loss: 4.865414 | lr: 4.9930e-04 | norm: 0.8569 | time: 14804.67ms | tok/sec: 35413.69\r\n",
      "step:   595 | loss: 4.917752 | lr: 5.0014e-04 | norm: 0.8366 | time: 14801.34ms | tok/sec: 35421.66\r\n",
      "step:   596 | loss: 4.853506 | lr: 5.0098e-04 | norm: 0.8595 | time: 14811.07ms | tok/sec: 35398.39\r\n",
      "step:   597 | loss: 4.932844 | lr: 5.0182e-04 | norm: 1.3051 | time: 14822.62ms | tok/sec: 35370.81\r\n",
      "step:   598 | loss: 4.908208 | lr: 5.0266e-04 | norm: 0.7626 | time: 14837.56ms | tok/sec: 35335.20\r\n",
      "step:   599 | loss: 4.846058 | lr: 5.0350e-04 | norm: 0.6903 | time: 14842.59ms | tok/sec: 35323.22\r\n",
      "validation loss: 4.9676\r\n",
      "step:   600 | loss: 4.842670 | lr: 5.0434e-04 | norm: 0.8567 | time: 18450.98ms | tok/sec: 28415.18\r\n",
      "step:   601 | loss: 4.839123 | lr: 5.0517e-04 | norm: 1.1103 | time: 14796.55ms | tok/sec: 35433.12\r\n",
      "step:   602 | loss: 4.856742 | lr: 5.0601e-04 | norm: 1.0061 | time: 14822.58ms | tok/sec: 35370.90\r\n",
      "step:   603 | loss: 4.934000 | lr: 5.0685e-04 | norm: 0.7967 | time: 14840.12ms | tok/sec: 35329.09\r\n",
      "step:   604 | loss: 5.006512 | lr: 5.0769e-04 | norm: 0.7918 | time: 14830.43ms | tok/sec: 35352.17\r\n",
      "step:   605 | loss: 5.045315 | lr: 5.0853e-04 | norm: 0.7670 | time: 14846.55ms | tok/sec: 35313.80\r\n",
      "step:   606 | loss: 4.996225 | lr: 5.0937e-04 | norm: 0.9039 | time: 14848.65ms | tok/sec: 35308.79\r\n",
      "step:   607 | loss: 5.015443 | lr: 5.1021e-04 | norm: 1.1013 | time: 14878.97ms | tok/sec: 35236.84\r\n",
      "step:   608 | loss: 5.002340 | lr: 5.1105e-04 | norm: 0.9936 | time: 14846.25ms | tok/sec: 35314.51\r\n",
      "step:   609 | loss: 4.995438 | lr: 5.1189e-04 | norm: 0.8083 | time: 14816.01ms | tok/sec: 35386.57\r\n",
      "step:   610 | loss: 4.996757 | lr: 5.1273e-04 | norm: 0.7363 | time: 14840.89ms | tok/sec: 35327.25\r\n",
      "step:   611 | loss: 4.986323 | lr: 5.1357e-04 | norm: 0.7790 | time: 14815.95ms | tok/sec: 35386.73\r\n",
      "step:   612 | loss: 4.997946 | lr: 5.1441e-04 | norm: 0.7635 | time: 14842.02ms | tok/sec: 35324.57\r\n",
      "step:   613 | loss: 4.946792 | lr: 5.1524e-04 | norm: 0.8301 | time: 14792.00ms | tok/sec: 35444.02\r\n",
      "step:   614 | loss: 4.999491 | lr: 5.1608e-04 | norm: 0.9673 | time: 14808.64ms | tok/sec: 35404.19\r\n",
      "step:   615 | loss: 5.019549 | lr: 5.1692e-04 | norm: 0.9904 | time: 14838.30ms | tok/sec: 35333.42\r\n",
      "step:   616 | loss: 4.940041 | lr: 5.1776e-04 | norm: 1.0470 | time: 14797.35ms | tok/sec: 35431.20\r\n",
      "step:   617 | loss: 4.986005 | lr: 5.1860e-04 | norm: 0.7975 | time: 14809.20ms | tok/sec: 35402.86\r\n",
      "step:   618 | loss: 4.928761 | lr: 5.1944e-04 | norm: 0.7367 | time: 14751.68ms | tok/sec: 35540.91\r\n",
      "step:   619 | loss: 4.923741 | lr: 5.2028e-04 | norm: 0.7097 | time: 14804.60ms | tok/sec: 35413.85\r\n",
      "step:   620 | loss: 4.994257 | lr: 5.2112e-04 | norm: 0.8699 | time: 14795.53ms | tok/sec: 35435.56\r\n",
      "step:   621 | loss: 4.934267 | lr: 5.2196e-04 | norm: 0.8747 | time: 14754.68ms | tok/sec: 35533.69\r\n",
      "step:   622 | loss: 4.958577 | lr: 5.2280e-04 | norm: 0.7596 | time: 14770.68ms | tok/sec: 35495.18\r\n",
      "step:   623 | loss: 4.926982 | lr: 5.2364e-04 | norm: 0.8422 | time: 14762.84ms | tok/sec: 35514.03\r\n",
      "step:   624 | loss: 4.920080 | lr: 5.2448e-04 | norm: 0.7761 | time: 14783.08ms | tok/sec: 35465.41\r\n",
      "validation loss: 4.8922\r\n",
      "step:   625 | loss: 4.916040 | lr: 5.2531e-04 | norm: 0.7066 | time: 18353.51ms | tok/sec: 28566.09\r\n",
      "step:   626 | loss: 4.956660 | lr: 5.2615e-04 | norm: 0.7172 | time: 14792.87ms | tok/sec: 35441.94\r\n",
      "step:   627 | loss: 4.840079 | lr: 5.2699e-04 | norm: 0.8081 | time: 14763.95ms | tok/sec: 35511.36\r\n",
      "step:   628 | loss: 4.875084 | lr: 5.2783e-04 | norm: 0.7542 | time: 14772.12ms | tok/sec: 35491.73\r\n",
      "step:   629 | loss: 4.857150 | lr: 5.2867e-04 | norm: 0.6675 | time: 14822.92ms | tok/sec: 35370.09\r\n",
      "step:   630 | loss: 4.856900 | lr: 5.2951e-04 | norm: 0.6905 | time: 14800.29ms | tok/sec: 35424.16\r\n",
      "step:   631 | loss: 4.839865 | lr: 5.3035e-04 | norm: 0.6309 | time: 14828.90ms | tok/sec: 35355.81\r\n",
      "step:   632 | loss: 4.832966 | lr: 5.3119e-04 | norm: 0.6421 | time: 14787.70ms | tok/sec: 35454.33\r\n",
      "step:   633 | loss: 4.870759 | lr: 5.3203e-04 | norm: 0.6709 | time: 14819.37ms | tok/sec: 35378.56\r\n",
      "step:   634 | loss: 4.847814 | lr: 5.3287e-04 | norm: 0.6756 | time: 14844.44ms | tok/sec: 35318.81\r\n",
      "step:   635 | loss: 4.874786 | lr: 5.3371e-04 | norm: 0.6643 | time: 14841.93ms | tok/sec: 35324.80\r\n",
      "step:   636 | loss: 4.902443 | lr: 5.3455e-04 | norm: 0.6049 | time: 14792.40ms | tok/sec: 35443.07\r\n",
      "step:   637 | loss: 5.129944 | lr: 5.3538e-04 | norm: 0.7655 | time: 14801.74ms | tok/sec: 35420.71\r\n",
      "step:   638 | loss: 4.801647 | lr: 5.3622e-04 | norm: 0.8298 | time: 14780.20ms | tok/sec: 35472.33\r\n",
      "step:   639 | loss: 4.840905 | lr: 5.3706e-04 | norm: 0.9526 | time: 14782.85ms | tok/sec: 35465.95\r\n",
      "step:   640 | loss: 4.677893 | lr: 5.3790e-04 | norm: 1.0011 | time: 14762.93ms | tok/sec: 35513.82\r\n",
      "step:   641 | loss: 4.808727 | lr: 5.3874e-04 | norm: 1.1143 | time: 14789.41ms | tok/sec: 35450.24\r\n",
      "step:   642 | loss: 4.806416 | lr: 5.3958e-04 | norm: 1.0202 | time: 14793.77ms | tok/sec: 35439.78\r\n",
      "step:   643 | loss: 4.829372 | lr: 5.4042e-04 | norm: 1.0429 | time: 14811.01ms | tok/sec: 35398.53\r\n",
      "step:   644 | loss: 4.779075 | lr: 5.4126e-04 | norm: 0.8111 | time: 14821.51ms | tok/sec: 35373.44\r\n",
      "step:   645 | loss: 4.732204 | lr: 5.4210e-04 | norm: 0.7100 | time: 14782.34ms | tok/sec: 35467.18\r\n",
      "step:   646 | loss: 4.742464 | lr: 5.4294e-04 | norm: 0.7023 | time: 14763.55ms | tok/sec: 35512.33\r\n",
      "step:   647 | loss: 4.751993 | lr: 5.4378e-04 | norm: 0.6428 | time: 14796.51ms | tok/sec: 35433.23\r\n",
      "step:   648 | loss: 4.773213 | lr: 5.4462e-04 | norm: 0.7194 | time: 14802.70ms | tok/sec: 35418.39\r\n",
      "step:   649 | loss: 4.734308 | lr: 5.4545e-04 | norm: 0.7345 | time: 14822.01ms | tok/sec: 35372.26\r\n",
      "validation loss: 4.8480\r\n",
      "step:   650 | loss: 4.845915 | lr: 5.4629e-04 | norm: 0.8276 | time: 18406.69ms | tok/sec: 28483.55\r\n",
      "step:   651 | loss: 4.911637 | lr: 5.4713e-04 | norm: 0.8842 | time: 14825.95ms | tok/sec: 35362.86\r\n",
      "step:   652 | loss: 4.972675 | lr: 5.4797e-04 | norm: 0.8368 | time: 14856.90ms | tok/sec: 35289.19\r\n",
      "step:   653 | loss: 4.912869 | lr: 5.4881e-04 | norm: 1.0166 | time: 14868.09ms | tok/sec: 35262.64\r\n",
      "step:   654 | loss: 4.883142 | lr: 5.4965e-04 | norm: 0.9624 | time: 14819.92ms | tok/sec: 35377.25\r\n",
      "step:   655 | loss: 4.929144 | lr: 5.5049e-04 | norm: 0.7905 | time: 14828.83ms | tok/sec: 35355.98\r\n",
      "step:   656 | loss: 4.914383 | lr: 5.5133e-04 | norm: 0.6876 | time: 14855.60ms | tok/sec: 35292.28\r\n",
      "step:   657 | loss: 4.856792 | lr: 5.5217e-04 | norm: 0.6703 | time: 14859.10ms | tok/sec: 35283.98\r\n",
      "step:   658 | loss: 4.845543 | lr: 5.5301e-04 | norm: 0.6496 | time: 14830.37ms | tok/sec: 35352.33\r\n",
      "step:   659 | loss: 4.917142 | lr: 5.5385e-04 | norm: 0.6914 | time: 14847.83ms | tok/sec: 35310.74\r\n",
      "step:   660 | loss: 4.836091 | lr: 5.5469e-04 | norm: 0.6163 | time: 14859.03ms | tok/sec: 35284.13\r\n",
      "step:   661 | loss: 4.866119 | lr: 5.5552e-04 | norm: 0.6621 | time: 14839.77ms | tok/sec: 35329.93\r\n",
      "step:   662 | loss: 4.786454 | lr: 5.5636e-04 | norm: 0.6990 | time: 14862.71ms | tok/sec: 35275.39\r\n",
      "step:   663 | loss: 4.810051 | lr: 5.5720e-04 | norm: 0.7831 | time: 14811.07ms | tok/sec: 35398.39\r\n",
      "step:   664 | loss: 4.790358 | lr: 5.5804e-04 | norm: 0.6829 | time: 14808.82ms | tok/sec: 35403.76\r\n",
      "step:   665 | loss: 4.777924 | lr: 5.5888e-04 | norm: 0.7523 | time: 14859.86ms | tok/sec: 35282.16\r\n",
      "step:   666 | loss: 4.831107 | lr: 5.5972e-04 | norm: 0.7457 | time: 14840.66ms | tok/sec: 35327.82\r\n",
      "step:   667 | loss: 4.782608 | lr: 5.6056e-04 | norm: 0.7146 | time: 14846.78ms | tok/sec: 35313.25\r\n",
      "step:   668 | loss: 4.784410 | lr: 5.6140e-04 | norm: 0.7356 | time: 14829.54ms | tok/sec: 35354.31\r\n",
      "step:   669 | loss: 4.782067 | lr: 5.6224e-04 | norm: 0.6446 | time: 14824.34ms | tok/sec: 35366.69\r\n",
      "step:   670 | loss: 4.710353 | lr: 5.6308e-04 | norm: 0.6870 | time: 14821.95ms | tok/sec: 35372.41\r\n",
      "step:   671 | loss: 4.791712 | lr: 5.6392e-04 | norm: 0.7203 | time: 14801.77ms | tok/sec: 35420.63\r\n",
      "step:   672 | loss: 4.769234 | lr: 5.6476e-04 | norm: 0.7228 | time: 14798.93ms | tok/sec: 35427.44\r\n",
      "step:   673 | loss: 4.768774 | lr: 5.6559e-04 | norm: 0.7542 | time: 14784.09ms | tok/sec: 35463.00\r\n",
      "step:   674 | loss: 4.735672 | lr: 5.6643e-04 | norm: 0.8072 | time: 14791.16ms | tok/sec: 35446.04\r\n",
      "validation loss: 4.7558\r\n",
      "step:   675 | loss: 4.717492 | lr: 5.6727e-04 | norm: 0.7015 | time: 18374.54ms | tok/sec: 28533.40\r\n",
      "step:   676 | loss: 4.747966 | lr: 5.6811e-04 | norm: 0.6335 | time: 14767.36ms | tok/sec: 35503.16\r\n",
      "step:   677 | loss: 4.768392 | lr: 5.6895e-04 | norm: 0.6836 | time: 14813.34ms | tok/sec: 35392.97\r\n",
      "step:   678 | loss: 4.753934 | lr: 5.6979e-04 | norm: 0.6995 | time: 14840.38ms | tok/sec: 35328.47\r\n",
      "step:   679 | loss: 4.718211 | lr: 5.7063e-04 | norm: 0.7687 | time: 14798.20ms | tok/sec: 35429.17\r\n",
      "step:   680 | loss: 4.731611 | lr: 5.7147e-04 | norm: 0.8032 | time: 14838.94ms | tok/sec: 35331.91\r\n",
      "step:   681 | loss: 4.716900 | lr: 5.7231e-04 | norm: 0.7029 | time: 14791.47ms | tok/sec: 35445.28\r\n",
      "step:   682 | loss: 4.723793 | lr: 5.7315e-04 | norm: 0.5911 | time: 14803.83ms | tok/sec: 35415.71\r\n",
      "step:   683 | loss: 4.722142 | lr: 5.7399e-04 | norm: 0.6173 | time: 14819.24ms | tok/sec: 35378.88\r\n",
      "step:   684 | loss: 4.687597 | lr: 5.7483e-04 | norm: 0.7871 | time: 14829.58ms | tok/sec: 35354.21\r\n",
      "step:   685 | loss: 4.648657 | lr: 5.7566e-04 | norm: 0.8518 | time: 14840.66ms | tok/sec: 35327.81\r\n",
      "step:   686 | loss: 4.617589 | lr: 5.7650e-04 | norm: 0.7677 | time: 14810.54ms | tok/sec: 35399.65\r\n",
      "step:   687 | loss: 4.661719 | lr: 5.7734e-04 | norm: 0.8639 | time: 14824.17ms | tok/sec: 35367.10\r\n",
      "step:   688 | loss: 4.652984 | lr: 5.7818e-04 | norm: 0.9527 | time: 14781.48ms | tok/sec: 35469.26\r\n",
      "step:   689 | loss: 4.648218 | lr: 5.7902e-04 | norm: 0.9746 | time: 14786.79ms | tok/sec: 35456.51\r\n",
      "step:   690 | loss: 4.662323 | lr: 5.7986e-04 | norm: 0.8476 | time: 14724.04ms | tok/sec: 35607.62\r\n",
      "step:   691 | loss: 4.661184 | lr: 5.8070e-04 | norm: 0.8635 | time: 14800.46ms | tok/sec: 35423.76\r\n",
      "step:   692 | loss: 4.682905 | lr: 5.8154e-04 | norm: 0.8802 | time: 14820.35ms | tok/sec: 35376.22\r\n",
      "step:   693 | loss: 4.625410 | lr: 5.8238e-04 | norm: 0.9369 | time: 14802.06ms | tok/sec: 35419.95\r\n",
      "step:   694 | loss: 4.691727 | lr: 5.8322e-04 | norm: 0.8474 | time: 14783.31ms | tok/sec: 35464.86\r\n",
      "step:   695 | loss: 4.585399 | lr: 5.8406e-04 | norm: 0.7058 | time: 14787.11ms | tok/sec: 35455.75\r\n",
      "step:   696 | loss: 4.707712 | lr: 5.8490e-04 | norm: 0.6902 | time: 14778.23ms | tok/sec: 35477.04\r\n",
      "step:   697 | loss: 4.840132 | lr: 5.8573e-04 | norm: 0.7529 | time: 14721.79ms | tok/sec: 35613.06\r\n",
      "step:   698 | loss: 4.888152 | lr: 5.8657e-04 | norm: 0.7825 | time: 14753.19ms | tok/sec: 35537.27\r\n",
      "step:   699 | loss: 4.797029 | lr: 5.8741e-04 | norm: 0.8653 | time: 14733.12ms | tok/sec: 35585.68\r\n",
      "validation loss: 4.7200\r\n",
      "step:   700 | loss: 4.789818 | lr: 5.8825e-04 | norm: 0.7967 | time: 18312.69ms | tok/sec: 28629.77\r\n",
      "step:   701 | loss: 4.796523 | lr: 5.8909e-04 | norm: 0.9658 | time: 14735.24ms | tok/sec: 35580.55\r\n",
      "step:   702 | loss: 4.739266 | lr: 5.8993e-04 | norm: 0.8136 | time: 14782.17ms | tok/sec: 35467.59\r\n",
      "step:   703 | loss: 4.790441 | lr: 5.9077e-04 | norm: 0.8458 | time: 14788.46ms | tok/sec: 35452.51\r\n",
      "step:   704 | loss: 4.798138 | lr: 5.9161e-04 | norm: 0.8242 | time: 14791.13ms | tok/sec: 35446.11\r\n",
      "step:   705 | loss: 4.727417 | lr: 5.9245e-04 | norm: 0.7229 | time: 14822.26ms | tok/sec: 35371.66\r\n",
      "step:   706 | loss: 4.746343 | lr: 5.9329e-04 | norm: 0.6785 | time: 14811.51ms | tok/sec: 35397.33\r\n",
      "step:   707 | loss: 4.790743 | lr: 5.9413e-04 | norm: 0.6564 | time: 14817.04ms | tok/sec: 35384.12\r\n",
      "step:   708 | loss: 4.761063 | lr: 5.9497e-04 | norm: 0.6520 | time: 14841.11ms | tok/sec: 35326.73\r\n",
      "step:   709 | loss: 4.780003 | lr: 5.9580e-04 | norm: 0.6674 | time: 14805.33ms | tok/sec: 35412.11\r\n",
      "step:   710 | loss: 4.710550 | lr: 5.9664e-04 | norm: 0.7550 | time: 14839.87ms | tok/sec: 35329.68\r\n",
      "step:   711 | loss: 4.726617 | lr: 5.9748e-04 | norm: 0.7840 | time: 14882.61ms | tok/sec: 35228.24\r\n",
      "step:   712 | loss: 4.693516 | lr: 5.9832e-04 | norm: 0.8088 | time: 14908.35ms | tok/sec: 35167.41\r\n",
      "step:   713 | loss: 4.696074 | lr: 5.9916e-04 | norm: 0.9005 | time: 14831.04ms | tok/sec: 35350.72\r\n",
      "step:   714 | loss: 4.665391 | lr: 6.0000e-04 | norm: 0.7250 | time: 14882.15ms | tok/sec: 35229.33\r\n",
      "step:   715 | loss: 4.734764 | lr: 6.0000e-04 | norm: 0.6063 | time: 14799.79ms | tok/sec: 35425.36\r\n",
      "step:   716 | loss: 4.683423 | lr: 5.9998e-04 | norm: 0.6126 | time: 14821.38ms | tok/sec: 35373.76\r\n",
      "step:   717 | loss: 4.699806 | lr: 5.9993e-04 | norm: 0.7567 | time: 14817.22ms | tok/sec: 35383.69\r\n",
      "step:   718 | loss: 4.766206 | lr: 5.9985e-04 | norm: 0.8749 | time: 14863.63ms | tok/sec: 35273.21\r\n",
      "step:   719 | loss: 4.699945 | lr: 5.9974e-04 | norm: 0.7358 | time: 14860.05ms | tok/sec: 35281.70\r\n",
      "step:   720 | loss: 4.693896 | lr: 5.9959e-04 | norm: 0.7548 | time: 14806.33ms | tok/sec: 35409.71\r\n",
      "step:   721 | loss: 4.596388 | lr: 5.9941e-04 | norm: 0.6516 | time: 14781.66ms | tok/sec: 35468.83\r\n",
      "step:   722 | loss: 4.627625 | lr: 5.9920e-04 | norm: 0.5627 | time: 14749.84ms | tok/sec: 35545.33\r\n",
      "step:   723 | loss: 4.615915 | lr: 5.9895e-04 | norm: 0.6029 | time: 14720.86ms | tok/sec: 35615.31\r\n",
      "step:   724 | loss: 4.626308 | lr: 5.9867e-04 | norm: 0.6203 | time: 14737.99ms | tok/sec: 35573.91\r\n",
      "validation loss: 4.6253\r\n",
      "step:   725 | loss: 4.657336 | lr: 5.9836e-04 | norm: 0.6718 | time: 18287.30ms | tok/sec: 28669.51\r\n",
      "step:   726 | loss: 4.681518 | lr: 5.9802e-04 | norm: 0.9711 | time: 14744.92ms | tok/sec: 35557.21\r\n",
      "step:   727 | loss: 4.633042 | lr: 5.9764e-04 | norm: 0.8600 | time: 14772.31ms | tok/sec: 35491.27\r\n",
      "step:   728 | loss: 4.598824 | lr: 5.9723e-04 | norm: 0.7552 | time: 14776.55ms | tok/sec: 35481.08\r\n",
      "step:   729 | loss: 4.612915 | lr: 5.9679e-04 | norm: 0.7055 | time: 14774.36ms | tok/sec: 35486.34\r\n",
      "step:   730 | loss: 4.604735 | lr: 5.9632e-04 | norm: 0.6956 | time: 14787.77ms | tok/sec: 35454.15\r\n",
      "step:   731 | loss: 4.623637 | lr: 5.9581e-04 | norm: 0.7072 | time: 14820.81ms | tok/sec: 35375.11\r\n",
      "step:   732 | loss: 4.532408 | lr: 5.9527e-04 | norm: 0.7516 | time: 14854.31ms | tok/sec: 35295.35\r\n",
      "step:   733 | loss: 4.519493 | lr: 5.9470e-04 | norm: 0.7037 | time: 14863.24ms | tok/sec: 35274.13\r\n",
      "step:   734 | loss: 4.483371 | lr: 5.9410e-04 | norm: 0.6017 | time: 14880.90ms | tok/sec: 35232.28\r\n",
      "step:   735 | loss: 4.480801 | lr: 5.9347e-04 | norm: 0.5941 | time: 14814.61ms | tok/sec: 35389.93\r\n",
      "step:   736 | loss: 4.501504 | lr: 5.9280e-04 | norm: 0.4983 | time: 14856.51ms | tok/sec: 35290.11\r\n",
      "step:   737 | loss: 4.474742 | lr: 5.9210e-04 | norm: 0.4987 | time: 14896.59ms | tok/sec: 35195.17\r\n",
      "step:   738 | loss: 4.495061 | lr: 5.9137e-04 | norm: 0.5143 | time: 14820.12ms | tok/sec: 35376.78\r\n",
      "step:   739 | loss: 4.479383 | lr: 5.9061e-04 | norm: 0.5181 | time: 14804.99ms | tok/sec: 35412.91\r\n",
      "step:   740 | loss: 4.439061 | lr: 5.8981e-04 | norm: 0.6213 | time: 14754.45ms | tok/sec: 35534.23\r\n",
      "step:   741 | loss: 4.475604 | lr: 5.8899e-04 | norm: 0.7132 | time: 14703.88ms | tok/sec: 35656.44\r\n",
      "step:   742 | loss: 4.444219 | lr: 5.8813e-04 | norm: 0.7914 | time: 14756.40ms | tok/sec: 35529.54\r\n",
      "step:   743 | loss: 4.477566 | lr: 5.8724e-04 | norm: 0.7657 | time: 14710.99ms | tok/sec: 35639.22\r\n",
      "step:   744 | loss: 4.642437 | lr: 5.8632e-04 | norm: 0.7058 | time: 14738.49ms | tok/sec: 35572.72\r\n",
      "step:   745 | loss: 4.597940 | lr: 5.8537e-04 | norm: 0.7277 | time: 14730.92ms | tok/sec: 35590.99\r\n",
      "step:   746 | loss: 4.640910 | lr: 5.8439e-04 | norm: 0.7167 | time: 14735.46ms | tok/sec: 35580.03\r\n",
      "step:   747 | loss: 4.656548 | lr: 5.8338e-04 | norm: 0.7123 | time: 14779.89ms | tok/sec: 35473.07\r\n",
      "step:   748 | loss: 4.634688 | lr: 5.8233e-04 | norm: 0.7394 | time: 14787.03ms | tok/sec: 35455.95\r\n",
      "step:   749 | loss: 4.588133 | lr: 5.8126e-04 | norm: 0.8277 | time: 14753.11ms | tok/sec: 35537.45\r\n",
      "validation loss: 4.5585\r\n",
      "step:   750 | loss: 4.650014 | lr: 5.8015e-04 | norm: 0.7557 | time: 18430.82ms | tok/sec: 28446.27\r\n",
      "step:   751 | loss: 4.724232 | lr: 5.7902e-04 | norm: 0.9132 | time: 14881.60ms | tok/sec: 35230.62\r\n",
      "step:   752 | loss: 4.622462 | lr: 5.7785e-04 | norm: 0.7946 | time: 14801.80ms | tok/sec: 35420.56\r\n",
      "step:   753 | loss: 4.657428 | lr: 5.7666e-04 | norm: 0.6244 | time: 14846.64ms | tok/sec: 35313.58\r\n",
      "step:   754 | loss: 4.614084 | lr: 5.7543e-04 | norm: 0.6285 | time: 14857.42ms | tok/sec: 35287.96\r\n",
      "step:   755 | loss: 4.590014 | lr: 5.7418e-04 | norm: 0.5317 | time: 14832.44ms | tok/sec: 35347.38\r\n",
      "step:   756 | loss: 4.597705 | lr: 5.7289e-04 | norm: 0.5605 | time: 14804.90ms | tok/sec: 35413.14\r\n",
      "step:   757 | loss: 4.552810 | lr: 5.7158e-04 | norm: 0.6087 | time: 14889.14ms | tok/sec: 35212.77\r\n",
      "step:   758 | loss: 4.578277 | lr: 5.7023e-04 | norm: 0.7009 | time: 14773.48ms | tok/sec: 35488.45\r\n",
      "step:   759 | loss: 4.629419 | lr: 5.6886e-04 | norm: 0.7636 | time: 14803.57ms | tok/sec: 35416.33\r\n",
      "step:   760 | loss: 4.547903 | lr: 5.6746e-04 | norm: 0.7514 | time: 14815.50ms | tok/sec: 35387.81\r\n",
      "step:   761 | loss: 4.627399 | lr: 5.6603e-04 | norm: 0.8530 | time: 14823.37ms | tok/sec: 35369.02\r\n",
      "step:   762 | loss: 4.633874 | lr: 5.6457e-04 | norm: 0.8352 | time: 16884.06ms | tok/sec: 31052.25\r\n",
      "step:   763 | loss: 4.561384 | lr: 5.6308e-04 | norm: 0.8001 | time: 14740.92ms | tok/sec: 35566.84\r\n",
      "step:   764 | loss: 4.591365 | lr: 5.6156e-04 | norm: 0.8772 | time: 14880.24ms | tok/sec: 35233.84\r\n",
      "step:   765 | loss: 4.537710 | lr: 5.6002e-04 | norm: 0.8417 | time: 14880.26ms | tok/sec: 35233.78\r\n",
      "step:   766 | loss: 4.592473 | lr: 5.5845e-04 | norm: 0.6622 | time: 14881.69ms | tok/sec: 35230.40\r\n",
      "step:   767 | loss: 4.536681 | lr: 5.5685e-04 | norm: 0.6527 | time: 14813.98ms | tok/sec: 35391.42\r\n",
      "step:   768 | loss: 4.489307 | lr: 5.5522e-04 | norm: 0.6027 | time: 14765.42ms | tok/sec: 35507.84\r\n",
      "step:   769 | loss: 4.481920 | lr: 5.5356e-04 | norm: 0.6493 | time: 14778.82ms | tok/sec: 35475.64\r\n",
      "step:   770 | loss: 4.523302 | lr: 5.5188e-04 | norm: 0.6205 | time: 14736.92ms | tok/sec: 35576.51\r\n",
      "step:   771 | loss: 4.489095 | lr: 5.5017e-04 | norm: 0.5902 | time: 14727.91ms | tok/sec: 35598.27\r\n",
      "step:   772 | loss: 4.524654 | lr: 5.4843e-04 | norm: 0.5272 | time: 14685.65ms | tok/sec: 35700.69\r\n",
      "step:   773 | loss: 4.498611 | lr: 5.4667e-04 | norm: 0.4764 | time: 14712.82ms | tok/sec: 35634.78\r\n",
      "step:   774 | loss: 4.502567 | lr: 5.4488e-04 | norm: 0.4828 | time: 14660.77ms | tok/sec: 35761.29\r\n",
      "validation loss: 4.4605\r\n",
      "step:   775 | loss: 4.447357 | lr: 5.4307e-04 | norm: 0.4610 | time: 18297.88ms | tok/sec: 28652.94\r\n",
      "step:   776 | loss: 4.456588 | lr: 5.4123e-04 | norm: 0.5377 | time: 14774.45ms | tok/sec: 35486.13\r\n",
      "step:   777 | loss: 4.441970 | lr: 5.3936e-04 | norm: 0.6310 | time: 14825.78ms | tok/sec: 35363.26\r\n",
      "step:   778 | loss: 4.409012 | lr: 5.3747e-04 | norm: 0.7813 | time: 14817.02ms | tok/sec: 35384.16\r\n",
      "step:   779 | loss: 4.475065 | lr: 5.3555e-04 | norm: 0.8181 | time: 14777.29ms | tok/sec: 35479.30\r\n",
      "step:   780 | loss: 4.390289 | lr: 5.3361e-04 | norm: 0.8455 | time: 14708.96ms | tok/sec: 35644.12\r\n",
      "step:   781 | loss: 4.383527 | lr: 5.3164e-04 | norm: 0.7229 | time: 14680.33ms | tok/sec: 35713.64\r\n",
      "step:   782 | loss: 4.318414 | lr: 5.2965e-04 | norm: 0.6681 | time: 14689.37ms | tok/sec: 35691.66\r\n",
      "step:   783 | loss: 4.327894 | lr: 5.2763e-04 | norm: 0.6316 | time: 14686.89ms | tok/sec: 35697.69\r\n",
      "step:   784 | loss: 4.348875 | lr: 5.2559e-04 | norm: 0.5892 | time: 14682.96ms | tok/sec: 35707.25\r\n",
      "step:   785 | loss: 4.434722 | lr: 5.2353e-04 | norm: 0.5375 | time: 14685.99ms | tok/sec: 35699.88\r\n",
      "step:   786 | loss: 4.317852 | lr: 5.2144e-04 | norm: 0.4897 | time: 14647.14ms | tok/sec: 35794.57\r\n",
      "step:   787 | loss: 4.375945 | lr: 5.1933e-04 | norm: 0.4968 | time: 14704.35ms | tok/sec: 35655.31\r\n",
      "step:   788 | loss: 4.335146 | lr: 5.1720e-04 | norm: 0.4906 | time: 14690.29ms | tok/sec: 35689.42\r\n",
      "step:   789 | loss: 4.354288 | lr: 5.1504e-04 | norm: 0.4821 | time: 14727.61ms | tok/sec: 35599.00\r\n",
      "step:   790 | loss: 4.459128 | lr: 5.1287e-04 | norm: 0.5859 | time: 14764.94ms | tok/sec: 35508.97\r\n",
      "step:   791 | loss: 4.474787 | lr: 5.1067e-04 | norm: 0.7171 | time: 14743.63ms | tok/sec: 35560.31\r\n",
      "step:   792 | loss: 4.491064 | lr: 5.0844e-04 | norm: 0.6583 | time: 14793.39ms | tok/sec: 35440.70\r\n",
      "step:   793 | loss: 4.497534 | lr: 5.0620e-04 | norm: 0.6184 | time: 14705.92ms | tok/sec: 35651.50\r\n",
      "step:   794 | loss: 4.472597 | lr: 5.0393e-04 | norm: 0.5732 | time: 14755.58ms | tok/sec: 35531.52\r\n",
      "step:   795 | loss: 4.495267 | lr: 5.0165e-04 | norm: 0.5503 | time: 14741.66ms | tok/sec: 35565.06\r\n",
      "step:   796 | loss: 4.517905 | lr: 4.9934e-04 | norm: 0.4239 | time: 14821.12ms | tok/sec: 35374.38\r\n",
      "step:   797 | loss: 4.428465 | lr: 4.9701e-04 | norm: 0.4786 | time: 14760.21ms | tok/sec: 35520.37\r\n",
      "step:   798 | loss: 4.449955 | lr: 4.9466e-04 | norm: 0.4586 | time: 14757.92ms | tok/sec: 35525.87\r\n",
      "step:   799 | loss: 4.432665 | lr: 4.9229e-04 | norm: 0.4406 | time: 14730.30ms | tok/sec: 35592.47\r\n",
      "validation loss: 4.3797\r\n",
      "step:   800 | loss: 4.464879 | lr: 4.8990e-04 | norm: 0.4207 | time: 18472.09ms | tok/sec: 28382.71\r\n",
      "step:   801 | loss: 4.411101 | lr: 4.8750e-04 | norm: 0.4658 | time: 14836.45ms | tok/sec: 35337.83\r\n",
      "step:   802 | loss: 4.468482 | lr: 4.8507e-04 | norm: 0.5035 | time: 14767.54ms | tok/sec: 35502.72\r\n",
      "step:   803 | loss: 4.455299 | lr: 4.8262e-04 | norm: 0.5349 | time: 14772.44ms | tok/sec: 35490.94\r\n",
      "step:   804 | loss: 4.451229 | lr: 4.8016e-04 | norm: 0.5307 | time: 14846.89ms | tok/sec: 35312.98\r\n",
      "step:   805 | loss: 4.420647 | lr: 4.7768e-04 | norm: 0.5063 | time: 14839.22ms | tok/sec: 35331.23\r\n",
      "step:   806 | loss: 4.414689 | lr: 4.7518e-04 | norm: 0.6129 | time: 14814.89ms | tok/sec: 35389.26\r\n",
      "step:   807 | loss: 4.432111 | lr: 4.7266e-04 | norm: 0.6726 | time: 14871.45ms | tok/sec: 35254.67\r\n",
      "step:   808 | loss: 4.461699 | lr: 4.7012e-04 | norm: 0.6360 | time: 14808.14ms | tok/sec: 35405.39\r\n",
      "step:   809 | loss: 4.415510 | lr: 4.6757e-04 | norm: 0.5321 | time: 14781.18ms | tok/sec: 35469.96\r\n",
      "step:   810 | loss: 4.428374 | lr: 4.6500e-04 | norm: 0.5841 | time: 14797.18ms | tok/sec: 35431.62\r\n",
      "step:   811 | loss: 4.373766 | lr: 4.6241e-04 | norm: 0.5849 | time: 14759.01ms | tok/sec: 35523.26\r\n",
      "step:   812 | loss: 4.381846 | lr: 4.5981e-04 | norm: 0.5946 | time: 14707.86ms | tok/sec: 35646.79\r\n",
      "step:   813 | loss: 4.378790 | lr: 4.5720e-04 | norm: 0.5720 | time: 14704.87ms | tok/sec: 35654.04\r\n",
      "step:   814 | loss: 4.373292 | lr: 4.5456e-04 | norm: 0.5745 | time: 14716.87ms | tok/sec: 35624.96\r\n",
      "step:   815 | loss: 4.353090 | lr: 4.5191e-04 | norm: 0.5760 | time: 14680.58ms | tok/sec: 35713.03\r\n",
      "step:   816 | loss: 4.327778 | lr: 4.4925e-04 | norm: 0.4758 | time: 14699.94ms | tok/sec: 35665.99\r\n",
      "step:   817 | loss: 4.309859 | lr: 4.4657e-04 | norm: 0.4434 | time: 14715.85ms | tok/sec: 35627.43\r\n",
      "step:   818 | loss: 4.351685 | lr: 4.4388e-04 | norm: 0.4937 | time: 14704.21ms | tok/sec: 35655.64\r\n",
      "step:   819 | loss: 4.360040 | lr: 4.4118e-04 | norm: 0.4805 | time: 14749.75ms | tok/sec: 35545.55\r\n",
      "step:   820 | loss: 4.320102 | lr: 4.3846e-04 | norm: 0.4389 | time: 14762.72ms | tok/sec: 35514.33\r\n",
      "step:   821 | loss: 4.313916 | lr: 4.3573e-04 | norm: 0.4996 | time: 14726.74ms | tok/sec: 35601.10\r\n",
      "step:   822 | loss: 4.388074 | lr: 4.3298e-04 | norm: 0.4855 | time: 14767.23ms | tok/sec: 35503.47\r\n",
      "step:   823 | loss: 4.327212 | lr: 4.3022e-04 | norm: 0.4399 | time: 14822.22ms | tok/sec: 35371.76\r\n",
      "step:   824 | loss: 4.199184 | lr: 4.2745e-04 | norm: 0.4482 | time: 14817.21ms | tok/sec: 35383.71\r\n",
      "validation loss: 4.3168\r\n",
      "step:   825 | loss: 4.196266 | lr: 4.2467e-04 | norm: 0.4349 | time: 18336.09ms | tok/sec: 28593.22\r\n",
      "step:   826 | loss: 4.255594 | lr: 4.2188e-04 | norm: 0.4897 | time: 14752.79ms | tok/sec: 35538.22\r\n",
      "step:   827 | loss: 4.233146 | lr: 4.1908e-04 | norm: 0.5674 | time: 14822.12ms | tok/sec: 35372.00\r\n",
      "step:   828 | loss: 4.256354 | lr: 4.1626e-04 | norm: 0.4632 | time: 14792.23ms | tok/sec: 35443.46\r\n",
      "step:   829 | loss: 4.202795 | lr: 4.1343e-04 | norm: 0.4146 | time: 14780.81ms | tok/sec: 35470.86\r\n",
      "step:   830 | loss: 4.193152 | lr: 4.1060e-04 | norm: 0.4076 | time: 14851.25ms | tok/sec: 35302.63\r\n",
      "step:   831 | loss: 4.212270 | lr: 4.0775e-04 | norm: 0.3841 | time: 14828.61ms | tok/sec: 35356.52\r\n",
      "step:   832 | loss: 4.098722 | lr: 4.0490e-04 | norm: 0.3761 | time: 14839.86ms | tok/sec: 35329.71\r\n",
      "step:   833 | loss: 4.178521 | lr: 4.0203e-04 | norm: 0.3962 | time: 14776.25ms | tok/sec: 35481.80\r\n",
      "step:   834 | loss: 4.160628 | lr: 3.9916e-04 | norm: 0.3932 | time: 14768.84ms | tok/sec: 35499.61\r\n",
      "step:   835 | loss: 4.324057 | lr: 3.9628e-04 | norm: 0.4559 | time: 14773.43ms | tok/sec: 35488.57\r\n",
      "step:   836 | loss: 4.340569 | lr: 3.9339e-04 | norm: 0.6347 | time: 14779.51ms | tok/sec: 35473.97\r\n",
      "step:   837 | loss: 4.341063 | lr: 3.9050e-04 | norm: 0.7285 | time: 14782.34ms | tok/sec: 35467.18\r\n",
      "step:   838 | loss: 4.392429 | lr: 3.8759e-04 | norm: 0.6615 | time: 14719.98ms | tok/sec: 35617.44\r\n",
      "step:   839 | loss: 4.372987 | lr: 3.8468e-04 | norm: 0.7326 | time: 14743.70ms | tok/sec: 35560.14\r\n",
      "step:   840 | loss: 4.342105 | lr: 3.8176e-04 | norm: 0.6356 | time: 14712.52ms | tok/sec: 35635.49\r\n",
      "step:   841 | loss: 4.411907 | lr: 3.7884e-04 | norm: 0.4592 | time: 14717.86ms | tok/sec: 35622.57\r\n",
      "step:   842 | loss: 4.378585 | lr: 3.7591e-04 | norm: 0.5001 | time: 14742.46ms | tok/sec: 35563.13\r\n",
      "step:   843 | loss: 4.322701 | lr: 3.7297e-04 | norm: 0.4179 | time: 14722.50ms | tok/sec: 35611.33\r\n",
      "step:   844 | loss: 4.397307 | lr: 3.7003e-04 | norm: 0.6087 | time: 14734.43ms | tok/sec: 35582.51\r\n",
      "step:   845 | loss: 4.372733 | lr: 3.6709e-04 | norm: 0.5266 | time: 14693.79ms | tok/sec: 35680.93\r\n",
      "step:   846 | loss: 4.368978 | lr: 3.6414e-04 | norm: 0.4683 | time: 14757.43ms | tok/sec: 35527.04\r\n",
      "step:   847 | loss: 4.338577 | lr: 3.6118e-04 | norm: 0.4537 | time: 14743.56ms | tok/sec: 35560.47\r\n",
      "step:   848 | loss: 4.385696 | lr: 3.5822e-04 | norm: 0.4423 | time: 14767.54ms | tok/sec: 35502.74\r\n",
      "step:   849 | loss: 4.342490 | lr: 3.5526e-04 | norm: 0.3871 | time: 14759.74ms | tok/sec: 35521.48\r\n",
      "validation loss: 4.2706\r\n",
      "step:   850 | loss: 4.323575 | lr: 3.5230e-04 | norm: 0.3888 | time: 18383.33ms | tok/sec: 28519.75\r\n",
      "step:   851 | loss: 4.308551 | lr: 3.4933e-04 | norm: 0.3535 | time: 14830.68ms | tok/sec: 35351.57\r\n",
      "step:   852 | loss: 4.357474 | lr: 3.4636e-04 | norm: 0.3979 | time: 14763.58ms | tok/sec: 35512.24\r\n",
      "step:   853 | loss: 4.355540 | lr: 3.4339e-04 | norm: 0.4203 | time: 14774.82ms | tok/sec: 35485.23\r\n",
      "step:   854 | loss: 4.289888 | lr: 3.4041e-04 | norm: 0.3912 | time: 14814.27ms | tok/sec: 35390.73\r\n",
      "step:   855 | loss: 4.320910 | lr: 3.3744e-04 | norm: 0.3954 | time: 14785.39ms | tok/sec: 35459.88\r\n",
      "step:   856 | loss: 4.321567 | lr: 3.3446e-04 | norm: 0.4362 | time: 14755.03ms | tok/sec: 35532.82\r\n",
      "step:   857 | loss: 4.359811 | lr: 3.3149e-04 | norm: 0.5132 | time: 14766.38ms | tok/sec: 35505.51\r\n",
      "step:   858 | loss: 4.236039 | lr: 3.2851e-04 | norm: 0.4794 | time: 14824.08ms | tok/sec: 35367.32\r\n",
      "step:   859 | loss: 4.267554 | lr: 3.2554e-04 | norm: 0.3954 | time: 14723.95ms | tok/sec: 35607.85\r\n",
      "step:   860 | loss: 4.250369 | lr: 3.2256e-04 | norm: 0.4395 | time: 14796.84ms | tok/sec: 35432.42\r\n",
      "step:   861 | loss: 4.239813 | lr: 3.1959e-04 | norm: 0.4042 | time: 14798.25ms | tok/sec: 35429.04\r\n",
      "step:   862 | loss: 4.264277 | lr: 3.1661e-04 | norm: 0.3826 | time: 14705.28ms | tok/sec: 35653.03\r\n",
      "step:   863 | loss: 4.263577 | lr: 3.1364e-04 | norm: 0.3950 | time: 14722.19ms | tok/sec: 35612.09\r\n",
      "step:   864 | loss: 4.206009 | lr: 3.1067e-04 | norm: 0.4373 | time: 14720.10ms | tok/sec: 35617.16\r\n",
      "step:   865 | loss: 4.250620 | lr: 3.0770e-04 | norm: 0.4038 | time: 14713.92ms | tok/sec: 35632.11\r\n",
      "step:   866 | loss: 4.212661 | lr: 3.0474e-04 | norm: 0.3879 | time: 14685.49ms | tok/sec: 35701.09\r\n",
      "step:   867 | loss: 4.247461 | lr: 3.0178e-04 | norm: 0.4311 | time: 14688.70ms | tok/sec: 35693.28\r\n",
      "step:   868 | loss: 4.184621 | lr: 2.9882e-04 | norm: 0.4107 | time: 14702.45ms | tok/sec: 35659.91\r\n",
      "step:   869 | loss: 4.126829 | lr: 2.9586e-04 | norm: 0.4050 | time: 14732.38ms | tok/sec: 35587.46\r\n",
      "step:   870 | loss: 4.160837 | lr: 2.9291e-04 | norm: 0.3900 | time: 14719.77ms | tok/sec: 35617.96\r\n",
      "step:   871 | loss: 4.103945 | lr: 2.8997e-04 | norm: 0.3495 | time: 14705.89ms | tok/sec: 35651.57\r\n",
      "step:   872 | loss: 4.122492 | lr: 2.8703e-04 | norm: 0.3859 | time: 14762.33ms | tok/sec: 35515.25\r\n",
      "step:   873 | loss: 4.131453 | lr: 2.8409e-04 | norm: 0.3789 | time: 14721.16ms | tok/sec: 35614.59\r\n",
      "step:   874 | loss: 4.132420 | lr: 2.8116e-04 | norm: 0.3716 | time: 14738.78ms | tok/sec: 35572.01\r\n",
      "validation loss: 4.2179\r\n",
      "step:   875 | loss: 4.053518 | lr: 2.7824e-04 | norm: 0.3804 | time: 18361.27ms | tok/sec: 28554.01\r\n",
      "step:   876 | loss: 4.038270 | lr: 2.7532e-04 | norm: 0.3592 | time: 14723.98ms | tok/sec: 35607.76\r\n",
      "step:   877 | loss: 4.165203 | lr: 2.7241e-04 | norm: 0.3466 | time: 14765.44ms | tok/sec: 35507.79\r\n",
      "step:   878 | loss: 4.108512 | lr: 2.6950e-04 | norm: 0.3680 | time: 14730.37ms | tok/sec: 35592.33\r\n",
      "step:   879 | loss: 4.140440 | lr: 2.6661e-04 | norm: 0.3775 | time: 14757.27ms | tok/sec: 35527.45\r\n",
      "step:   880 | loss: 4.133108 | lr: 2.6372e-04 | norm: 0.4210 | time: 14745.55ms | tok/sec: 35555.67\r\n",
      "step:   881 | loss: 4.194069 | lr: 2.6084e-04 | norm: 0.4182 | time: 14820.85ms | tok/sec: 35375.03\r\n",
      "step:   882 | loss: 4.346545 | lr: 2.5797e-04 | norm: 0.4190 | time: 14827.52ms | tok/sec: 35359.11\r\n",
      "step:   883 | loss: 4.346540 | lr: 2.5510e-04 | norm: 0.4506 | time: 14820.11ms | tok/sec: 35376.81\r\n",
      "step:   884 | loss: 4.246941 | lr: 2.5225e-04 | norm: 0.4066 | time: 14744.39ms | tok/sec: 35558.48\r\n",
      "step:   885 | loss: 4.312778 | lr: 2.4940e-04 | norm: 0.3898 | time: 14742.74ms | tok/sec: 35562.45\r\n",
      "step:   886 | loss: 4.232401 | lr: 2.4657e-04 | norm: 0.3934 | time: 14800.83ms | tok/sec: 35422.89\r\n",
      "step:   887 | loss: 4.269655 | lr: 2.4374e-04 | norm: 0.3750 | time: 14723.19ms | tok/sec: 35609.68\r\n",
      "step:   888 | loss: 4.268827 | lr: 2.4092e-04 | norm: 0.3686 | time: 14715.80ms | tok/sec: 35627.56\r\n",
      "step:   889 | loss: 4.275241 | lr: 2.3812e-04 | norm: 0.4445 | time: 14693.04ms | tok/sec: 35682.75\r\n",
      "step:   890 | loss: 4.259920 | lr: 2.3533e-04 | norm: 0.3721 | time: 14699.39ms | tok/sec: 35667.33\r\n",
      "step:   891 | loss: 4.249781 | lr: 2.3255e-04 | norm: 0.3880 | time: 14697.25ms | tok/sec: 35672.51\r\n",
      "step:   892 | loss: 4.272754 | lr: 2.2978e-04 | norm: 0.3616 | time: 14663.51ms | tok/sec: 35754.60\r\n",
      "step:   893 | loss: 4.216627 | lr: 2.2702e-04 | norm: 0.3730 | time: 14701.80ms | tok/sec: 35661.48\r\n",
      "step:   894 | loss: 4.232244 | lr: 2.2427e-04 | norm: 0.3510 | time: 14725.26ms | tok/sec: 35604.67\r\n",
      "step:   895 | loss: 4.257477 | lr: 2.2154e-04 | norm: 0.3469 | time: 14767.40ms | tok/sec: 35503.07\r\n",
      "step:   896 | loss: 4.217206 | lr: 2.1882e-04 | norm: 0.3719 | time: 14733.35ms | tok/sec: 35585.13\r\n",
      "step:   897 | loss: 4.211786 | lr: 2.1612e-04 | norm: 0.3291 | time: 14703.15ms | tok/sec: 35658.21\r\n",
      "step:   898 | loss: 4.234384 | lr: 2.1343e-04 | norm: 0.3412 | time: 14745.21ms | tok/sec: 35556.50\r\n",
      "step:   899 | loss: 4.207832 | lr: 2.1075e-04 | norm: 0.3302 | time: 14701.10ms | tok/sec: 35663.17\r\n",
      "validation loss: 4.1770\r\n",
      "step:   900 | loss: 4.218666 | lr: 2.0809e-04 | norm: 0.3235 | time: 18299.60ms | tok/sec: 28650.24\r\n",
      "step:   901 | loss: 4.202370 | lr: 2.0544e-04 | norm: 0.3183 | time: 14745.03ms | tok/sec: 35556.94\r\n",
      "step:   902 | loss: 4.231652 | lr: 2.0280e-04 | norm: 0.3468 | time: 14746.17ms | tok/sec: 35554.18\r\n",
      "step:   903 | loss: 4.212483 | lr: 2.0019e-04 | norm: 0.3609 | time: 14747.64ms | tok/sec: 35550.63\r\n",
      "step:   904 | loss: 4.131701 | lr: 1.9759e-04 | norm: 0.3257 | time: 14765.37ms | tok/sec: 35507.94\r\n",
      "step:   905 | loss: 4.217348 | lr: 1.9500e-04 | norm: 0.3438 | time: 14793.23ms | tok/sec: 35441.07\r\n",
      "step:   906 | loss: 4.157411 | lr: 1.9243e-04 | norm: 0.3116 | time: 14758.50ms | tok/sec: 35524.47\r\n",
      "step:   907 | loss: 4.143703 | lr: 1.8988e-04 | norm: 0.3659 | time: 14753.71ms | tok/sec: 35536.02\r\n",
      "step:   908 | loss: 4.158813 | lr: 1.8734e-04 | norm: 0.3582 | time: 14804.18ms | tok/sec: 35414.87\r\n",
      "step:   909 | loss: 4.175315 | lr: 1.8482e-04 | norm: 0.3471 | time: 14815.68ms | tok/sec: 35387.37\r\n",
      "step:   910 | loss: 4.208611 | lr: 1.8232e-04 | norm: 0.3248 | time: 14784.48ms | tok/sec: 35462.05\r\n",
      "step:   911 | loss: 4.180107 | lr: 1.7984e-04 | norm: 0.3168 | time: 14804.45ms | tok/sec: 35414.22\r\n",
      "step:   912 | loss: 4.199706 | lr: 1.7738e-04 | norm: 0.3611 | time: 14835.45ms | tok/sec: 35340.22\r\n",
      "step:   913 | loss: 4.191192 | lr: 1.7493e-04 | norm: 0.3675 | time: 14817.12ms | tok/sec: 35383.94\r\n",
      "step:   914 | loss: 4.186473 | lr: 1.7250e-04 | norm: 0.3389 | time: 14840.26ms | tok/sec: 35328.76\r\n",
      "step:   915 | loss: 4.020687 | lr: 1.7010e-04 | norm: 0.3696 | time: 14818.18ms | tok/sec: 35381.40\r\n",
      "step:   916 | loss: 4.066006 | lr: 1.6771e-04 | norm: 0.3840 | time: 14810.10ms | tok/sec: 35400.72\r\n",
      "step:   917 | loss: 4.081809 | lr: 1.6534e-04 | norm: 0.3664 | time: 14828.65ms | tok/sec: 35356.43\r\n",
      "step:   918 | loss: 4.063617 | lr: 1.6299e-04 | norm: 0.3325 | time: 14863.28ms | tok/sec: 35274.05\r\n",
      "step:   919 | loss: 4.048287 | lr: 1.6066e-04 | norm: 0.3441 | time: 14879.21ms | tok/sec: 35236.29\r\n",
      "step:   920 | loss: 4.006842 | lr: 1.5835e-04 | norm: 0.3327 | time: 14840.62ms | tok/sec: 35327.90\r\n",
      "step:   921 | loss: 4.046973 | lr: 1.5607e-04 | norm: 0.3629 | time: 14753.42ms | tok/sec: 35536.70\r\n",
      "step:   922 | loss: 4.082940 | lr: 1.5380e-04 | norm: 0.3225 | time: 14720.22ms | tok/sec: 35616.86\r\n",
      "step:   923 | loss: 4.073210 | lr: 1.5156e-04 | norm: 0.3073 | time: 14713.18ms | tok/sec: 35633.91\r\n",
      "step:   924 | loss: 4.053668 | lr: 1.4933e-04 | norm: 0.2992 | time: 14739.41ms | tok/sec: 35570.48\r\n",
      "validation loss: 4.1493\r\n",
      "step:   925 | loss: 4.069161 | lr: 1.4713e-04 | norm: 0.3161 | time: 18293.88ms | tok/sec: 28659.21\r\n",
      "step:   926 | loss: 4.093108 | lr: 1.4496e-04 | norm: 0.3362 | time: 14734.71ms | tok/sec: 35581.84\r\n",
      "step:   927 | loss: 4.239079 | lr: 1.4280e-04 | norm: 0.3233 | time: 14756.08ms | tok/sec: 35530.30\r\n",
      "step:   928 | loss: 4.255935 | lr: 1.4067e-04 | norm: 0.3421 | time: 14737.38ms | tok/sec: 35575.38\r\n",
      "step:   929 | loss: 4.185577 | lr: 1.3856e-04 | norm: 0.3596 | time: 14762.06ms | tok/sec: 35515.91\r\n",
      "step:   930 | loss: 4.182075 | lr: 1.3647e-04 | norm: 0.3168 | time: 14752.68ms | tok/sec: 35538.50\r\n",
      "step:   931 | loss: 4.220221 | lr: 1.3441e-04 | norm: 0.3473 | time: 14798.75ms | tok/sec: 35427.86\r\n",
      "step:   932 | loss: 4.198982 | lr: 1.3237e-04 | norm: 0.3330 | time: 14797.51ms | tok/sec: 35430.83\r\n",
      "step:   933 | loss: 4.259072 | lr: 1.3035e-04 | norm: 0.3653 | time: 14755.57ms | tok/sec: 35531.52\r\n",
      "step:   934 | loss: 4.192634 | lr: 1.2836e-04 | norm: 0.3544 | time: 14813.61ms | tok/sec: 35392.32\r\n",
      "step:   935 | loss: 4.250081 | lr: 1.2639e-04 | norm: 0.3575 | time: 14850.53ms | tok/sec: 35304.33\r\n",
      "step:   936 | loss: 4.248120 | lr: 1.2445e-04 | norm: 0.3278 | time: 14754.47ms | tok/sec: 35534.18\r\n",
      "step:   937 | loss: 4.230072 | lr: 1.2253e-04 | norm: 0.3310 | time: 14758.06ms | tok/sec: 35525.53\r\n",
      "step:   938 | loss: 4.174728 | lr: 1.2064e-04 | norm: 0.3305 | time: 14680.12ms | tok/sec: 35714.16\r\n",
      "step:   939 | loss: 4.235385 | lr: 1.1877e-04 | norm: 0.3419 | time: 14715.99ms | tok/sec: 35627.10\r\n",
      "step:   940 | loss: 4.206819 | lr: 1.1693e-04 | norm: 0.3306 | time: 14720.52ms | tok/sec: 35616.14\r\n",
      "step:   941 | loss: 4.158360 | lr: 1.1512e-04 | norm: 0.3270 | time: 14717.16ms | tok/sec: 35624.26\r\n",
      "step:   942 | loss: 4.249179 | lr: 1.1333e-04 | norm: 0.3063 | time: 14749.18ms | tok/sec: 35546.92\r\n",
      "step:   943 | loss: 4.193765 | lr: 1.1157e-04 | norm: 0.3174 | time: 14748.30ms | tok/sec: 35549.04\r\n",
      "step:   944 | loss: 4.187173 | lr: 1.0983e-04 | norm: 0.3021 | time: 14771.15ms | tok/sec: 35494.06\r\n",
      "step:   945 | loss: 4.187762 | lr: 1.0812e-04 | norm: 0.3179 | time: 14792.43ms | tok/sec: 35443.00\r\n",
      "step:   946 | loss: 4.173629 | lr: 1.0644e-04 | norm: 0.3117 | time: 14826.09ms | tok/sec: 35362.52\r\n",
      "step:   947 | loss: 4.187033 | lr: 1.0478e-04 | norm: 0.3062 | time: 14825.89ms | tok/sec: 35363.00\r\n",
      "step:   948 | loss: 4.193347 | lr: 1.0315e-04 | norm: 0.2758 | time: 14784.27ms | tok/sec: 35462.55\r\n",
      "step:   949 | loss: 4.197136 | lr: 1.0155e-04 | norm: 0.2993 | time: 14810.54ms | tok/sec: 35399.66\r\n",
      "validation loss: 4.1197\r\n",
      "step:   950 | loss: 4.147913 | lr: 9.9982e-05 | norm: 0.2878 | time: 18334.02ms | tok/sec: 28596.46\r\n",
      "step:   951 | loss: 4.155203 | lr: 9.8437e-05 | norm: 0.2797 | time: 14759.23ms | tok/sec: 35522.73\r\n",
      "step:   952 | loss: 4.093274 | lr: 9.6921e-05 | norm: 0.2933 | time: 14766.28ms | tok/sec: 35505.77\r\n",
      "step:   953 | loss: 4.074516 | lr: 9.5433e-05 | norm: 0.2775 | time: 16917.73ms | tok/sec: 30990.45\r\n",
      "step:   954 | loss: 4.151214 | lr: 9.3973e-05 | norm: 0.2645 | time: 14824.82ms | tok/sec: 35365.56\r\n",
      "step:   955 | loss: 4.139076 | lr: 9.2542e-05 | norm: 0.2770 | time: 14844.67ms | tok/sec: 35318.26\r\n",
      "step:   956 | loss: 4.107865 | lr: 9.1140e-05 | norm: 0.2747 | time: 14786.90ms | tok/sec: 35456.26\r\n",
      "step:   957 | loss: 4.093582 | lr: 8.9767e-05 | norm: 0.3047 | time: 14694.13ms | tok/sec: 35680.09\r\n",
      "step:   958 | loss: 4.066319 | lr: 8.8423e-05 | norm: 0.2995 | time: 14697.34ms | tok/sec: 35672.30\r\n",
      "step:   959 | loss: 4.109247 | lr: 8.7109e-05 | norm: 0.2724 | time: 14676.94ms | tok/sec: 35721.88\r\n",
      "step:   960 | loss: 4.142278 | lr: 8.5824e-05 | norm: 0.3318 | time: 14677.71ms | tok/sec: 35720.02\r\n",
      "step:   961 | loss: 4.038888 | lr: 8.4568e-05 | norm: 0.3768 | time: 14669.34ms | tok/sec: 35740.40\r\n",
      "step:   962 | loss: 4.019291 | lr: 8.3343e-05 | norm: 0.3298 | time: 14778.42ms | tok/sec: 35476.59\r\n",
      "step:   963 | loss: 4.005189 | lr: 8.2147e-05 | norm: 0.3256 | time: 14775.24ms | tok/sec: 35484.24\r\n",
      "step:   964 | loss: 3.963375 | lr: 8.0982e-05 | norm: 0.3282 | time: 14762.81ms | tok/sec: 35514.11\r\n",
      "step:   965 | loss: 4.040905 | lr: 7.9847e-05 | norm: 0.3409 | time: 14763.16ms | tok/sec: 35513.26\r\n",
      "step:   966 | loss: 3.993408 | lr: 7.8742e-05 | norm: 0.3319 | time: 14829.85ms | tok/sec: 35353.55\r\n",
      "step:   967 | loss: 3.990602 | lr: 7.7668e-05 | norm: 0.3027 | time: 14781.23ms | tok/sec: 35469.85\r\n",
      "step:   968 | loss: 4.001994 | lr: 7.6624e-05 | norm: 0.2979 | time: 14806.98ms | tok/sec: 35408.16\r\n",
      "step:   969 | loss: 4.014067 | lr: 7.5611e-05 | norm: 0.3002 | time: 14800.50ms | tok/sec: 35423.68\r\n",
      "step:   970 | loss: 4.025969 | lr: 7.4629e-05 | norm: 0.2826 | time: 14785.12ms | tok/sec: 35460.53\r\n",
      "step:   971 | loss: 4.044314 | lr: 7.3678e-05 | norm: 0.2858 | time: 14728.19ms | tok/sec: 35597.59\r\n",
      "step:   972 | loss: 4.019459 | lr: 7.2759e-05 | norm: 0.2949 | time: 14773.85ms | tok/sec: 35487.58\r\n",
      "step:   973 | loss: 4.137136 | lr: 7.1870e-05 | norm: 0.2902 | time: 14798.57ms | tok/sec: 35428.29\r\n",
      "step:   974 | loss: 4.167726 | lr: 7.1013e-05 | norm: 0.3063 | time: 14780.72ms | tok/sec: 35471.07\r\n",
      "validation loss: 4.1062\r\n",
      "step:   975 | loss: 4.104485 | lr: 7.0188e-05 | norm: 0.3316 | time: 18304.98ms | tok/sec: 28641.83\r\n",
      "step:   976 | loss: 4.200445 | lr: 6.9394e-05 | norm: 0.2945 | time: 14838.49ms | tok/sec: 35332.97\r\n",
      "step:   977 | loss: 4.188794 | lr: 6.8631e-05 | norm: 0.3305 | time: 14803.53ms | tok/sec: 35416.41\r\n",
      "step:   978 | loss: 4.205327 | lr: 6.7901e-05 | norm: 0.2851 | time: 14817.54ms | tok/sec: 35382.94\r\n",
      "step:   979 | loss: 4.201066 | lr: 6.7202e-05 | norm: 0.3189 | time: 14783.40ms | tok/sec: 35464.63\r\n",
      "step:   980 | loss: 4.168900 | lr: 6.6535e-05 | norm: 0.2860 | time: 14745.06ms | tok/sec: 35556.87\r\n",
      "step:   981 | loss: 4.181403 | lr: 6.5900e-05 | norm: 0.3013 | time: 14808.06ms | tok/sec: 35405.59\r\n",
      "step:   982 | loss: 4.121107 | lr: 6.5297e-05 | norm: 0.2880 | time: 14797.38ms | tok/sec: 35431.13\r\n",
      "step:   983 | loss: 4.176606 | lr: 6.4727e-05 | norm: 0.2751 | time: 14797.41ms | tok/sec: 35431.06\r\n",
      "step:   984 | loss: 4.160096 | lr: 6.4188e-05 | norm: 0.2776 | time: 14792.47ms | tok/sec: 35442.89\r\n",
      "step:   985 | loss: 4.125356 | lr: 6.3682e-05 | norm: 0.2878 | time: 14760.71ms | tok/sec: 35519.17\r\n",
      "step:   986 | loss: 4.135195 | lr: 6.3209e-05 | norm: 0.2803 | time: 14814.71ms | tok/sec: 35389.69\r\n",
      "step:   987 | loss: 4.143121 | lr: 6.2767e-05 | norm: 0.2750 | time: 14837.30ms | tok/sec: 35335.81\r\n",
      "step:   988 | loss: 4.176089 | lr: 6.2359e-05 | norm: 0.2942 | time: 14724.07ms | tok/sec: 35607.53\r\n",
      "step:   989 | loss: 4.131652 | lr: 6.1982e-05 | norm: 0.2924 | time: 14714.86ms | tok/sec: 35629.82\r\n",
      "step:   990 | loss: 4.121272 | lr: 6.1639e-05 | norm: 0.2787 | time: 14711.46ms | tok/sec: 35638.07\r\n",
      "step:   991 | loss: 4.167202 | lr: 6.1328e-05 | norm: 0.2839 | time: 14762.47ms | tok/sec: 35514.93\r\n",
      "step:   992 | loss: 4.145654 | lr: 6.1049e-05 | norm: 0.2655 | time: 14732.38ms | tok/sec: 35587.46\r\n",
      "step:   993 | loss: 4.133436 | lr: 6.0803e-05 | norm: 0.2725 | time: 14753.95ms | tok/sec: 35535.42\r\n",
      "step:   994 | loss: 4.101963 | lr: 6.0590e-05 | norm: 0.2863 | time: 14746.21ms | tok/sec: 35554.09\r\n",
      "step:   995 | loss: 4.109556 | lr: 6.0410e-05 | norm: 0.2691 | time: 14720.04ms | tok/sec: 35617.29\r\n",
      "step:   996 | loss: 4.077963 | lr: 6.0262e-05 | norm: 0.2751 | time: 14695.85ms | tok/sec: 35675.91\r\n",
      "step:   997 | loss: 4.097624 | lr: 6.0148e-05 | norm: 0.2654 | time: 14762.60ms | tok/sec: 35514.62\r\n",
      "step:   998 | loss: 4.100339 | lr: 6.0066e-05 | norm: 0.2761 | time: 14712.99ms | tok/sec: 35634.37\r\n",
      "validation loss: 4.0905\r\n",
      "step:   999 | loss: 4.091728 | lr: 6.0016e-05 | norm: 0.2780 | time: 18927.19ms | tok/sec: 27700.26\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f0d74",
   "metadata": {
    "papermill": {
     "duration": 0.155546,
     "end_time": "2024-07-08T16:58:50.657781",
     "exception": false,
     "start_time": "2024-07-08T16:58:50.502235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## If you want to run on a single device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8887b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T16:58:50.967317Z",
     "iopub.status.busy": "2024-07-08T16:58:50.966228Z",
     "iopub.status.idle": "2024-07-08T16:58:50.971362Z",
     "shell.execute_reply": "2024-07-08T16:58:50.970470Z"
    },
    "papermill": {
     "duration": 0.161817,
     "end_time": "2024-07-08T16:58:50.973338",
     "exception": false,
     "start_time": "2024-07-08T16:58:50.811521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f7660d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T16:58:51.284216Z",
     "iopub.status.busy": "2024-07-08T16:58:51.283626Z",
     "iopub.status.idle": "2024-07-08T16:58:51.287649Z",
     "shell.execute_reply": "2024-07-08T16:58:51.286781Z"
    },
    "papermill": {
     "duration": 0.161764,
     "end_time": "2024-07-08T16:58:51.289840",
     "exception": false,
     "start_time": "2024-07-08T16:58:51.128076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# num_return_sequences = 5\n",
    "# # this is not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99366ef9",
   "metadata": {
    "_cell_guid": "ba7816d1-4ccd-474c-ae38-5cb99ac6e7fc",
    "_uuid": "b44fbc42-eec7-44ab-917a-5d383e443954",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-08T16:58:51.600419Z",
     "iopub.status.busy": "2024-07-08T16:58:51.599740Z",
     "iopub.status.idle": "2024-07-08T16:58:51.604346Z",
     "shell.execute_reply": "2024-07-08T16:58:51.603630Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.162799,
     "end_time": "2024-07-08T16:58:51.606307",
     "exception": false,
     "start_time": "2024-07-08T16:58:51.443508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # generate! tokens is (B, T) where B = 5, T = 8\n",
    "# # set the seed to 42\n",
    "# torch.manual_seed(42)\n",
    "# # torch.cuda.manual_seed(42)\n",
    "# while x.size(1) < max_length:\n",
    "#     # forward the model to get the logits\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(x) # (B, T, vocab_size)\n",
    "#         # take the logits at the last position\n",
    "#         logits = logits[:, -1, :] # (B, vocab_size)\n",
    "#         # get the probabilities\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "#         # do top-k sampling of 50 (huggingface pipeline default)\n",
    "#         # topk_probs here become (5, 50), topk_indices is (5, 50)\n",
    "#         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "#         # select a token from the top-k probabilities\n",
    "#         ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "#         # gather the corresponding indices\n",
    "#         xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "#         # append to the sequence\n",
    "#         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# for i in range(num_return_sequences):\n",
    "#     tokens = x[i, :max_length].tolist()\n",
    "#     decoded = enc.decode(tokens)\n",
    "#     print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5354353,
     "sourceId": 8905566,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15198.850859,
   "end_time": "2024-07-08T16:58:52.111777",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-08T12:45:33.260918",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
