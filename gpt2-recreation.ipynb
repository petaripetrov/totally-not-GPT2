{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561322b1",
   "metadata": {
    "_cell_guid": "4c394a99-f5a3-4b23-95cd-477128ee993e",
    "_uuid": "1670d5c9-6e4f-477a-a5d1-356672775fbc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 140.088508,
     "end_time": "2024-07-10T10:43:56.408104",
     "exception": false,
     "start_time": "2024-07-10T10:41:36.319596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (4.66.4)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (2.19.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from tiktoken) (2024.5.10)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/__apptainer__/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m102.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken tqdm datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c9a7ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T10:43:56.565385Z",
     "iopub.status.busy": "2024-07-10T10:43:56.565068Z",
     "iopub.status.idle": "2024-07-10T10:43:59.969175Z",
     "shell.execute_reply": "2024-07-10T10:43:59.968077Z"
    },
    "papermill": {
     "duration": 3.486603,
     "end_time": "2024-07-10T10:43:59.971579",
     "exception": false,
     "start_time": "2024-07-10T10:43:56.484976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-10 10:43:57--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: 'input.txt'\r\n",
      "\r\n",
      "input.txt           100%[===================>]   1.06M  5.18MB/s    in 0.2s    \r\n",
      "\r\n",
      "2024-07-10 10:43:58 (5.18 MB/s) - 'input.txt' saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c73f190",
   "metadata": {
    "papermill": {
     "duration": 0.08701,
     "end_time": "2024-07-10T10:44:00.135299",
     "exception": false,
     "start_time": "2024-07-10T10:44:00.048289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fineweb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fineweb.py\n",
    "\n",
    "\"\"\"\n",
    "FineWeb-Edu dataset (for srs pretraining)\n",
    "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
    "Downloads and tokenizes the data and saves data shards to disk.\n",
    "Run simply as:\n",
    "$ python fineweb.py\n",
    "Will save shards to the local directory \"edu_fineweb10B\".\n",
    "\"\"\"\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------\n",
    "local_dir = \"/scratch/ppetrov1/edu_fineweb10B\"\n",
    "remote_name = \"sample-10BT\"\n",
    "shard_size = int(1e8) # 100M tokens per shard, total of 100 shards\n",
    "\n",
    "# create the cache if it doesn't exist yet\n",
    "DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), local_dir)\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# download the dataset\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n",
    "\n",
    "# init the tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
    "\n",
    "def tokenize(doc):\n",
    "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
    "    tokens = [eot]\n",
    "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "    return tokens_np_uint16\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "    \n",
    "# tokenize all documents and write output shards, each of shard_size tokens (last shard has remainder)\n",
    "max_shards = 50\n",
    "nprocs = max(1, os.cpu_count()//2)\n",
    "with mp.Pool(nprocs) as pool:\n",
    "    shard_index = 0\n",
    "    # preallocate buffer to hold current shard\n",
    "    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "    token_count = 0\n",
    "    progress_bar = None\n",
    "    for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
    "        # is there enough space in the current shard for the new tokens?\n",
    "        if token_count + len(tokens) < shard_size:\n",
    "            # simply append tokens to current shard\n",
    "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "            token_count += len(tokens)\n",
    "            # update progress bar\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "            progress_bar.update(len(tokens))\n",
    "        else:\n",
    "            # write the current shard and start a new one\n",
    "            split = \"val\" if shard_index == 0 else \"train\"\n",
    "            filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
    "            remainder = shard_size - token_count\n",
    "            progress_bar.update(remainder)\n",
    "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
    "            write_datafile(filename, all_tokens_np)\n",
    "            shard_index += 1\n",
    "            progress_bar = None\n",
    "            # populate the next shard with the leftovers of the current doc\n",
    "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
    "            token_count = len(tokens)-remainder\n",
    "\n",
    "    # write any remaining tokens as the last shard\n",
    "    if token_count != 0 and not shard_index > max_shards:\n",
    "        split = \"val\" if shard_index == 0 else \"train\"\n",
    "        filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "        write_datafile(filename, all_tokens_np[:token_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a74860",
   "metadata": {
    "papermill": {
     "duration": 0.084014,
     "end_time": "2024-07-10T10:44:00.335393",
     "exception": false,
     "start_time": "2024-07-10T10:44:00.251379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|███████████████████| 25.5k/25.5k [00:00<00:00, 117MB/s]\n",
      "Resolving data files: 100%|███████████████| 2110/2110 [00:01<00:00, 1248.71it/s]\n",
      "Downloading data:  32%|███████               | 692M/2.15G [04:31<32:41, 745kB/s]"
     ]
    }
   ],
   "source": [
    "!python fineweb.py # run this if you have not loaded the data, else just use the already loaded location TODO check if the data is already loaded or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fae0fb",
   "metadata": {
    "papermill": {
     "duration": 0.086294,
     "end_time": "2024-07-10T10:44:00.497137",
     "exception": false,
     "start_time": "2024-07-10T10:44:00.410843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_loader.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    \n",
    "    return ptt\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T, process_rank, num_processes, split, data_root=\"edu_fineweb10B\"):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.is_master = process_rank == 0\n",
    "        assert split in {'train', 'val'}, f\"Invalid split {split}, must be train or val\"\n",
    "        \n",
    "        # load tokens from disk and store them in memory\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        \n",
    "        assert len(shards) > 0, f\"no shards found for split: {split}\"\n",
    "        if self.is_master:\n",
    "            print(f\"found {len(shards)} shards for split: {split}\")\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds, advance to the next shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d6db5f",
   "metadata": {
    "papermill": {
     "duration": 0.090153,
     "end_time": "2024-07-10T10:44:00.662572",
     "exception": false,
     "start_time": "2024-07-10T10:44:00.572419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import inspect\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    path: str = \"gpt2\"\n",
    "    min_lr_factor: float = 0.1\n",
    "    warmup_steps: int = 27 # need to explain why\n",
    "    warmdown_steps: int = 214 # need to explain why\n",
    "    lr: float = 16e-4\n",
    "        \n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to the next batch\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T, T) matrix for all the queries and keys)\n",
    "        \n",
    "#         att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "#         att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "#         att = F.softmax(att, dim=-1)\n",
    "#         y = att @ v # (B , nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output project\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd) # generally, this should be initialized too but pytorch does it correctly\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig, process_rank: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.is_master = process_rank == 0\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        # calculate loss if targets are defined\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained GPT: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head, and n_embd are determined from medel_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),    # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),   # 350M params \n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),   # 774M params \n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),   # 1558M params \n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257   # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024    # always 1024 for GPT model checkpoints\n",
    "        \n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard mask / buffer\n",
    "\n",
    "        # init a higgingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # also ignore\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically, the OpenAI checkpoints use a \"Conv1D\" module, but we only want to use a vanila\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_lr(self, it, max_steps): ## TODO maybe move\n",
    "        # 1) linear warmup for warmup_steps steps\n",
    "        if it < self.config.warmup_steps:\n",
    "            return self.config.lr * (it + 1) / self.config.warmup_steps\n",
    "        # 2) constant lr for a while\n",
    "        elif it < max_steps - self.config.warmdown_steps:\n",
    "            return self.config.lr\n",
    "        # 3) linear warmdown\n",
    "        else:\n",
    "            decay_ratio = (max_steps - it) / self.config.warmdown_steps\n",
    "#         assert 0 <= decay_ratio <= 1\n",
    "#             coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "            return self.config.lr * decay_ratio\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all of the candidate parameters (that require gradients)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameter that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. weight tensors in matmuls + embeddings decay, all biases and layernorms don't\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=False) # fused is slower on kaggle for some reason TODO figure out why\n",
    "        if self.is_master:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters in total\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters in total\")\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ef2744",
   "metadata": {
    "_cell_guid": "35edd86f-4e81-4576-a17b-187e0c29f353",
    "_uuid": "26d76cd9-11aa-4fc3-94bb-724b5d0da166",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.087777,
     "end_time": "2024-07-10T10:44:00.825603",
     "exception": false,
     "start_time": "2024-07-10T10:44:00.737826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from data_loader import DataLoader\n",
    "from model import GPT, GPTConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ___________________________________Set up DDP_____________________________________\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), \"for now we need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla process\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process= True\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "\n",
    "print(f\"using device: {device}\")\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "    \n",
    "total_batch_size = 524288 # we see (roughly) half a shard per batch\n",
    "B = 8\n",
    "T = 1024\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f\"total desired batch size: {total_batch_size}\")\n",
    "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "train_loader = DataLoader(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\", data_root=\"/kaggle/input/finewebedu-5b\")\n",
    "val_loader = DataLoader(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\", data_root=\"/kaggle/input/finewebedu-5b\")\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig(warmup_steps = 715), ddp_rank)\n",
    "model.to(device)\n",
    "model = torch.compile(model) # TODO explain this better ?\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    \n",
    "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "# max_steps = 9536 # We only load half of Fineweb-EDU due to drive constraints\n",
    "max_steps = 1000 # defo not enough but cant do more due to GPU compute constraints\n",
    "\n",
    "# optimize! the model\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), eps=1e-8) # TODO read about AdamW\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=18e-4, device=device)\n",
    "\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
    "    pass\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "val_step = 25\n",
    "\n",
    "for step in range(max_steps):\n",
    "    start = time.time()\n",
    "    last_step = (step == max_steps -1)\n",
    "    \n",
    "    if step % val_step == 0 or last_step: #abstract this into a property on the model config or other constant\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            \n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                with torch.autocast(device_type=device_type, dtype=torch.float16):\n",
    "                    logits, loss = model(x, y)\n",
    "                \n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "                \n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "                if step > 0 and (step % 500 == 0 or last_step):\n",
    "                    # optionally write model checkpoints\n",
    "                    checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
    "                    checkpoint = {\n",
    "                        'model': raw_model.state_dict(),\n",
    "                        'config': raw_model.config,\n",
    "                        'step': step,\n",
    "                        'val_loss': val_loss_accum.item()\n",
    "                    }\n",
    "                    # you might also want to add optimizer.state_dict() and\n",
    "                    # rng seeds etc., if you want to resume training\n",
    "                    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    \n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1) # switch to the DDP context manager\n",
    "            \n",
    "        with torch.autocast(device_type=device_type, dtype=torch.float16): \n",
    "            # TODO look into how to get a gradient scaler working\n",
    "            logits, loss = model(x, y)\n",
    "        \n",
    "        # we have to scale the loss to account for gradient accumulation\n",
    "        # because the gradients just add on each successive backward()\n",
    "        # addition of gradients corresponds to a SUM in the objective, but\n",
    "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    \n",
    "    scaler.unscale_(optimizer)\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = model.module.get_lr(step, max_steps) if ddp else model.get_lr(step, max_steps) # try out the scheduler provided by pytorch (if there are any)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    miliseconds = total_time*1000\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / total_time\n",
    "    \n",
    "    if master_process:\n",
    "        log_str = f\"step: {step:5d} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | time: {miliseconds:.2f}ms | tok/sec: {tokens_per_sec:.2f}\"\n",
    "        print(log_str)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{log_str}\\n\")\n",
    "    \n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ba50e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T10:44:00.979488Z",
     "iopub.status.busy": "2024-07-10T10:44:00.978876Z",
     "iopub.status.idle": "2024-07-10T15:05:25.021971Z",
     "shell.execute_reply": "2024-07-10T15:05:25.020823Z"
    },
    "papermill": {
     "duration": 15684.122317,
     "end_time": "2024-07-10T15:05:25.024416",
     "exception": false,
     "start_time": "2024-07-10T10:44:00.902099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0710 10:44:03.806000 134547159340864 torch/distributed/run.py:757] \r\n",
      "W0710 10:44:03.806000 134547159340864 torch/distributed/run.py:757] *****************************************\r\n",
      "W0710 10:44:03.806000 134547159340864 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0710 10:44:03.806000 134547159340864 torch/distributed/run.py:757] *****************************************\r\n",
      "using device: cuda:0\r\n",
      "total desired batch size: 524288\r\n",
      "=> calculated gradient accumulation steps: 32\r\n",
      "found 50 shards for split: train\r\n",
      "using device: cuda:1\r\n",
      "found 1 shards for split: val\r\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters in total\r\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters in total\r\n",
      "using fused AdamW: True\r\n",
      "validation loss: 10.9499\r\n",
      "step:     0 | loss: 10.954966 | lr: 2.2378e-06 | norm: 15.3500 | time: 91298.31ms | tok/sec: 5742.58\r\n",
      "step:     1 | loss: 10.821445 | lr: 4.4755e-06 | norm: 14.3172 | time: 14465.13ms | tok/sec: 36244.96\r\n",
      "step:     2 | loss: 10.592018 | lr: 6.7133e-06 | norm: 11.5002 | time: 14927.85ms | tok/sec: 35121.47\r\n",
      "step:     3 | loss: 10.355320 | lr: 8.9510e-06 | norm: 8.5944 | time: 15474.74ms | tok/sec: 33880.25\r\n",
      "step:     4 | loss: 10.171724 | lr: 1.1189e-05 | norm: 6.8194 | time: 16238.92ms | tok/sec: 32285.90\r\n",
      "step:     5 | loss: 10.006744 | lr: 1.3427e-05 | norm: 5.2748 | time: 15781.32ms | tok/sec: 33222.05\r\n",
      "step:     6 | loss: 9.887335 | lr: 1.5664e-05 | norm: 4.0270 | time: 15128.30ms | tok/sec: 34656.10\r\n",
      "step:     7 | loss: 9.800512 | lr: 1.7902e-05 | norm: 3.2570 | time: 15008.79ms | tok/sec: 34932.06\r\n",
      "step:     8 | loss: 9.720143 | lr: 2.0140e-05 | norm: 2.8654 | time: 15301.04ms | tok/sec: 34264.87\r\n",
      "step:     9 | loss: 9.682137 | lr: 2.2378e-05 | norm: 2.5891 | time: 15549.99ms | tok/sec: 33716.30\r\n",
      "step:    10 | loss: 9.632119 | lr: 2.4615e-05 | norm: 2.3799 | time: 15525.72ms | tok/sec: 33768.99\r\n",
      "step:    11 | loss: 9.619648 | lr: 2.6853e-05 | norm: 2.3181 | time: 15270.79ms | tok/sec: 34332.73\r\n",
      "step:    12 | loss: 9.597092 | lr: 2.9091e-05 | norm: 2.2529 | time: 15137.75ms | tok/sec: 34634.48\r\n",
      "step:    13 | loss: 9.574646 | lr: 3.1329e-05 | norm: 2.1985 | time: 15187.52ms | tok/sec: 34520.97\r\n",
      "step:    14 | loss: 9.513899 | lr: 3.3566e-05 | norm: 2.2388 | time: 15219.87ms | tok/sec: 34447.59\r\n",
      "step:    15 | loss: 9.488281 | lr: 3.5804e-05 | norm: 2.2544 | time: 15272.68ms | tok/sec: 34328.49\r\n",
      "step:    16 | loss: 9.455544 | lr: 3.8042e-05 | norm: 2.1739 | time: 15295.56ms | tok/sec: 34277.14\r\n",
      "step:    17 | loss: 9.403190 | lr: 4.0280e-05 | norm: 2.1293 | time: 15310.69ms | tok/sec: 34243.27\r\n",
      "step:    18 | loss: 9.442432 | lr: 4.2517e-05 | norm: 2.0292 | time: 15326.86ms | tok/sec: 34207.13\r\n",
      "step:    19 | loss: 9.418201 | lr: 4.4755e-05 | norm: 2.1819 | time: 15286.72ms | tok/sec: 34296.96\r\n",
      "step:    20 | loss: 9.272829 | lr: 4.6993e-05 | norm: 1.9996 | time: 15270.93ms | tok/sec: 34332.42\r\n",
      "step:    21 | loss: 9.212389 | lr: 4.9231e-05 | norm: 2.3209 | time: 15288.54ms | tok/sec: 34292.88\r\n",
      "step:    22 | loss: 9.165787 | lr: 5.1469e-05 | norm: 1.9025 | time: 15258.95ms | tok/sec: 34359.37\r\n",
      "step:    23 | loss: 9.150101 | lr: 5.3706e-05 | norm: 2.9269 | time: 15291.52ms | tok/sec: 34286.20\r\n",
      "step:    24 | loss: 9.044718 | lr: 5.5944e-05 | norm: 2.0142 | time: 15270.18ms | tok/sec: 34334.10\r\n",
      "validation loss: 9.0096\r\n",
      "step:    25 | loss: 8.987564 | lr: 5.8182e-05 | norm: 2.1491 | time: 19239.45ms | tok/sec: 27250.68\r\n",
      "step:    26 | loss: 8.948952 | lr: 6.0420e-05 | norm: 2.4022 | time: 15299.42ms | tok/sec: 34268.48\r\n",
      "step:    27 | loss: 8.860617 | lr: 6.2657e-05 | norm: 1.9635 | time: 15328.31ms | tok/sec: 34203.90\r\n",
      "step:    28 | loss: 8.801065 | lr: 6.4895e-05 | norm: 2.3981 | time: 15394.05ms | tok/sec: 34057.83\r\n",
      "step:    29 | loss: 8.704774 | lr: 6.7133e-05 | norm: 1.9893 | time: 15360.64ms | tok/sec: 34131.91\r\n",
      "step:    30 | loss: 8.729485 | lr: 6.9371e-05 | norm: 1.9268 | time: 15413.04ms | tok/sec: 34015.88\r\n",
      "step:    31 | loss: 8.624836 | lr: 7.1608e-05 | norm: 1.8276 | time: 15360.00ms | tok/sec: 34133.33\r\n",
      "step:    32 | loss: 8.579075 | lr: 7.3846e-05 | norm: 1.7642 | time: 15350.04ms | tok/sec: 34155.48\r\n",
      "step:    33 | loss: 8.498648 | lr: 7.6084e-05 | norm: 1.6517 | time: 15298.00ms | tok/sec: 34271.66\r\n",
      "step:    34 | loss: 8.373590 | lr: 7.8322e-05 | norm: 1.7728 | time: 15318.25ms | tok/sec: 34226.37\r\n",
      "step:    35 | loss: 8.366516 | lr: 8.0559e-05 | norm: 1.6175 | time: 15294.30ms | tok/sec: 34279.96\r\n",
      "step:    36 | loss: 8.313226 | lr: 8.2797e-05 | norm: 1.4642 | time: 15301.49ms | tok/sec: 34263.85\r\n",
      "step:    37 | loss: 8.223944 | lr: 8.5035e-05 | norm: 1.6964 | time: 15276.55ms | tok/sec: 34319.80\r\n",
      "step:    38 | loss: 8.177919 | lr: 8.7273e-05 | norm: 1.5869 | time: 15324.32ms | tok/sec: 34212.80\r\n",
      "step:    39 | loss: 8.150286 | lr: 8.9510e-05 | norm: 1.4290 | time: 15290.10ms | tok/sec: 34289.38\r\n",
      "step:    40 | loss: 8.095095 | lr: 9.1748e-05 | norm: 1.3104 | time: 15289.78ms | tok/sec: 34290.11\r\n",
      "step:    41 | loss: 8.001014 | lr: 9.3986e-05 | norm: 1.3926 | time: 15287.13ms | tok/sec: 34296.04\r\n",
      "step:    42 | loss: 7.929433 | lr: 9.6224e-05 | norm: 1.5042 | time: 15298.28ms | tok/sec: 34271.05\r\n",
      "step:    43 | loss: 7.938405 | lr: 9.8462e-05 | norm: 1.2986 | time: 15297.27ms | tok/sec: 34273.31\r\n",
      "step:    44 | loss: 7.859172 | lr: 1.0070e-04 | norm: 1.1929 | time: 15289.85ms | tok/sec: 34289.95\r\n",
      "step:    45 | loss: 7.834564 | lr: 1.0294e-04 | norm: 1.2473 | time: 15310.63ms | tok/sec: 34243.40\r\n",
      "step:    46 | loss: 7.770865 | lr: 1.0517e-04 | norm: 1.3067 | time: 15267.03ms | tok/sec: 34341.20\r\n",
      "step:    47 | loss: 7.779616 | lr: 1.0741e-04 | norm: 1.1916 | time: 15280.70ms | tok/sec: 34310.47\r\n",
      "step:    48 | loss: 7.765447 | lr: 1.0965e-04 | norm: 1.3744 | time: 15288.65ms | tok/sec: 34292.63\r\n",
      "step:    49 | loss: 7.666269 | lr: 1.1189e-04 | norm: 1.1385 | time: 15285.52ms | tok/sec: 34299.65\r\n",
      "validation loss: 7.6603\r\n",
      "step:    50 | loss: 7.682365 | lr: 1.1413e-04 | norm: 0.9215 | time: 18926.28ms | tok/sec: 27701.58\r\n",
      "step:    51 | loss: 7.622855 | lr: 1.1636e-04 | norm: 1.2605 | time: 15270.69ms | tok/sec: 34332.95\r\n",
      "step:    52 | loss: 7.588214 | lr: 1.1860e-04 | norm: 1.0575 | time: 15312.66ms | tok/sec: 34238.86\r\n",
      "step:    53 | loss: 7.511761 | lr: 1.2084e-04 | norm: 0.8755 | time: 15308.57ms | tok/sec: 34248.00\r\n",
      "step:    54 | loss: 7.522687 | lr: 1.2308e-04 | norm: 0.9921 | time: 15312.47ms | tok/sec: 34239.29\r\n",
      "step:    55 | loss: 7.499205 | lr: 1.2531e-04 | norm: 1.0958 | time: 15319.37ms | tok/sec: 34223.86\r\n",
      "step:    56 | loss: 7.452225 | lr: 1.2755e-04 | norm: 1.3701 | time: 15318.09ms | tok/sec: 34226.73\r\n",
      "step:    57 | loss: 7.429919 | lr: 1.2979e-04 | norm: 0.8381 | time: 15328.06ms | tok/sec: 34204.47\r\n",
      "step:    58 | loss: 7.358557 | lr: 1.3203e-04 | norm: 1.6239 | time: 15346.31ms | tok/sec: 34163.79\r\n",
      "step:    59 | loss: 7.311312 | lr: 1.3427e-04 | norm: 0.7828 | time: 15331.31ms | tok/sec: 34197.20\r\n",
      "step:    60 | loss: 7.334067 | lr: 1.3650e-04 | norm: 1.6633 | time: 15344.55ms | tok/sec: 34167.70\r\n",
      "step:    61 | loss: 7.340926 | lr: 1.3874e-04 | norm: 0.9783 | time: 15350.04ms | tok/sec: 34155.47\r\n",
      "step:    62 | loss: 7.227194 | lr: 1.4098e-04 | norm: 0.6879 | time: 15365.30ms | tok/sec: 34121.56\r\n",
      "step:    63 | loss: 7.216878 | lr: 1.4322e-04 | norm: 0.9883 | time: 15378.55ms | tok/sec: 34092.17\r\n",
      "step:    64 | loss: 7.189033 | lr: 1.4545e-04 | norm: 0.8980 | time: 15342.55ms | tok/sec: 34172.16\r\n",
      "step:    65 | loss: 7.225956 | lr: 1.4769e-04 | norm: 1.4082 | time: 15318.63ms | tok/sec: 34225.50\r\n",
      "step:    66 | loss: 7.158448 | lr: 1.4993e-04 | norm: 1.5312 | time: 15380.02ms | tok/sec: 34088.89\r\n",
      "step:    67 | loss: 7.169928 | lr: 1.5217e-04 | norm: 0.8208 | time: 15374.44ms | tok/sec: 34101.27\r\n",
      "step:    68 | loss: 7.148207 | lr: 1.5441e-04 | norm: 0.8259 | time: 15334.67ms | tok/sec: 34189.71\r\n",
      "step:    69 | loss: 7.057511 | lr: 1.5664e-04 | norm: 1.5971 | time: 15364.93ms | tok/sec: 34122.38\r\n",
      "step:    70 | loss: 7.030880 | lr: 1.5888e-04 | norm: 0.7180 | time: 15422.81ms | tok/sec: 33994.32\r\n",
      "step:    71 | loss: 7.065842 | lr: 1.6112e-04 | norm: 1.0104 | time: 15388.88ms | tok/sec: 34069.27\r\n",
      "step:    72 | loss: 7.014464 | lr: 1.6336e-04 | norm: 0.8665 | time: 15372.69ms | tok/sec: 34105.15\r\n",
      "step:    73 | loss: 7.002929 | lr: 1.6559e-04 | norm: 0.9342 | time: 15379.40ms | tok/sec: 34090.27\r\n",
      "step:    74 | loss: 6.927721 | lr: 1.6783e-04 | norm: 0.6880 | time: 15406.23ms | tok/sec: 34030.90\r\n",
      "validation loss: 7.0402\r\n",
      "step:    75 | loss: 6.976568 | lr: 1.7007e-04 | norm: 0.9224 | time: 19062.03ms | tok/sec: 27504.31\r\n",
      "step:    76 | loss: 6.916138 | lr: 1.7231e-04 | norm: 0.9782 | time: 15364.05ms | tok/sec: 34124.33\r\n",
      "step:    77 | loss: 6.952504 | lr: 1.7455e-04 | norm: 1.1637 | time: 15405.95ms | tok/sec: 34031.53\r\n",
      "step:    78 | loss: 6.916005 | lr: 1.7678e-04 | norm: 0.8383 | time: 15480.99ms | tok/sec: 33866.56\r\n",
      "step:    79 | loss: 6.847523 | lr: 1.7902e-04 | norm: 0.6829 | time: 15428.56ms | tok/sec: 33981.65\r\n",
      "step:    80 | loss: 6.850473 | lr: 1.8126e-04 | norm: 0.6559 | time: 15386.49ms | tok/sec: 34074.57\r\n",
      "step:    81 | loss: 6.858785 | lr: 1.8350e-04 | norm: 0.7944 | time: 15396.23ms | tok/sec: 34053.00\r\n",
      "step:    82 | loss: 6.790815 | lr: 1.8573e-04 | norm: 0.7048 | time: 15401.98ms | tok/sec: 34040.31\r\n",
      "step:    83 | loss: 6.744594 | lr: 1.8797e-04 | norm: 0.5665 | time: 15356.03ms | tok/sec: 34142.16\r\n",
      "step:    84 | loss: 6.753985 | lr: 1.9021e-04 | norm: 0.5749 | time: 15355.76ms | tok/sec: 34142.75\r\n",
      "step:    85 | loss: 6.714733 | lr: 1.9245e-04 | norm: 0.6404 | time: 15338.39ms | tok/sec: 34181.41\r\n",
      "step:    86 | loss: 6.703959 | lr: 1.9469e-04 | norm: 0.8512 | time: 15358.55ms | tok/sec: 34136.56\r\n",
      "step:    87 | loss: 6.734374 | lr: 1.9692e-04 | norm: 1.3662 | time: 15391.67ms | tok/sec: 34063.10\r\n",
      "step:    88 | loss: 6.710710 | lr: 1.9916e-04 | norm: 1.0139 | time: 15350.86ms | tok/sec: 34153.65\r\n",
      "step:    89 | loss: 6.748012 | lr: 2.0140e-04 | norm: 0.5850 | time: 15374.96ms | tok/sec: 34100.13\r\n",
      "step:    90 | loss: 6.670510 | lr: 2.0364e-04 | norm: 0.8573 | time: 15339.57ms | tok/sec: 34178.79\r\n",
      "step:    91 | loss: 6.668267 | lr: 2.0587e-04 | norm: 1.0338 | time: 15358.84ms | tok/sec: 34135.91\r\n",
      "step:    92 | loss: 6.712007 | lr: 2.0811e-04 | norm: 0.8889 | time: 15403.71ms | tok/sec: 34036.48\r\n",
      "step:    93 | loss: 6.782683 | lr: 2.1035e-04 | norm: 0.7262 | time: 15372.03ms | tok/sec: 34106.63\r\n",
      "step:    94 | loss: 6.792949 | lr: 2.1259e-04 | norm: 0.7367 | time: 15383.28ms | tok/sec: 34081.68\r\n",
      "step:    95 | loss: 6.784729 | lr: 2.1483e-04 | norm: 0.8597 | time: 15385.96ms | tok/sec: 34075.75\r\n",
      "step:    96 | loss: 6.734319 | lr: 2.1706e-04 | norm: 0.8979 | time: 15365.94ms | tok/sec: 34120.13\r\n",
      "step:    97 | loss: 6.688519 | lr: 2.1930e-04 | norm: 0.8232 | time: 15387.15ms | tok/sec: 34073.10\r\n",
      "step:    98 | loss: 6.735121 | lr: 2.2154e-04 | norm: 0.8236 | time: 15386.58ms | tok/sec: 34074.37\r\n",
      "step:    99 | loss: 6.690429 | lr: 2.2378e-04 | norm: 1.1037 | time: 15369.15ms | tok/sec: 34113.01\r\n",
      "validation loss: 6.6843\r\n",
      "step:   100 | loss: 6.736626 | lr: 2.2601e-04 | norm: 1.0252 | time: 19084.98ms | tok/sec: 27471.23\r\n",
      "step:   101 | loss: 6.662806 | lr: 2.2825e-04 | norm: 1.0947 | time: 15389.10ms | tok/sec: 34068.80\r\n",
      "step:   102 | loss: 6.636179 | lr: 2.3049e-04 | norm: 0.5767 | time: 15394.40ms | tok/sec: 34057.06\r\n",
      "step:   103 | loss: 6.529576 | lr: 2.3273e-04 | norm: 0.6423 | time: 15413.01ms | tok/sec: 34015.94\r\n",
      "step:   104 | loss: 6.630649 | lr: 2.3497e-04 | norm: 0.8085 | time: 15430.17ms | tok/sec: 33978.11\r\n",
      "step:   105 | loss: 6.636714 | lr: 2.3720e-04 | norm: 1.0077 | time: 15469.66ms | tok/sec: 33891.36\r\n",
      "step:   106 | loss: 6.604315 | lr: 2.3944e-04 | norm: 0.9792 | time: 15490.09ms | tok/sec: 33846.67\r\n",
      "step:   107 | loss: 6.537755 | lr: 2.4168e-04 | norm: 0.9857 | time: 15451.69ms | tok/sec: 33930.79\r\n",
      "step:   108 | loss: 6.607997 | lr: 2.4392e-04 | norm: 1.0116 | time: 15484.74ms | tok/sec: 33858.37\r\n",
      "step:   109 | loss: 6.608181 | lr: 2.4615e-04 | norm: 0.8544 | time: 15445.54ms | tok/sec: 33944.29\r\n",
      "step:   110 | loss: 6.555634 | lr: 2.4839e-04 | norm: 0.7725 | time: 15447.31ms | tok/sec: 33940.40\r\n",
      "step:   111 | loss: 6.525391 | lr: 2.5063e-04 | norm: 0.6318 | time: 15404.86ms | tok/sec: 34033.94\r\n",
      "step:   112 | loss: 6.498113 | lr: 2.5287e-04 | norm: 0.5452 | time: 15479.87ms | tok/sec: 33869.01\r\n",
      "step:   113 | loss: 6.546605 | lr: 2.5510e-04 | norm: 0.5868 | time: 15490.07ms | tok/sec: 33846.72\r\n",
      "step:   114 | loss: 6.554756 | lr: 2.5734e-04 | norm: 0.4799 | time: 15430.65ms | tok/sec: 33977.06\r\n",
      "step:   115 | loss: 6.521167 | lr: 2.5958e-04 | norm: 0.6375 | time: 15461.82ms | tok/sec: 33908.57\r\n",
      "step:   116 | loss: 6.455637 | lr: 2.6182e-04 | norm: 0.9315 | time: 15400.52ms | tok/sec: 34043.53\r\n",
      "step:   117 | loss: 6.399235 | lr: 2.6406e-04 | norm: 1.0819 | time: 15440.20ms | tok/sec: 33956.03\r\n",
      "step:   118 | loss: 6.412235 | lr: 2.6629e-04 | norm: 0.8242 | time: 15417.41ms | tok/sec: 34006.23\r\n",
      "step:   119 | loss: 6.449444 | lr: 2.6853e-04 | norm: 0.8012 | time: 15451.01ms | tok/sec: 33932.29\r\n",
      "step:   120 | loss: 6.422849 | lr: 2.7077e-04 | norm: 0.9049 | time: 15435.56ms | tok/sec: 33966.25\r\n",
      "step:   121 | loss: 6.392381 | lr: 2.7301e-04 | norm: 0.9097 | time: 15439.77ms | tok/sec: 33956.99\r\n",
      "step:   122 | loss: 6.395582 | lr: 2.7524e-04 | norm: 0.7856 | time: 15449.39ms | tok/sec: 33935.84\r\n",
      "step:   123 | loss: 6.387397 | lr: 2.7748e-04 | norm: 0.5397 | time: 15429.39ms | tok/sec: 33979.83\r\n",
      "step:   124 | loss: 6.398643 | lr: 2.7972e-04 | norm: 0.4714 | time: 15478.59ms | tok/sec: 33871.82\r\n",
      "validation loss: 6.4592\r\n",
      "step:   125 | loss: 6.393627 | lr: 2.8196e-04 | norm: 0.6399 | time: 19108.29ms | tok/sec: 27437.72\r\n",
      "step:   126 | loss: 6.375420 | lr: 2.8420e-04 | norm: 0.5545 | time: 15468.70ms | tok/sec: 33893.48\r\n",
      "step:   127 | loss: 6.343986 | lr: 2.8643e-04 | norm: 0.6637 | time: 15463.65ms | tok/sec: 33904.54\r\n",
      "step:   128 | loss: 6.299598 | lr: 2.8867e-04 | norm: 0.5153 | time: 15428.75ms | tok/sec: 33981.24\r\n",
      "step:   129 | loss: 6.285398 | lr: 2.9091e-04 | norm: 0.5631 | time: 15417.15ms | tok/sec: 34006.81\r\n",
      "step:   130 | loss: 6.350619 | lr: 2.9315e-04 | norm: 0.9019 | time: 15434.95ms | tok/sec: 33967.58\r\n",
      "step:   131 | loss: 6.333402 | lr: 2.9538e-04 | norm: 1.5233 | time: 15409.69ms | tok/sec: 34023.27\r\n",
      "step:   132 | loss: 6.330308 | lr: 2.9762e-04 | norm: 0.9158 | time: 15425.36ms | tok/sec: 33988.69\r\n",
      "step:   133 | loss: 6.242301 | lr: 2.9986e-04 | norm: 0.7893 | time: 15413.53ms | tok/sec: 34014.80\r\n",
      "step:   134 | loss: 6.234945 | lr: 3.0210e-04 | norm: 0.6866 | time: 15403.28ms | tok/sec: 34037.42\r\n",
      "step:   135 | loss: 6.300432 | lr: 3.0434e-04 | norm: 0.6135 | time: 15428.00ms | tok/sec: 33982.90\r\n",
      "step:   136 | loss: 6.270608 | lr: 3.0657e-04 | norm: 0.6461 | time: 15408.59ms | tok/sec: 34025.70\r\n",
      "step:   137 | loss: 6.297298 | lr: 3.0881e-04 | norm: 0.6297 | time: 15385.87ms | tok/sec: 34075.94\r\n",
      "step:   138 | loss: 6.230983 | lr: 3.1105e-04 | norm: 0.5224 | time: 15387.77ms | tok/sec: 34071.73\r\n",
      "step:   139 | loss: 6.333153 | lr: 3.1329e-04 | norm: 0.6098 | time: 15412.76ms | tok/sec: 34016.49\r\n",
      "step:   140 | loss: 6.450420 | lr: 3.1552e-04 | norm: 0.7455 | time: 15344.11ms | tok/sec: 34168.68\r\n",
      "step:   141 | loss: 6.339264 | lr: 3.1776e-04 | norm: 0.7050 | time: 15345.40ms | tok/sec: 34165.80\r\n",
      "step:   142 | loss: 6.391111 | lr: 3.2000e-04 | norm: 0.6748 | time: 15419.84ms | tok/sec: 34000.88\r\n",
      "step:   143 | loss: 6.334224 | lr: 3.2224e-04 | norm: 0.8229 | time: 15439.32ms | tok/sec: 33957.97\r\n",
      "step:   144 | loss: 6.353255 | lr: 3.2448e-04 | norm: 1.4750 | time: 15444.62ms | tok/sec: 33946.31\r\n",
      "step:   145 | loss: 6.335023 | lr: 3.2671e-04 | norm: 1.0018 | time: 15469.71ms | tok/sec: 33891.27\r\n",
      "step:   146 | loss: 6.352239 | lr: 3.2895e-04 | norm: 0.9890 | time: 15430.37ms | tok/sec: 33977.67\r\n",
      "step:   147 | loss: 6.361555 | lr: 3.3119e-04 | norm: 1.0490 | time: 15482.11ms | tok/sec: 33864.12\r\n",
      "step:   148 | loss: 6.352885 | lr: 3.3343e-04 | norm: 0.7865 | time: 15471.36ms | tok/sec: 33887.64\r\n",
      "step:   149 | loss: 6.323175 | lr: 3.3566e-04 | norm: 0.5936 | time: 15505.16ms | tok/sec: 33813.77\r\n",
      "validation loss: 6.3036\r\n",
      "step:   150 | loss: 6.322080 | lr: 3.3790e-04 | norm: 0.5788 | time: 19166.83ms | tok/sec: 27353.92\r\n",
      "step:   151 | loss: 6.235662 | lr: 3.4014e-04 | norm: 0.5872 | time: 15486.58ms | tok/sec: 33854.34\r\n",
      "step:   152 | loss: 6.262896 | lr: 3.4238e-04 | norm: 0.4776 | time: 15546.11ms | tok/sec: 33724.71\r\n",
      "step:   153 | loss: 6.319800 | lr: 3.4462e-04 | norm: 0.5334 | time: 15593.20ms | tok/sec: 33622.87\r\n",
      "step:   154 | loss: 6.228564 | lr: 3.4685e-04 | norm: 0.6925 | time: 15493.25ms | tok/sec: 33839.77\r\n",
      "step:   155 | loss: 6.209989 | lr: 3.4909e-04 | norm: 0.9797 | time: 15471.54ms | tok/sec: 33887.25\r\n",
      "step:   156 | loss: 6.264650 | lr: 3.5133e-04 | norm: 0.8703 | time: 15538.27ms | tok/sec: 33741.73\r\n",
      "step:   157 | loss: 6.317049 | lr: 3.5357e-04 | norm: 0.6479 | time: 15498.73ms | tok/sec: 33827.79\r\n",
      "step:   158 | loss: 6.311511 | lr: 3.5580e-04 | norm: 0.6154 | time: 15475.88ms | tok/sec: 33877.75\r\n",
      "step:   159 | loss: 6.228060 | lr: 3.5804e-04 | norm: 0.5833 | time: 15474.91ms | tok/sec: 33879.88\r\n",
      "step:   160 | loss: 6.166121 | lr: 3.6028e-04 | norm: 0.5634 | time: 15437.61ms | tok/sec: 33961.74\r\n",
      "step:   161 | loss: 6.259790 | lr: 3.6252e-04 | norm: 0.6226 | time: 15506.38ms | tok/sec: 33811.11\r\n",
      "step:   162 | loss: 6.155051 | lr: 3.6476e-04 | norm: 0.5606 | time: 15488.95ms | tok/sec: 33849.17\r\n",
      "step:   163 | loss: 6.122009 | lr: 3.6699e-04 | norm: 0.5823 | time: 15435.28ms | tok/sec: 33966.85\r\n",
      "step:   164 | loss: 6.135003 | lr: 3.6923e-04 | norm: 0.5067 | time: 15399.88ms | tok/sec: 34044.94\r\n",
      "step:   165 | loss: 6.177302 | lr: 3.7147e-04 | norm: 0.5783 | time: 15433.22ms | tok/sec: 33971.40\r\n",
      "step:   166 | loss: 6.031850 | lr: 3.7371e-04 | norm: 0.6548 | time: 15388.43ms | tok/sec: 34070.26\r\n",
      "step:   167 | loss: 6.136191 | lr: 3.7594e-04 | norm: 0.7335 | time: 15427.78ms | tok/sec: 33983.38\r\n",
      "step:   168 | loss: 6.127883 | lr: 3.7818e-04 | norm: 1.2012 | time: 15439.73ms | tok/sec: 33957.08\r\n",
      "step:   169 | loss: 6.124177 | lr: 3.8042e-04 | norm: 0.7153 | time: 15438.21ms | tok/sec: 33960.41\r\n",
      "step:   170 | loss: 6.117163 | lr: 3.8266e-04 | norm: 0.9936 | time: 15484.26ms | tok/sec: 33859.43\r\n",
      "step:   171 | loss: 6.165134 | lr: 3.8490e-04 | norm: 1.2812 | time: 15431.24ms | tok/sec: 33975.74\r\n",
      "step:   172 | loss: 6.110216 | lr: 3.8713e-04 | norm: 0.6625 | time: 15417.54ms | tok/sec: 34005.93\r\n",
      "step:   173 | loss: 6.089160 | lr: 3.8937e-04 | norm: 0.8320 | time: 15444.30ms | tok/sec: 33947.01\r\n",
      "step:   174 | loss: 6.052043 | lr: 3.9161e-04 | norm: 0.8683 | time: 15390.80ms | tok/sec: 34065.03\r\n",
      "validation loss: 6.1791\r\n",
      "step:   175 | loss: 6.099871 | lr: 3.9385e-04 | norm: 0.7381 | time: 19082.45ms | tok/sec: 27474.87\r\n",
      "step:   176 | loss: 6.059165 | lr: 3.9608e-04 | norm: 0.8590 | time: 15408.04ms | tok/sec: 34026.92\r\n",
      "step:   177 | loss: 6.087731 | lr: 3.9832e-04 | norm: 0.6714 | time: 15406.55ms | tok/sec: 34030.19\r\n",
      "step:   178 | loss: 6.015758 | lr: 4.0056e-04 | norm: 0.7506 | time: 15429.13ms | tok/sec: 33980.39\r\n",
      "step:   179 | loss: 6.002115 | lr: 4.0280e-04 | norm: 0.7319 | time: 15467.16ms | tok/sec: 33896.85\r\n",
      "step:   180 | loss: 6.001970 | lr: 4.0503e-04 | norm: 0.6782 | time: 15457.45ms | tok/sec: 33918.14\r\n",
      "step:   181 | loss: 6.017209 | lr: 4.0727e-04 | norm: 0.5241 | time: 15427.95ms | tok/sec: 33982.99\r\n",
      "step:   182 | loss: 6.011664 | lr: 4.0951e-04 | norm: 0.5112 | time: 15472.42ms | tok/sec: 33885.33\r\n",
      "step:   183 | loss: 5.998786 | lr: 4.1175e-04 | norm: 0.4405 | time: 15455.10ms | tok/sec: 33923.30\r\n",
      "step:   184 | loss: 6.027205 | lr: 4.1399e-04 | norm: 0.4762 | time: 15470.41ms | tok/sec: 33889.73\r\n",
      "step:   185 | loss: 6.079761 | lr: 4.1622e-04 | norm: 0.4458 | time: 15474.12ms | tok/sec: 33881.61\r\n",
      "step:   186 | loss: 6.101978 | lr: 4.1846e-04 | norm: 0.4995 | time: 15459.26ms | tok/sec: 33914.18\r\n",
      "step:   187 | loss: 6.158234 | lr: 4.2070e-04 | norm: 0.5563 | time: 15440.67ms | tok/sec: 33955.01\r\n",
      "step:   188 | loss: 6.117973 | lr: 4.2294e-04 | norm: 0.7510 | time: 15430.59ms | tok/sec: 33977.19\r\n",
      "step:   189 | loss: 6.111469 | lr: 4.2517e-04 | norm: 1.0370 | time: 15409.59ms | tok/sec: 34023.48\r\n",
      "step:   190 | loss: 6.150020 | lr: 4.2741e-04 | norm: 0.8396 | time: 18794.93ms | tok/sec: 27895.19\r\n",
      "step:   191 | loss: 6.086936 | lr: 4.2965e-04 | norm: 0.7377 | time: 15453.91ms | tok/sec: 33925.90\r\n",
      "step:   192 | loss: 6.105727 | lr: 4.3189e-04 | norm: 0.7025 | time: 15608.72ms | tok/sec: 33589.44\r\n",
      "step:   193 | loss: 6.099411 | lr: 4.3413e-04 | norm: 0.6402 | time: 15543.02ms | tok/sec: 33731.42\r\n",
      "step:   194 | loss: 6.019052 | lr: 4.3636e-04 | norm: 0.5661 | time: 15399.39ms | tok/sec: 34046.01\r\n",
      "step:   195 | loss: 6.044366 | lr: 4.3860e-04 | norm: 0.6131 | time: 15331.36ms | tok/sec: 34197.10\r\n",
      "step:   196 | loss: 6.014658 | lr: 4.4084e-04 | norm: 0.7017 | time: 15351.49ms | tok/sec: 34152.26\r\n",
      "step:   197 | loss: 6.062570 | lr: 4.4308e-04 | norm: 1.0563 | time: 15359.85ms | tok/sec: 34133.66\r\n",
      "step:   198 | loss: 6.057117 | lr: 4.4531e-04 | norm: 0.6657 | time: 15339.47ms | tok/sec: 34179.01\r\n",
      "step:   199 | loss: 6.022540 | lr: 4.4755e-04 | norm: 0.6864 | time: 15386.84ms | tok/sec: 34073.80\r\n",
      "validation loss: 6.0416\r\n",
      "step:   200 | loss: 6.033893 | lr: 4.4979e-04 | norm: 0.7868 | time: 19841.67ms | tok/sec: 26423.59\r\n",
      "step:   201 | loss: 6.009344 | lr: 4.5203e-04 | norm: 0.7459 | time: 15535.53ms | tok/sec: 33747.67\r\n",
      "step:   202 | loss: 6.015257 | lr: 4.5427e-04 | norm: 0.5356 | time: 15516.46ms | tok/sec: 33789.15\r\n",
      "step:   203 | loss: 5.994101 | lr: 4.5650e-04 | norm: 0.5328 | time: 15474.61ms | tok/sec: 33880.54\r\n",
      "step:   204 | loss: 5.992441 | lr: 4.5874e-04 | norm: 0.5266 | time: 15476.77ms | tok/sec: 33875.79\r\n",
      "step:   205 | loss: 6.013162 | lr: 4.6098e-04 | norm: 0.4900 | time: 15429.06ms | tok/sec: 33980.55\r\n",
      "step:   206 | loss: 6.008759 | lr: 4.6322e-04 | norm: 0.5385 | time: 15470.13ms | tok/sec: 33890.35\r\n",
      "step:   207 | loss: 5.986365 | lr: 4.6545e-04 | norm: 0.5538 | time: 15421.51ms | tok/sec: 33997.19\r\n",
      "step:   208 | loss: 5.927177 | lr: 4.6769e-04 | norm: 0.4526 | time: 15429.65ms | tok/sec: 33979.24\r\n",
      "step:   209 | loss: 5.882335 | lr: 4.6993e-04 | norm: 0.4954 | time: 15450.09ms | tok/sec: 33934.31\r\n",
      "step:   210 | loss: 5.913017 | lr: 4.7217e-04 | norm: 0.5202 | time: 15390.63ms | tok/sec: 34065.40\r\n",
      "step:   211 | loss: 5.960476 | lr: 4.7441e-04 | norm: 0.7626 | time: 15359.32ms | tok/sec: 34134.85\r\n",
      "step:   212 | loss: 5.971539 | lr: 4.7664e-04 | norm: 0.7454 | time: 15373.27ms | tok/sec: 34103.88\r\n",
      "step:   213 | loss: 5.945978 | lr: 4.7888e-04 | norm: 0.5437 | time: 15406.27ms | tok/sec: 34030.81\r\n",
      "step:   214 | loss: 5.953086 | lr: 4.8112e-04 | norm: 0.7516 | time: 15383.33ms | tok/sec: 34081.58\r\n",
      "step:   215 | loss: 5.962674 | lr: 4.8336e-04 | norm: 0.8188 | time: 15412.44ms | tok/sec: 34017.20\r\n",
      "step:   216 | loss: 5.902451 | lr: 4.8559e-04 | norm: 0.8024 | time: 15416.91ms | tok/sec: 34007.34\r\n",
      "step:   217 | loss: 5.922489 | lr: 4.8783e-04 | norm: 0.7304 | time: 15451.02ms | tok/sec: 33932.25\r\n",
      "step:   218 | loss: 5.883805 | lr: 4.9007e-04 | norm: 0.6493 | time: 15406.97ms | tok/sec: 34029.27\r\n",
      "step:   219 | loss: 5.868711 | lr: 4.9231e-04 | norm: 0.5690 | time: 15469.25ms | tok/sec: 33892.28\r\n",
      "step:   220 | loss: 5.834816 | lr: 4.9455e-04 | norm: 0.5527 | time: 15501.97ms | tok/sec: 33820.73\r\n",
      "step:   221 | loss: 5.840743 | lr: 4.9678e-04 | norm: 0.4710 | time: 15572.49ms | tok/sec: 33667.57\r\n",
      "step:   222 | loss: 5.860166 | lr: 4.9902e-04 | norm: 0.4509 | time: 15526.39ms | tok/sec: 33767.54\r\n",
      "step:   223 | loss: 5.783566 | lr: 5.0126e-04 | norm: 0.4390 | time: 15490.53ms | tok/sec: 33845.71\r\n",
      "step:   224 | loss: 5.798413 | lr: 5.0350e-04 | norm: 0.5214 | time: 15435.66ms | tok/sec: 33966.02\r\n",
      "validation loss: 5.9205\r\n",
      "step:   225 | loss: 5.803523 | lr: 5.0573e-04 | norm: 0.6222 | time: 19103.97ms | tok/sec: 27443.93\r\n",
      "step:   226 | loss: 5.767365 | lr: 5.0797e-04 | norm: 0.8197 | time: 15326.09ms | tok/sec: 34208.85\r\n",
      "step:   227 | loss: 5.871927 | lr: 5.1021e-04 | norm: 0.5925 | time: 15388.03ms | tok/sec: 34071.16\r\n",
      "step:   228 | loss: 5.785006 | lr: 5.1245e-04 | norm: 0.5522 | time: 15397.26ms | tok/sec: 34050.74\r\n",
      "step:   229 | loss: 5.770760 | lr: 5.1469e-04 | norm: 0.6340 | time: 15442.03ms | tok/sec: 33952.02\r\n",
      "step:   230 | loss: 5.796976 | lr: 5.1692e-04 | norm: 0.6676 | time: 15464.05ms | tok/sec: 33903.67\r\n",
      "step:   231 | loss: 5.762563 | lr: 5.1916e-04 | norm: 0.7627 | time: 15485.03ms | tok/sec: 33857.74\r\n",
      "step:   232 | loss: 5.931214 | lr: 5.2140e-04 | norm: 0.6516 | time: 15532.76ms | tok/sec: 33753.68\r\n",
      "step:   233 | loss: 5.939920 | lr: 5.2364e-04 | norm: 0.7260 | time: 15501.61ms | tok/sec: 33821.52\r\n",
      "step:   234 | loss: 5.939198 | lr: 5.2587e-04 | norm: 0.7701 | time: 15505.81ms | tok/sec: 33812.36\r\n",
      "step:   235 | loss: 5.945455 | lr: 5.2811e-04 | norm: 0.6693 | time: 15529.38ms | tok/sec: 33761.05\r\n",
      "step:   236 | loss: 5.963194 | lr: 5.3035e-04 | norm: 0.7472 | time: 15538.99ms | tok/sec: 33740.17\r\n",
      "step:   237 | loss: 5.942367 | lr: 5.3259e-04 | norm: 0.6367 | time: 15503.17ms | tok/sec: 33818.12\r\n",
      "step:   238 | loss: 5.890148 | lr: 5.3483e-04 | norm: 0.6008 | time: 15561.83ms | tok/sec: 33690.64\r\n",
      "step:   239 | loss: 5.884096 | lr: 5.3706e-04 | norm: 0.5531 | time: 15486.50ms | tok/sec: 33854.53\r\n",
      "step:   240 | loss: 5.900578 | lr: 5.3930e-04 | norm: 0.6223 | time: 15393.10ms | tok/sec: 34059.93\r\n",
      "step:   241 | loss: 5.884161 | lr: 5.4154e-04 | norm: 0.5749 | time: 15423.54ms | tok/sec: 33992.70\r\n",
      "step:   242 | loss: 5.898243 | lr: 5.4378e-04 | norm: 0.4670 | time: 15384.76ms | tok/sec: 34078.40\r\n",
      "step:   243 | loss: 5.898057 | lr: 5.4601e-04 | norm: 0.4869 | time: 15432.85ms | tok/sec: 33972.20\r\n",
      "step:   244 | loss: 5.820500 | lr: 5.4825e-04 | norm: 0.5883 | time: 15431.43ms | tok/sec: 33975.34\r\n",
      "step:   245 | loss: 5.783614 | lr: 5.5049e-04 | norm: 0.5842 | time: 15406.45ms | tok/sec: 34030.42\r\n",
      "step:   246 | loss: 5.804200 | lr: 5.5273e-04 | norm: 0.5463 | time: 15403.52ms | tok/sec: 34036.89\r\n",
      "step:   247 | loss: 5.863001 | lr: 5.5497e-04 | norm: 0.6321 | time: 15468.97ms | tok/sec: 33892.88\r\n",
      "step:   248 | loss: 5.838262 | lr: 5.5720e-04 | norm: 0.7265 | time: 15462.74ms | tok/sec: 33906.54\r\n",
      "step:   249 | loss: 5.814101 | lr: 5.5944e-04 | norm: 0.7525 | time: 15403.21ms | tok/sec: 34037.59\r\n",
      "validation loss: 5.8337\r\n",
      "step:   250 | loss: 5.856533 | lr: 5.6168e-04 | norm: 0.8208 | time: 19135.82ms | tok/sec: 27398.26\r\n",
      "step:   251 | loss: 5.797412 | lr: 5.6392e-04 | norm: 0.6750 | time: 15432.08ms | tok/sec: 33973.91\r\n",
      "step:   252 | loss: 5.832003 | lr: 5.6615e-04 | norm: 0.6974 | time: 15472.90ms | tok/sec: 33884.27\r\n",
      "step:   253 | loss: 5.829060 | lr: 5.6839e-04 | norm: 0.7506 | time: 15459.28ms | tok/sec: 33914.12\r\n",
      "step:   254 | loss: 5.849713 | lr: 5.7063e-04 | norm: 0.8546 | time: 15477.01ms | tok/sec: 33875.28\r\n",
      "step:   255 | loss: 5.735742 | lr: 5.7287e-04 | norm: 0.6760 | time: 15458.52ms | tok/sec: 33915.79\r\n",
      "step:   256 | loss: 5.776981 | lr: 5.7510e-04 | norm: 0.5211 | time: 15482.71ms | tok/sec: 33862.82\r\n",
      "step:   257 | loss: 5.748754 | lr: 5.7734e-04 | norm: 0.5227 | time: 15492.13ms | tok/sec: 33842.22\r\n",
      "step:   258 | loss: 5.721762 | lr: 5.7958e-04 | norm: 0.5041 | time: 15434.83ms | tok/sec: 33967.84\r\n",
      "step:   259 | loss: 5.731597 | lr: 5.8182e-04 | norm: 0.5463 | time: 15439.80ms | tok/sec: 33956.91\r\n",
      "step:   260 | loss: 5.683970 | lr: 5.8406e-04 | norm: 0.6263 | time: 15490.09ms | tok/sec: 33846.68\r\n",
      "step:   261 | loss: 5.742214 | lr: 5.8629e-04 | norm: 0.8979 | time: 15467.04ms | tok/sec: 33897.10\r\n",
      "step:   262 | loss: 5.701923 | lr: 5.8853e-04 | norm: 0.6650 | time: 15431.21ms | tok/sec: 33975.83\r\n",
      "step:   263 | loss: 5.673817 | lr: 5.9077e-04 | norm: 0.4995 | time: 15421.04ms | tok/sec: 33998.23\r\n",
      "step:   264 | loss: 5.700224 | lr: 5.9301e-04 | norm: 0.6493 | time: 15413.18ms | tok/sec: 34015.56\r\n",
      "step:   265 | loss: 5.694812 | lr: 5.9524e-04 | norm: 0.5586 | time: 15488.01ms | tok/sec: 33851.23\r\n",
      "step:   266 | loss: 5.725093 | lr: 5.9748e-04 | norm: 0.5050 | time: 15431.00ms | tok/sec: 33976.28\r\n",
      "step:   267 | loss: 5.620925 | lr: 5.9972e-04 | norm: 0.4365 | time: 15417.18ms | tok/sec: 34006.74\r\n",
      "step:   268 | loss: 5.628159 | lr: 6.0196e-04 | norm: 0.4617 | time: 15427.89ms | tok/sec: 33983.13\r\n",
      "step:   269 | loss: 5.656118 | lr: 6.0420e-04 | norm: 0.5874 | time: 15433.88ms | tok/sec: 33969.93\r\n",
      "step:   270 | loss: 5.677864 | lr: 6.0643e-04 | norm: 0.7255 | time: 15443.95ms | tok/sec: 33947.79\r\n",
      "step:   271 | loss: 5.595710 | lr: 6.0867e-04 | norm: 0.5732 | time: 15411.82ms | tok/sec: 34018.56\r\n",
      "step:   272 | loss: 5.618431 | lr: 6.1091e-04 | norm: 0.6533 | time: 15406.18ms | tok/sec: 34031.01\r\n",
      "step:   273 | loss: 5.616926 | lr: 6.1315e-04 | norm: 0.7596 | time: 15464.76ms | tok/sec: 33902.11\r\n",
      "step:   274 | loss: 5.582421 | lr: 6.1538e-04 | norm: 0.6642 | time: 15467.13ms | tok/sec: 33896.92\r\n",
      "validation loss: 5.7278\r\n",
      "step:   275 | loss: 5.654089 | lr: 6.1762e-04 | norm: 0.6107 | time: 19145.41ms | tok/sec: 27384.53\r\n",
      "step:   276 | loss: 5.652462 | lr: 6.1986e-04 | norm: 0.5393 | time: 15419.44ms | tok/sec: 34001.76\r\n",
      "step:   277 | loss: 5.585140 | lr: 6.2210e-04 | norm: 0.5700 | time: 15397.09ms | tok/sec: 34051.10\r\n",
      "step:   278 | loss: 5.769363 | lr: 6.2434e-04 | norm: 0.8371 | time: 15484.83ms | tok/sec: 33858.17\r\n",
      "step:   279 | loss: 5.738406 | lr: 6.2657e-04 | norm: 0.9909 | time: 15482.12ms | tok/sec: 33864.10\r\n",
      "step:   280 | loss: 5.786865 | lr: 6.2881e-04 | norm: 0.8585 | time: 15480.68ms | tok/sec: 33867.24\r\n",
      "step:   281 | loss: 5.764169 | lr: 6.3105e-04 | norm: 0.8406 | time: 15500.61ms | tok/sec: 33823.71\r\n",
      "step:   282 | loss: 5.753740 | lr: 6.3329e-04 | norm: 0.8150 | time: 15529.17ms | tok/sec: 33761.49\r\n",
      "step:   283 | loss: 5.741590 | lr: 6.3552e-04 | norm: 0.5983 | time: 15504.53ms | tok/sec: 33815.16\r\n",
      "step:   284 | loss: 5.790034 | lr: 6.3776e-04 | norm: 0.5778 | time: 15496.84ms | tok/sec: 33831.92\r\n",
      "step:   285 | loss: 5.704598 | lr: 6.4000e-04 | norm: 0.6168 | time: 15498.69ms | tok/sec: 33827.89\r\n",
      "step:   286 | loss: 5.737216 | lr: 6.4224e-04 | norm: 0.5882 | time: 15507.21ms | tok/sec: 33809.31\r\n",
      "step:   287 | loss: 5.707140 | lr: 6.4448e-04 | norm: 0.4797 | time: 15532.87ms | tok/sec: 33753.46\r\n",
      "step:   288 | loss: 5.666599 | lr: 6.4671e-04 | norm: 0.5256 | time: 15566.26ms | tok/sec: 33681.05\r\n",
      "step:   289 | loss: 5.680459 | lr: 6.4895e-04 | norm: 0.5517 | time: 15520.44ms | tok/sec: 33780.49\r\n",
      "step:   290 | loss: 5.619598 | lr: 6.5119e-04 | norm: 0.5004 | time: 15492.25ms | tok/sec: 33841.95\r\n",
      "step:   291 | loss: 5.629201 | lr: 6.5343e-04 | norm: 0.4906 | time: 15491.22ms | tok/sec: 33844.20\r\n",
      "step:   292 | loss: 5.580729 | lr: 6.5566e-04 | norm: 0.5420 | time: 15455.67ms | tok/sec: 33922.05\r\n",
      "step:   293 | loss: 5.634277 | lr: 6.5790e-04 | norm: 0.6235 | time: 15473.88ms | tok/sec: 33882.13\r\n",
      "step:   294 | loss: 5.688493 | lr: 6.6014e-04 | norm: 0.6897 | time: 15439.93ms | tok/sec: 33956.62\r\n",
      "step:   295 | loss: 5.633102 | lr: 6.6238e-04 | norm: 0.6955 | time: 15390.84ms | tok/sec: 34064.93\r\n",
      "step:   296 | loss: 5.641675 | lr: 6.6462e-04 | norm: 0.6700 | time: 15405.92ms | tok/sec: 34031.60\r\n",
      "step:   297 | loss: 5.639424 | lr: 6.6685e-04 | norm: 0.7819 | time: 15385.88ms | tok/sec: 34075.92\r\n",
      "step:   298 | loss: 5.635890 | lr: 6.6909e-04 | norm: 0.7468 | time: 15381.85ms | tok/sec: 34084.85\r\n",
      "step:   299 | loss: 5.599375 | lr: 6.7133e-04 | norm: 0.6250 | time: 15388.35ms | tok/sec: 34070.44\r\n",
      "validation loss: 5.6124\r\n",
      "step:   300 | loss: 5.690127 | lr: 6.7357e-04 | norm: 0.5422 | time: 19078.16ms | tok/sec: 27481.06\r\n",
      "step:   301 | loss: 5.554137 | lr: 6.7580e-04 | norm: 0.5151 | time: 15370.12ms | tok/sec: 34110.87\r\n",
      "step:   302 | loss: 5.546158 | lr: 6.7804e-04 | norm: 0.4835 | time: 15437.78ms | tok/sec: 33961.36\r\n",
      "step:   303 | loss: 5.557546 | lr: 6.8028e-04 | norm: 0.4692 | time: 15412.70ms | tok/sec: 34016.62\r\n",
      "step:   304 | loss: 5.497469 | lr: 6.8252e-04 | norm: 0.5409 | time: 15431.31ms | tok/sec: 33975.59\r\n",
      "step:   305 | loss: 5.510491 | lr: 6.8476e-04 | norm: 0.5880 | time: 15464.12ms | tok/sec: 33903.51\r\n",
      "step:   306 | loss: 5.520929 | lr: 6.8699e-04 | norm: 0.6060 | time: 15495.57ms | tok/sec: 33834.71\r\n",
      "step:   307 | loss: 5.493480 | lr: 6.8923e-04 | norm: 0.5759 | time: 15416.00ms | tok/sec: 34009.35\r\n",
      "step:   308 | loss: 5.518221 | lr: 6.9147e-04 | norm: 0.5576 | time: 15466.04ms | tok/sec: 33899.30\r\n",
      "step:   309 | loss: 5.479154 | lr: 6.9371e-04 | norm: 0.6349 | time: 15433.43ms | tok/sec: 33970.94\r\n",
      "step:   310 | loss: 5.484785 | lr: 6.9594e-04 | norm: 0.6809 | time: 15446.63ms | tok/sec: 33941.91\r\n",
      "step:   311 | loss: 5.469671 | lr: 6.9818e-04 | norm: 0.5519 | time: 15419.81ms | tok/sec: 34000.93\r\n",
      "step:   312 | loss: 5.468967 | lr: 7.0042e-04 | norm: 0.4888 | time: 15475.99ms | tok/sec: 33877.52\r\n",
      "step:   313 | loss: 5.422649 | lr: 7.0266e-04 | norm: 0.4820 | time: 15450.39ms | tok/sec: 33933.64\r\n",
      "step:   314 | loss: 5.433382 | lr: 7.0490e-04 | norm: 0.5061 | time: 15463.01ms | tok/sec: 33905.94\r\n",
      "step:   315 | loss: 5.377188 | lr: 7.0713e-04 | norm: 0.4599 | time: 15477.95ms | tok/sec: 33873.21\r\n",
      "step:   316 | loss: 5.450012 | lr: 7.0937e-04 | norm: 0.4880 | time: 15438.07ms | tok/sec: 33960.71\r\n",
      "step:   317 | loss: 5.396238 | lr: 7.1161e-04 | norm: 0.4919 | time: 15480.10ms | tok/sec: 33868.52\r\n",
      "step:   318 | loss: 5.438453 | lr: 7.1385e-04 | norm: 0.5816 | time: 15392.62ms | tok/sec: 34061.00\r\n",
      "step:   319 | loss: 5.421635 | lr: 7.1608e-04 | norm: 0.6871 | time: 15412.11ms | tok/sec: 34017.92\r\n",
      "step:   320 | loss: 5.353789 | lr: 7.1832e-04 | norm: 0.5536 | time: 15361.67ms | tok/sec: 34129.63\r\n",
      "step:   321 | loss: 5.391372 | lr: 7.2056e-04 | norm: 0.5964 | time: 15373.77ms | tok/sec: 34102.77\r\n",
      "step:   322 | loss: 5.376698 | lr: 7.2280e-04 | norm: 0.7039 | time: 15399.11ms | tok/sec: 34046.63\r\n",
      "step:   323 | loss: 5.383440 | lr: 7.2503e-04 | norm: 0.7589 | time: 15452.49ms | tok/sec: 33929.04\r\n",
      "step:   324 | loss: 5.525256 | lr: 7.2727e-04 | norm: 0.7319 | time: 15386.84ms | tok/sec: 34073.79\r\n",
      "validation loss: 5.5364\r\n",
      "step:   325 | loss: 5.608968 | lr: 7.2951e-04 | norm: 0.9554 | time: 19176.71ms | tok/sec: 27339.83\r\n",
      "step:   326 | loss: 5.582957 | lr: 7.3175e-04 | norm: 0.9542 | time: 15429.64ms | tok/sec: 33979.28\r\n",
      "step:   327 | loss: 5.617993 | lr: 7.3399e-04 | norm: 0.9549 | time: 15474.01ms | tok/sec: 33881.84\r\n",
      "step:   328 | loss: 5.609056 | lr: 7.3622e-04 | norm: 0.8714 | time: 15405.87ms | tok/sec: 34031.70\r\n",
      "step:   329 | loss: 5.584711 | lr: 7.3846e-04 | norm: 0.9796 | time: 15426.33ms | tok/sec: 33986.56\r\n",
      "step:   330 | loss: 5.602703 | lr: 7.4070e-04 | norm: 0.7361 | time: 15448.26ms | tok/sec: 33938.33\r\n",
      "step:   331 | loss: 5.574298 | lr: 7.4294e-04 | norm: 0.7585 | time: 15433.34ms | tok/sec: 33971.13\r\n",
      "step:   332 | loss: 5.623876 | lr: 7.4517e-04 | norm: 0.8304 | time: 15411.33ms | tok/sec: 34019.64\r\n",
      "step:   333 | loss: 5.567212 | lr: 7.4741e-04 | norm: 0.8067 | time: 15417.11ms | tok/sec: 34006.90\r\n",
      "step:   334 | loss: 5.560792 | lr: 7.4965e-04 | norm: 0.6767 | time: 15481.29ms | tok/sec: 33865.92\r\n",
      "step:   335 | loss: 5.566526 | lr: 7.5189e-04 | norm: 0.5715 | time: 15496.29ms | tok/sec: 33833.14\r\n",
      "step:   336 | loss: 5.457798 | lr: 7.5413e-04 | norm: 0.5368 | time: 15451.76ms | tok/sec: 33930.62\r\n",
      "step:   337 | loss: 5.485674 | lr: 7.5636e-04 | norm: 0.6258 | time: 15481.94ms | tok/sec: 33864.49\r\n",
      "step:   338 | loss: 5.563270 | lr: 7.5860e-04 | norm: 0.9475 | time: 15510.96ms | tok/sec: 33801.14\r\n",
      "step:   339 | loss: 5.522346 | lr: 7.6084e-04 | norm: 0.8839 | time: 15455.03ms | tok/sec: 33923.46\r\n",
      "step:   340 | loss: 5.495921 | lr: 7.6308e-04 | norm: 0.6405 | time: 15462.04ms | tok/sec: 33908.07\r\n",
      "step:   341 | loss: 5.481797 | lr: 7.6531e-04 | norm: 0.6296 | time: 15395.83ms | tok/sec: 34053.89\r\n",
      "step:   342 | loss: 5.521586 | lr: 7.6755e-04 | norm: 0.4702 | time: 15482.87ms | tok/sec: 33862.45\r\n",
      "step:   343 | loss: 5.418376 | lr: 7.6979e-04 | norm: 0.5132 | time: 15454.85ms | tok/sec: 33923.84\r\n",
      "step:   344 | loss: 5.429653 | lr: 7.7203e-04 | norm: 0.4435 | time: 15491.60ms | tok/sec: 33843.38\r\n",
      "step:   345 | loss: 5.399771 | lr: 7.7427e-04 | norm: 0.4639 | time: 15490.22ms | tok/sec: 33846.38\r\n",
      "step:   346 | loss: 5.381432 | lr: 7.7650e-04 | norm: 0.4175 | time: 15443.94ms | tok/sec: 33947.82\r\n",
      "step:   347 | loss: 5.420208 | lr: 7.7874e-04 | norm: 0.5289 | time: 15490.90ms | tok/sec: 33844.91\r\n",
      "step:   348 | loss: 5.354825 | lr: 7.8098e-04 | norm: 0.5281 | time: 15466.81ms | tok/sec: 33897.62\r\n",
      "step:   349 | loss: 5.374696 | lr: 7.8322e-04 | norm: 0.6131 | time: 15431.24ms | tok/sec: 33975.76\r\n",
      "validation loss: 5.4189\r\n",
      "step:   350 | loss: 5.376717 | lr: 7.8545e-04 | norm: 0.7218 | time: 19134.74ms | tok/sec: 27399.80\r\n",
      "step:   351 | loss: 5.383202 | lr: 7.8769e-04 | norm: 0.5876 | time: 15405.08ms | tok/sec: 34033.44\r\n",
      "step:   352 | loss: 5.350838 | lr: 7.8993e-04 | norm: 0.6674 | time: 15393.47ms | tok/sec: 34059.12\r\n",
      "step:   353 | loss: 5.330560 | lr: 7.9217e-04 | norm: 0.5876 | time: 15366.88ms | tok/sec: 34118.05\r\n",
      "step:   354 | loss: 5.369589 | lr: 7.9441e-04 | norm: 0.5068 | time: 15340.80ms | tok/sec: 34176.05\r\n",
      "step:   355 | loss: 5.348917 | lr: 7.9664e-04 | norm: 0.5707 | time: 15369.25ms | tok/sec: 34112.79\r\n",
      "step:   356 | loss: 5.289662 | lr: 7.9888e-04 | norm: 0.5648 | time: 15325.48ms | tok/sec: 34210.22\r\n",
      "step:   357 | loss: 5.294167 | lr: 8.0112e-04 | norm: 0.4587 | time: 15344.45ms | tok/sec: 34167.92\r\n",
      "step:   358 | loss: 5.293701 | lr: 8.0336e-04 | norm: 0.4646 | time: 15350.28ms | tok/sec: 34154.95\r\n",
      "step:   359 | loss: 5.229940 | lr: 8.0559e-04 | norm: 0.4397 | time: 15406.20ms | tok/sec: 34030.97\r\n",
      "step:   360 | loss: 5.240798 | lr: 8.0783e-04 | norm: 0.4104 | time: 15410.72ms | tok/sec: 34021.00\r\n",
      "step:   361 | loss: 5.254570 | lr: 8.1007e-04 | norm: 0.4414 | time: 15389.43ms | tok/sec: 34068.07\r\n",
      "step:   362 | loss: 5.301877 | lr: 8.1231e-04 | norm: 0.5293 | time: 15449.03ms | tok/sec: 33936.62\r\n",
      "step:   363 | loss: 5.222879 | lr: 8.1455e-04 | norm: 0.5950 | time: 15402.17ms | tok/sec: 34039.87\r\n",
      "step:   364 | loss: 5.246593 | lr: 8.1678e-04 | norm: 0.6830 | time: 15406.67ms | tok/sec: 34029.94\r\n",
      "step:   365 | loss: 5.215335 | lr: 8.1902e-04 | norm: 0.6331 | time: 15431.73ms | tok/sec: 33974.68\r\n",
      "step:   366 | loss: 5.190009 | lr: 8.2126e-04 | norm: 0.6186 | time: 15422.96ms | tok/sec: 33993.98\r\n",
      "step:   367 | loss: 5.238900 | lr: 8.2350e-04 | norm: 0.6838 | time: 15449.39ms | tok/sec: 33935.84\r\n",
      "step:   368 | loss: 5.250907 | lr: 8.2573e-04 | norm: 0.6510 | time: 15426.81ms | tok/sec: 33985.50\r\n",
      "step:   369 | loss: 5.186669 | lr: 8.2797e-04 | norm: 0.6100 | time: 15448.67ms | tok/sec: 33937.43\r\n",
      "step:   370 | loss: 5.237502 | lr: 8.3021e-04 | norm: 0.6857 | time: 15484.48ms | tok/sec: 33858.93\r\n",
      "step:   371 | loss: 5.374032 | lr: 8.3245e-04 | norm: 0.8025 | time: 15494.59ms | tok/sec: 33836.84\r\n",
      "step:   372 | loss: 5.460643 | lr: 8.3469e-04 | norm: 0.8320 | time: 15477.83ms | tok/sec: 33873.48\r\n",
      "step:   373 | loss: 5.357211 | lr: 8.3692e-04 | norm: 0.7880 | time: 15469.98ms | tok/sec: 33890.68\r\n",
      "step:   374 | loss: 5.354636 | lr: 8.3916e-04 | norm: 0.7030 | time: 15468.32ms | tok/sec: 33894.31\r\n",
      "validation loss: 5.3216\r\n",
      "step:   375 | loss: 5.371802 | lr: 8.4140e-04 | norm: 0.5894 | time: 19104.88ms | tok/sec: 27442.62\r\n",
      "step:   376 | loss: 5.396434 | lr: 8.4364e-04 | norm: 0.6683 | time: 15413.51ms | tok/sec: 34014.83\r\n",
      "step:   377 | loss: 5.392515 | lr: 8.4587e-04 | norm: 0.6807 | time: 15453.81ms | tok/sec: 33926.14\r\n",
      "step:   378 | loss: 5.410973 | lr: 8.4811e-04 | norm: 0.7347 | time: 15432.16ms | tok/sec: 33973.72\r\n",
      "step:   379 | loss: 5.316406 | lr: 8.5035e-04 | norm: 0.7050 | time: 15423.01ms | tok/sec: 33993.88\r\n",
      "step:   380 | loss: 5.347392 | lr: 8.5259e-04 | norm: 0.6352 | time: 15422.61ms | tok/sec: 33994.77\r\n",
      "step:   381 | loss: 5.347239 | lr: 8.5483e-04 | norm: 0.5295 | time: 18476.24ms | tok/sec: 28376.34\r\n",
      "step:   382 | loss: 5.283975 | lr: 8.5706e-04 | norm: 0.5757 | time: 15490.19ms | tok/sec: 33846.46\r\n",
      "step:   383 | loss: 5.300600 | lr: 8.5930e-04 | norm: 0.7122 | time: 15608.16ms | tok/sec: 33590.63\r\n",
      "step:   384 | loss: 5.293967 | lr: 8.6154e-04 | norm: 0.8217 | time: 15448.52ms | tok/sec: 33937.76\r\n",
      "step:   385 | loss: 5.371253 | lr: 8.6378e-04 | norm: 0.7588 | time: 15392.67ms | tok/sec: 34060.89\r\n",
      "step:   386 | loss: 5.380038 | lr: 8.6601e-04 | norm: 0.6795 | time: 15405.74ms | tok/sec: 34032.00\r\n",
      "step:   387 | loss: 5.321940 | lr: 8.6825e-04 | norm: 0.4945 | time: 15415.63ms | tok/sec: 34010.15\r\n",
      "step:   388 | loss: 5.234235 | lr: 8.7049e-04 | norm: 0.5193 | time: 15359.50ms | tok/sec: 34134.45\r\n",
      "step:   389 | loss: 5.281436 | lr: 8.7273e-04 | norm: 0.5486 | time: 15320.70ms | tok/sec: 34220.88\r\n",
      "step:   390 | loss: 5.266766 | lr: 8.7497e-04 | norm: 0.6317 | time: 15415.68ms | tok/sec: 34010.06\r\n",
      "step:   391 | loss: 5.258583 | lr: 8.7720e-04 | norm: 0.5566 | time: 15374.84ms | tok/sec: 34100.38\r\n",
      "step:   392 | loss: 5.214025 | lr: 8.7944e-04 | norm: 0.5705 | time: 15408.67ms | tok/sec: 34025.52\r\n",
      "step:   393 | loss: 5.236689 | lr: 8.8168e-04 | norm: 0.5742 | time: 15370.47ms | tok/sec: 34110.09\r\n",
      "step:   394 | loss: 5.172995 | lr: 8.8392e-04 | norm: 0.6090 | time: 15414.41ms | tok/sec: 34012.85\r\n",
      "step:   395 | loss: 5.151091 | lr: 8.8615e-04 | norm: 0.6938 | time: 15431.90ms | tok/sec: 33974.30\r\n",
      "step:   396 | loss: 5.176023 | lr: 8.8839e-04 | norm: 0.6394 | time: 15391.43ms | tok/sec: 34063.63\r\n",
      "step:   397 | loss: 5.193900 | lr: 8.9063e-04 | norm: 0.7539 | time: 15396.97ms | tok/sec: 34051.37\r\n",
      "step:   398 | loss: 5.167362 | lr: 8.9287e-04 | norm: 0.6826 | time: 15372.20ms | tok/sec: 34106.25\r\n",
      "step:   399 | loss: 5.181249 | lr: 8.9510e-04 | norm: 0.5706 | time: 15452.83ms | tok/sec: 33928.29\r\n",
      "validation loss: 5.2083\r\n",
      "step:   400 | loss: 5.158356 | lr: 8.9734e-04 | norm: 0.5657 | time: 19135.95ms | tok/sec: 27398.07\r\n",
      "step:   401 | loss: 5.165636 | lr: 8.9958e-04 | norm: 0.5819 | time: 15391.86ms | tok/sec: 34062.69\r\n",
      "step:   402 | loss: 5.152021 | lr: 9.0182e-04 | norm: 0.7122 | time: 15379.24ms | tok/sec: 34090.64\r\n",
      "step:   403 | loss: 5.152008 | lr: 9.0406e-04 | norm: 0.6067 | time: 15464.96ms | tok/sec: 33901.66\r\n",
      "step:   404 | loss: 5.145704 | lr: 9.0629e-04 | norm: 0.6372 | time: 15426.90ms | tok/sec: 33985.31\r\n",
      "step:   405 | loss: 5.107274 | lr: 9.0853e-04 | norm: 0.5653 | time: 15394.37ms | tok/sec: 34057.13\r\n",
      "step:   406 | loss: 5.052731 | lr: 9.1077e-04 | norm: 0.5438 | time: 15390.67ms | tok/sec: 34065.32\r\n",
      "step:   407 | loss: 5.014395 | lr: 9.1301e-04 | norm: 0.5139 | time: 15371.00ms | tok/sec: 34108.91\r\n",
      "step:   408 | loss: 5.037171 | lr: 9.1524e-04 | norm: 0.4638 | time: 15424.71ms | tok/sec: 33990.14\r\n",
      "step:   409 | loss: 5.044463 | lr: 9.1748e-04 | norm: 0.5061 | time: 15368.14ms | tok/sec: 34115.26\r\n",
      "step:   410 | loss: 5.047882 | lr: 9.1972e-04 | norm: 0.5152 | time: 15336.93ms | tok/sec: 34184.67\r\n",
      "step:   411 | loss: 5.041333 | lr: 9.2196e-04 | norm: 0.5746 | time: 15312.98ms | tok/sec: 34238.14\r\n",
      "step:   412 | loss: 5.060429 | lr: 9.2420e-04 | norm: 0.7258 | time: 15327.21ms | tok/sec: 34206.36\r\n",
      "step:   413 | loss: 5.036756 | lr: 9.2643e-04 | norm: 0.6744 | time: 15310.62ms | tok/sec: 34243.41\r\n",
      "step:   414 | loss: 5.060991 | lr: 9.2867e-04 | norm: 0.7600 | time: 15365.99ms | tok/sec: 34120.03\r\n",
      "step:   415 | loss: 5.048534 | lr: 9.3091e-04 | norm: 0.6929 | time: 15386.48ms | tok/sec: 34074.60\r\n",
      "step:   416 | loss: 5.103679 | lr: 9.3315e-04 | norm: 0.6181 | time: 15369.38ms | tok/sec: 34112.51\r\n",
      "step:   417 | loss: 5.248381 | lr: 9.3538e-04 | norm: 0.6416 | time: 15492.76ms | tok/sec: 33840.84\r\n",
      "step:   418 | loss: 5.237768 | lr: 9.3762e-04 | norm: 0.6599 | time: 15448.37ms | tok/sec: 33938.08\r\n",
      "step:   419 | loss: 5.227935 | lr: 9.3986e-04 | norm: 0.6669 | time: 15421.95ms | tok/sec: 33996.23\r\n",
      "step:   420 | loss: 5.200050 | lr: 9.4210e-04 | norm: 0.8213 | time: 15476.39ms | tok/sec: 33876.63\r\n",
      "step:   421 | loss: 5.243608 | lr: 9.4434e-04 | norm: 0.8509 | time: 15445.25ms | tok/sec: 33944.94\r\n",
      "step:   422 | loss: 5.222684 | lr: 9.4657e-04 | norm: 0.9176 | time: 15488.97ms | tok/sec: 33849.11\r\n",
      "step:   423 | loss: 5.295134 | lr: 9.4881e-04 | norm: 0.7960 | time: 15454.84ms | tok/sec: 33923.88\r\n",
      "step:   424 | loss: 5.244064 | lr: 9.5105e-04 | norm: 0.8938 | time: 15496.55ms | tok/sec: 33832.55\r\n",
      "validation loss: 5.1834\r\n",
      "step:   425 | loss: 5.232583 | lr: 9.5329e-04 | norm: 0.8294 | time: 19107.11ms | tok/sec: 27439.42\r\n",
      "step:   426 | loss: 5.176295 | lr: 9.5552e-04 | norm: 0.6967 | time: 15482.64ms | tok/sec: 33862.97\r\n",
      "step:   427 | loss: 5.163490 | lr: 9.5776e-04 | norm: 0.5958 | time: 15491.46ms | tok/sec: 33843.68\r\n",
      "step:   428 | loss: 5.136110 | lr: 9.6000e-04 | norm: 0.5458 | time: 15478.45ms | tok/sec: 33872.12\r\n",
      "step:   429 | loss: 5.237851 | lr: 9.6224e-04 | norm: 0.5138 | time: 15490.78ms | tok/sec: 33845.17\r\n",
      "step:   430 | loss: 5.181731 | lr: 9.6448e-04 | norm: 0.5778 | time: 15504.32ms | tok/sec: 33815.60\r\n",
      "step:   431 | loss: 5.123837 | lr: 9.6671e-04 | norm: 0.5973 | time: 15454.41ms | tok/sec: 33924.80\r\n",
      "step:   432 | loss: 5.203658 | lr: 9.6895e-04 | norm: 0.5533 | time: 15467.42ms | tok/sec: 33896.29\r\n",
      "step:   433 | loss: 5.207901 | lr: 9.7119e-04 | norm: 0.5561 | time: 15442.88ms | tok/sec: 33950.14\r\n",
      "step:   434 | loss: 5.091226 | lr: 9.7343e-04 | norm: 0.6195 | time: 15435.28ms | tok/sec: 33966.85\r\n",
      "step:   435 | loss: 5.125591 | lr: 9.7566e-04 | norm: 0.5998 | time: 15460.66ms | tok/sec: 33911.11\r\n",
      "step:   436 | loss: 5.054541 | lr: 9.7790e-04 | norm: 0.5049 | time: 15401.28ms | tok/sec: 34041.85\r\n",
      "step:   437 | loss: 5.187042 | lr: 9.8014e-04 | norm: 0.5646 | time: 15476.25ms | tok/sec: 33876.94\r\n",
      "step:   438 | loss: 5.154893 | lr: 9.8238e-04 | norm: 0.5699 | time: 15527.32ms | tok/sec: 33765.51\r\n",
      "step:   439 | loss: 5.141655 | lr: 9.8462e-04 | norm: 0.6069 | time: 15448.85ms | tok/sec: 33937.03\r\n",
      "step:   440 | loss: 4.966593 | lr: 9.8685e-04 | norm: 0.6177 | time: 15451.30ms | tok/sec: 33931.64\r\n",
      "step:   441 | loss: 5.020398 | lr: 9.8909e-04 | norm: 0.5687 | time: 15439.35ms | tok/sec: 33957.90\r\n",
      "step:   442 | loss: 5.003903 | lr: 9.9133e-04 | norm: 0.6044 | time: 15474.79ms | tok/sec: 33880.13\r\n",
      "step:   443 | loss: 4.985104 | lr: 9.9357e-04 | norm: 0.6092 | time: 15458.73ms | tok/sec: 33915.33\r\n",
      "step:   444 | loss: 5.024615 | lr: 9.9580e-04 | norm: 0.6412 | time: 15462.94ms | tok/sec: 33906.09\r\n",
      "step:   445 | loss: 5.023434 | lr: 9.9804e-04 | norm: 0.5510 | time: 15403.46ms | tok/sec: 34037.02\r\n",
      "step:   446 | loss: 4.984610 | lr: 1.0003e-03 | norm: 0.4839 | time: 15423.12ms | tok/sec: 33993.64\r\n",
      "step:   447 | loss: 5.025757 | lr: 1.0025e-03 | norm: 0.4950 | time: 15454.38ms | tok/sec: 33924.87\r\n",
      "step:   448 | loss: 5.037775 | lr: 1.0048e-03 | norm: 0.5473 | time: 15424.80ms | tok/sec: 33989.93\r\n",
      "step:   449 | loss: 4.969254 | lr: 1.0070e-03 | norm: 0.5520 | time: 15435.69ms | tok/sec: 33965.97\r\n",
      "validation loss: 5.0143\r\n",
      "step:   450 | loss: 4.978502 | lr: 1.0092e-03 | norm: 0.5698 | time: 19111.85ms | tok/sec: 27432.62\r\n",
      "step:   451 | loss: 5.013892 | lr: 1.0115e-03 | norm: 0.7073 | time: 15432.84ms | tok/sec: 33972.23\r\n",
      "step:   452 | loss: 4.875485 | lr: 1.0137e-03 | norm: 0.5766 | time: 15408.51ms | tok/sec: 34025.87\r\n",
      "step:   453 | loss: 4.920138 | lr: 1.0159e-03 | norm: 0.6700 | time: 15429.08ms | tok/sec: 33980.51\r\n",
      "step:   454 | loss: 4.964262 | lr: 1.0182e-03 | norm: 0.8938 | time: 15422.82ms | tok/sec: 33994.30\r\n",
      "step:   455 | loss: 4.880109 | lr: 1.0204e-03 | norm: 0.7793 | time: 15505.90ms | tok/sec: 33812.15\r\n",
      "step:   456 | loss: 4.941698 | lr: 1.0227e-03 | norm: 0.5658 | time: 15465.34ms | tok/sec: 33900.84\r\n",
      "step:   457 | loss: 4.920390 | lr: 1.0249e-03 | norm: 0.5939 | time: 15430.04ms | tok/sec: 33978.39\r\n",
      "step:   458 | loss: 4.903123 | lr: 1.0271e-03 | norm: 0.5019 | time: 15400.04ms | tok/sec: 34044.60\r\n",
      "step:   459 | loss: 4.940540 | lr: 1.0294e-03 | norm: 0.5864 | time: 15469.52ms | tok/sec: 33891.67\r\n",
      "step:   460 | loss: 4.886974 | lr: 1.0316e-03 | norm: 0.6388 | time: 15433.85ms | tok/sec: 33970.00\r\n",
      "step:   461 | loss: 4.877813 | lr: 1.0338e-03 | norm: 0.6078 | time: 15471.10ms | tok/sec: 33888.21\r\n",
      "step:   462 | loss: 4.800605 | lr: 1.0361e-03 | norm: 0.5982 | time: 15417.33ms | tok/sec: 34006.40\r\n",
      "step:   463 | loss: 4.924264 | lr: 1.0383e-03 | norm: 0.6480 | time: 15462.86ms | tok/sec: 33906.28\r\n",
      "step:   464 | loss: 4.964216 | lr: 1.0406e-03 | norm: 0.5514 | time: 15389.84ms | tok/sec: 34067.15\r\n",
      "step:   465 | loss: 5.008475 | lr: 1.0428e-03 | norm: 0.5571 | time: 15411.26ms | tok/sec: 34019.80\r\n",
      "step:   466 | loss: 5.069534 | lr: 1.0450e-03 | norm: 0.4923 | time: 15387.31ms | tok/sec: 34072.76\r\n",
      "step:   467 | loss: 4.987417 | lr: 1.0473e-03 | norm: 0.5414 | time: 15421.80ms | tok/sec: 33996.56\r\n",
      "step:   468 | loss: 5.015568 | lr: 1.0495e-03 | norm: 0.5885 | time: 15443.91ms | tok/sec: 33947.89\r\n",
      "step:   469 | loss: 4.999124 | lr: 1.0517e-03 | norm: 0.6520 | time: 15427.06ms | tok/sec: 33984.96\r\n",
      "step:   470 | loss: 4.977021 | lr: 1.0540e-03 | norm: 0.5506 | time: 15428.02ms | tok/sec: 33982.85\r\n",
      "step:   471 | loss: 4.918289 | lr: 1.0562e-03 | norm: 0.5413 | time: 15424.43ms | tok/sec: 33990.76\r\n",
      "step:   472 | loss: 5.004608 | lr: 1.0585e-03 | norm: 0.5029 | time: 15392.93ms | tok/sec: 34060.31\r\n",
      "step:   473 | loss: 4.974371 | lr: 1.0607e-03 | norm: 0.4390 | time: 15442.68ms | tok/sec: 33950.59\r\n",
      "step:   474 | loss: 5.013238 | lr: 1.0629e-03 | norm: 0.4575 | time: 15477.48ms | tok/sec: 33874.26\r\n",
      "validation loss: 4.9061\r\n",
      "step:   475 | loss: 4.973198 | lr: 1.0652e-03 | norm: 0.4335 | time: 19351.41ms | tok/sec: 27093.01\r\n",
      "step:   476 | loss: 4.883495 | lr: 1.0674e-03 | norm: 0.4706 | time: 15373.81ms | tok/sec: 34102.68\r\n",
      "step:   477 | loss: 5.004082 | lr: 1.0697e-03 | norm: 0.5393 | time: 15410.61ms | tok/sec: 34021.24\r\n",
      "step:   478 | loss: 4.947446 | lr: 1.0719e-03 | norm: 0.7555 | time: 15450.63ms | tok/sec: 33933.12\r\n",
      "step:   479 | loss: 5.027359 | lr: 1.0741e-03 | norm: 0.7791 | time: 15410.15ms | tok/sec: 34022.26\r\n",
      "step:   480 | loss: 4.958265 | lr: 1.0764e-03 | norm: 0.6730 | time: 15423.67ms | tok/sec: 33992.43\r\n",
      "step:   481 | loss: 4.977108 | lr: 1.0786e-03 | norm: 0.5583 | time: 15487.65ms | tok/sec: 33852.00\r\n",
      "step:   482 | loss: 4.939055 | lr: 1.0808e-03 | norm: 0.5668 | time: 15425.59ms | tok/sec: 33988.21\r\n",
      "step:   483 | loss: 4.976161 | lr: 1.0831e-03 | norm: 0.5544 | time: 15448.74ms | tok/sec: 33937.27\r\n",
      "step:   484 | loss: 4.924111 | lr: 1.0853e-03 | norm: 0.6489 | time: 15437.48ms | tok/sec: 33962.02\r\n",
      "step:   485 | loss: 4.980434 | lr: 1.0876e-03 | norm: 0.6679 | time: 15450.22ms | tok/sec: 33934.00\r\n",
      "step:   486 | loss: 4.917257 | lr: 1.0898e-03 | norm: 0.5238 | time: 15433.23ms | tok/sec: 33971.37\r\n",
      "step:   487 | loss: 4.913961 | lr: 1.0920e-03 | norm: 0.5089 | time: 15466.05ms | tok/sec: 33899.29\r\n",
      "step:   488 | loss: 4.856912 | lr: 1.0943e-03 | norm: 0.4837 | time: 15464.71ms | tok/sec: 33902.21\r\n",
      "step:   489 | loss: 4.804546 | lr: 1.0965e-03 | norm: 0.4362 | time: 15473.13ms | tok/sec: 33883.78\r\n",
      "step:   490 | loss: 4.849742 | lr: 1.0987e-03 | norm: 0.3994 | time: 15407.84ms | tok/sec: 34027.36\r\n",
      "step:   491 | loss: 4.814654 | lr: 1.1010e-03 | norm: 0.4592 | time: 15450.73ms | tok/sec: 33932.89\r\n",
      "step:   492 | loss: 4.846694 | lr: 1.1032e-03 | norm: 0.5372 | time: 15393.75ms | tok/sec: 34058.50\r\n",
      "step:   493 | loss: 4.807348 | lr: 1.1055e-03 | norm: 0.5720 | time: 15429.81ms | tok/sec: 33978.90\r\n",
      "step:   494 | loss: 4.851500 | lr: 1.1077e-03 | norm: 0.4851 | time: 15442.37ms | tok/sec: 33951.27\r\n",
      "step:   495 | loss: 4.801152 | lr: 1.1099e-03 | norm: 0.4615 | time: 15433.53ms | tok/sec: 33970.71\r\n",
      "step:   496 | loss: 4.820680 | lr: 1.1122e-03 | norm: 0.4482 | time: 15423.04ms | tok/sec: 33993.81\r\n",
      "step:   497 | loss: 4.790776 | lr: 1.1144e-03 | norm: 0.5342 | time: 15440.17ms | tok/sec: 33956.10\r\n",
      "step:   498 | loss: 4.755110 | lr: 1.1166e-03 | norm: 0.5726 | time: 15411.83ms | tok/sec: 34018.54\r\n",
      "step:   499 | loss: 4.721412 | lr: 1.1189e-03 | norm: 0.7397 | time: 15408.07ms | tok/sec: 34026.85\r\n",
      "validation loss: 4.8763\r\n",
      "step:   500 | loss: 4.834085 | lr: 1.1211e-03 | norm: 0.8003 | time: 19926.54ms | tok/sec: 26311.05\r\n",
      "step:   501 | loss: 4.777185 | lr: 1.1234e-03 | norm: 0.7838 | time: 15401.27ms | tok/sec: 34041.86\r\n",
      "step:   502 | loss: 4.802878 | lr: 1.1256e-03 | norm: 0.7889 | time: 15412.01ms | tok/sec: 34018.15\r\n",
      "step:   503 | loss: 4.728982 | lr: 1.1278e-03 | norm: 0.7496 | time: 15427.92ms | tok/sec: 33983.07\r\n",
      "step:   504 | loss: 4.815511 | lr: 1.1301e-03 | norm: 0.6764 | time: 15468.67ms | tok/sec: 33893.55\r\n",
      "step:   505 | loss: 4.757924 | lr: 1.1323e-03 | norm: 0.6898 | time: 15493.38ms | tok/sec: 33839.49\r\n",
      "step:   506 | loss: 4.779409 | lr: 1.1345e-03 | norm: 0.6037 | time: 15462.85ms | tok/sec: 33906.29\r\n",
      "step:   507 | loss: 4.776804 | lr: 1.1368e-03 | norm: 0.5334 | time: 15445.59ms | tok/sec: 33944.19\r\n",
      "step:   508 | loss: 4.672297 | lr: 1.1390e-03 | norm: 0.5989 | time: 15468.47ms | tok/sec: 33893.97\r\n",
      "step:   509 | loss: 4.722145 | lr: 1.1413e-03 | norm: 0.5800 | time: 15408.72ms | tok/sec: 34025.41\r\n",
      "step:   510 | loss: 4.881641 | lr: 1.1435e-03 | norm: 0.4927 | time: 15417.92ms | tok/sec: 34005.10\r\n",
      "step:   511 | loss: 4.900129 | lr: 1.1457e-03 | norm: 0.5652 | time: 15372.96ms | tok/sec: 34104.55\r\n",
      "step:   512 | loss: 4.913714 | lr: 1.1480e-03 | norm: 0.6702 | time: 15364.56ms | tok/sec: 34123.19\r\n",
      "step:   513 | loss: 4.888968 | lr: 1.1502e-03 | norm: 0.6232 | time: 15406.52ms | tok/sec: 34030.26\r\n",
      "step:   514 | loss: 4.915685 | lr: 1.1524e-03 | norm: 0.6751 | time: 15399.58ms | tok/sec: 34045.61\r\n",
      "step:   515 | loss: 4.895164 | lr: 1.1547e-03 | norm: 0.6613 | time: 15433.82ms | tok/sec: 33970.07\r\n",
      "step:   516 | loss: 5.046905 | lr: 1.1569e-03 | norm: 0.5390 | time: 15431.82ms | tok/sec: 33974.48\r\n",
      "step:   517 | loss: 4.968060 | lr: 1.1592e-03 | norm: 0.7000 | time: 15480.97ms | tok/sec: 33866.60\r\n",
      "step:   518 | loss: 4.898977 | lr: 1.1614e-03 | norm: 0.7487 | time: 15455.49ms | tok/sec: 33922.45\r\n",
      "step:   519 | loss: 4.951950 | lr: 1.1636e-03 | norm: 0.7460 | time: 15457.91ms | tok/sec: 33917.13\r\n",
      "step:   520 | loss: 4.917724 | lr: 1.1659e-03 | norm: 0.7314 | time: 15474.74ms | tok/sec: 33880.24\r\n",
      "step:   521 | loss: 4.888028 | lr: 1.1681e-03 | norm: 0.5777 | time: 15450.56ms | tok/sec: 33933.27\r\n",
      "step:   522 | loss: 4.881226 | lr: 1.1703e-03 | norm: 0.4423 | time: 15466.18ms | tok/sec: 33899.01\r\n",
      "step:   523 | loss: 4.807109 | lr: 1.1726e-03 | norm: 0.4849 | time: 15403.77ms | tok/sec: 34036.34\r\n",
      "step:   524 | loss: 4.826281 | lr: 1.1748e-03 | norm: 0.4443 | time: 15393.48ms | tok/sec: 34059.10\r\n",
      "validation loss: 4.7795\r\n",
      "step:   525 | loss: 4.801083 | lr: 1.1771e-03 | norm: 0.4441 | time: 19081.31ms | tok/sec: 27476.53\r\n",
      "step:   526 | loss: 4.819918 | lr: 1.1793e-03 | norm: 0.4175 | time: 15365.16ms | tok/sec: 34121.88\r\n",
      "step:   527 | loss: 4.797449 | lr: 1.1815e-03 | norm: 0.4191 | time: 15334.47ms | tok/sec: 34190.16\r\n",
      "step:   528 | loss: 4.796662 | lr: 1.1838e-03 | norm: 0.4686 | time: 15351.83ms | tok/sec: 34151.50\r\n",
      "step:   529 | loss: 4.799278 | lr: 1.1860e-03 | norm: 0.5198 | time: 15333.65ms | tok/sec: 34191.99\r\n",
      "step:   530 | loss: 4.785156 | lr: 1.1883e-03 | norm: 0.4891 | time: 15325.50ms | tok/sec: 34210.17\r\n",
      "step:   531 | loss: 4.783577 | lr: 1.1905e-03 | norm: 0.4631 | time: 15329.76ms | tok/sec: 34200.67\r\n",
      "step:   532 | loss: 4.746480 | lr: 1.1927e-03 | norm: 0.4618 | time: 15358.01ms | tok/sec: 34137.76\r\n",
      "step:   533 | loss: 4.763232 | lr: 1.1950e-03 | norm: 0.4785 | time: 15366.31ms | tok/sec: 34119.31\r\n",
      "step:   534 | loss: 4.721341 | lr: 1.1972e-03 | norm: 0.5718 | time: 15387.01ms | tok/sec: 34073.41\r\n",
      "step:   535 | loss: 4.725938 | lr: 1.1994e-03 | norm: 0.5108 | time: 15392.10ms | tok/sec: 34062.15\r\n",
      "step:   536 | loss: 4.774038 | lr: 1.2017e-03 | norm: 0.4858 | time: 15376.26ms | tok/sec: 34097.23\r\n",
      "step:   537 | loss: 4.675507 | lr: 1.2039e-03 | norm: 0.4777 | time: 15372.45ms | tok/sec: 34105.69\r\n",
      "step:   538 | loss: 4.712161 | lr: 1.2062e-03 | norm: 0.5069 | time: 15451.32ms | tok/sec: 33931.60\r\n",
      "step:   539 | loss: 4.704588 | lr: 1.2084e-03 | norm: 0.4669 | time: 15484.09ms | tok/sec: 33859.78\r\n",
      "step:   540 | loss: 4.657294 | lr: 1.2106e-03 | norm: 0.4136 | time: 15400.54ms | tok/sec: 34043.48\r\n",
      "step:   541 | loss: 4.625984 | lr: 1.2129e-03 | norm: 0.3842 | time: 15412.26ms | tok/sec: 34017.59\r\n",
      "step:   542 | loss: 4.699691 | lr: 1.2151e-03 | norm: 0.3624 | time: 15390.67ms | tok/sec: 34065.32\r\n",
      "step:   543 | loss: 4.675246 | lr: 1.2173e-03 | norm: 0.3905 | time: 15388.04ms | tok/sec: 34071.13\r\n",
      "step:   544 | loss: 4.675359 | lr: 1.2196e-03 | norm: 0.4131 | time: 15387.74ms | tok/sec: 34071.81\r\n",
      "step:   545 | loss: 4.606570 | lr: 1.2218e-03 | norm: 0.4944 | time: 15421.97ms | tok/sec: 33996.17\r\n",
      "step:   546 | loss: 4.569333 | lr: 1.2241e-03 | norm: 0.6292 | time: 15430.99ms | tok/sec: 33976.30\r\n",
      "step:   547 | loss: 4.569032 | lr: 1.2263e-03 | norm: 0.7074 | time: 15463.36ms | tok/sec: 33905.19\r\n",
      "step:   548 | loss: 4.633807 | lr: 1.2285e-03 | norm: 0.7794 | time: 15426.07ms | tok/sec: 33987.14\r\n",
      "step:   549 | loss: 4.664209 | lr: 1.2308e-03 | norm: 0.8372 | time: 15441.98ms | tok/sec: 33952.11\r\n",
      "validation loss: 4.7424\r\n",
      "step:   550 | loss: 4.644373 | lr: 1.2330e-03 | norm: 0.7432 | time: 19128.30ms | tok/sec: 27409.02\r\n",
      "step:   551 | loss: 4.644479 | lr: 1.2352e-03 | norm: 0.7257 | time: 15483.58ms | tok/sec: 33860.91\r\n",
      "step:   552 | loss: 4.587216 | lr: 1.2375e-03 | norm: 0.7459 | time: 15467.39ms | tok/sec: 33896.34\r\n",
      "step:   553 | loss: 4.694808 | lr: 1.2397e-03 | norm: 0.7228 | time: 15477.38ms | tok/sec: 33874.46\r\n",
      "step:   554 | loss: 4.618052 | lr: 1.2420e-03 | norm: 0.5856 | time: 15479.81ms | tok/sec: 33869.14\r\n",
      "step:   555 | loss: 4.588800 | lr: 1.2442e-03 | norm: 0.5151 | time: 15488.60ms | tok/sec: 33849.93\r\n",
      "step:   556 | loss: 4.612063 | lr: 1.2464e-03 | norm: 0.4694 | time: 15425.81ms | tok/sec: 33987.72\r\n",
      "step:   557 | loss: 4.752006 | lr: 1.2487e-03 | norm: 0.5473 | time: 15454.23ms | tok/sec: 33925.20\r\n",
      "step:   558 | loss: 4.776957 | lr: 1.2509e-03 | norm: 0.6610 | time: 15464.64ms | tok/sec: 33902.37\r\n",
      "step:   559 | loss: 4.715487 | lr: 1.2531e-03 | norm: 0.5974 | time: 15463.61ms | tok/sec: 33904.63\r\n",
      "step:   560 | loss: 4.788709 | lr: 1.2554e-03 | norm: 0.6630 | time: 15448.70ms | tok/sec: 33937.36\r\n",
      "step:   561 | loss: 4.739124 | lr: 1.2576e-03 | norm: 0.7115 | time: 15457.00ms | tok/sec: 33919.12\r\n",
      "step:   562 | loss: 4.755368 | lr: 1.2599e-03 | norm: 0.6816 | time: 15432.54ms | tok/sec: 33972.90\r\n",
      "step:   563 | loss: 4.719697 | lr: 1.2621e-03 | norm: 0.5868 | time: 15472.60ms | tok/sec: 33884.93\r\n",
      "step:   564 | loss: 4.762021 | lr: 1.2643e-03 | norm: 0.5535 | time: 15456.54ms | tok/sec: 33920.14\r\n",
      "step:   565 | loss: 4.741186 | lr: 1.2666e-03 | norm: 0.5416 | time: 15461.05ms | tok/sec: 33910.24\r\n",
      "step:   566 | loss: 4.686141 | lr: 1.2688e-03 | norm: 0.5010 | time: 15476.01ms | tok/sec: 33877.46\r\n",
      "step:   567 | loss: 4.758323 | lr: 1.2710e-03 | norm: 0.4501 | time: 15493.22ms | tok/sec: 33839.83\r\n",
      "step:   568 | loss: 4.682395 | lr: 1.2733e-03 | norm: 0.4571 | time: 15469.13ms | tok/sec: 33892.52\r\n",
      "step:   569 | loss: 4.711030 | lr: 1.2755e-03 | norm: 0.4371 | time: 15429.23ms | tok/sec: 33980.17\r\n",
      "step:   570 | loss: 4.664439 | lr: 1.2778e-03 | norm: 0.5006 | time: 15413.30ms | tok/sec: 34015.29\r\n",
      "step:   571 | loss: 4.669872 | lr: 1.2800e-03 | norm: 0.6434 | time: 15365.80ms | tok/sec: 34120.44\r\n",
      "step:   572 | loss: 4.695872 | lr: 1.2822e-03 | norm: 0.7337 | time: 17965.96ms | tok/sec: 29182.29\r\n",
      "step:   573 | loss: 4.684017 | lr: 1.2845e-03 | norm: 0.5819 | time: 15548.89ms | tok/sec: 33718.68\r\n",
      "step:   574 | loss: 4.647823 | lr: 1.2867e-03 | norm: 0.5226 | time: 15531.78ms | tok/sec: 33755.81\r\n",
      "validation loss: 4.6337\r\n",
      "step:   575 | loss: 4.649259 | lr: 1.2890e-03 | norm: 0.5326 | time: 19126.96ms | tok/sec: 27410.94\r\n",
      "step:   576 | loss: 4.655902 | lr: 1.2912e-03 | norm: 0.5372 | time: 15476.35ms | tok/sec: 33876.72\r\n",
      "step:   577 | loss: 4.681232 | lr: 1.2934e-03 | norm: 0.5247 | time: 15490.71ms | tok/sec: 33845.31\r\n",
      "step:   578 | loss: 4.657236 | lr: 1.2957e-03 | norm: 0.4786 | time: 15468.68ms | tok/sec: 33893.53\r\n",
      "step:   579 | loss: 4.627127 | lr: 1.2979e-03 | norm: 0.4566 | time: 15462.43ms | tok/sec: 33907.22\r\n",
      "step:   580 | loss: 4.561585 | lr: 1.3001e-03 | norm: 0.4882 | time: 15492.52ms | tok/sec: 33841.36\r\n",
      "step:   581 | loss: 4.603492 | lr: 1.3024e-03 | norm: 0.4432 | time: 15492.32ms | tok/sec: 33841.81\r\n",
      "step:   582 | loss: 4.442153 | lr: 1.3046e-03 | norm: 0.5766 | time: 15434.90ms | tok/sec: 33967.69\r\n",
      "step:   583 | loss: 4.537981 | lr: 1.3069e-03 | norm: 0.5547 | time: 15458.22ms | tok/sec: 33916.46\r\n",
      "step:   584 | loss: 4.587769 | lr: 1.3091e-03 | norm: 0.5866 | time: 15496.11ms | tok/sec: 33833.52\r\n",
      "step:   585 | loss: 4.556461 | lr: 1.3113e-03 | norm: 0.5502 | time: 15445.41ms | tok/sec: 33944.57\r\n",
      "step:   586 | loss: 4.573012 | lr: 1.3136e-03 | norm: 0.5509 | time: 15429.94ms | tok/sec: 33978.62\r\n",
      "step:   587 | loss: 4.589507 | lr: 1.3158e-03 | norm: 0.5088 | time: 15410.35ms | tok/sec: 34021.80\r\n",
      "step:   588 | loss: 4.566862 | lr: 1.3180e-03 | norm: 0.4676 | time: 15437.41ms | tok/sec: 33962.17\r\n",
      "step:   589 | loss: 4.599246 | lr: 1.3203e-03 | norm: 0.5356 | time: 15452.85ms | tok/sec: 33928.23\r\n",
      "step:   590 | loss: 4.615589 | lr: 1.3225e-03 | norm: 0.6769 | time: 15409.56ms | tok/sec: 34023.55\r\n",
      "step:   591 | loss: 4.576168 | lr: 1.3248e-03 | norm: 0.6363 | time: 15462.78ms | tok/sec: 33906.46\r\n",
      "step:   592 | loss: 4.436594 | lr: 1.3270e-03 | norm: 0.5860 | time: 15438.00ms | tok/sec: 33960.87\r\n",
      "step:   593 | loss: 4.447146 | lr: 1.3292e-03 | norm: 0.5706 | time: 15470.20ms | tok/sec: 33890.20\r\n",
      "step:   594 | loss: 4.447351 | lr: 1.3315e-03 | norm: 0.5183 | time: 15462.70ms | tok/sec: 33906.62\r\n",
      "step:   595 | loss: 4.506063 | lr: 1.3337e-03 | norm: 0.5462 | time: 15431.27ms | tok/sec: 33975.69\r\n",
      "step:   596 | loss: 4.424810 | lr: 1.3359e-03 | norm: 0.4496 | time: 15398.51ms | tok/sec: 34047.97\r\n",
      "step:   597 | loss: 4.475836 | lr: 1.3382e-03 | norm: 0.4374 | time: 15417.54ms | tok/sec: 34005.95\r\n",
      "step:   598 | loss: 4.463658 | lr: 1.3404e-03 | norm: 0.3910 | time: 15467.16ms | tok/sec: 33896.84\r\n",
      "step:   599 | loss: 4.416060 | lr: 1.3427e-03 | norm: 0.4417 | time: 15422.99ms | tok/sec: 33993.93\r\n",
      "validation loss: 4.5408\r\n",
      "step:   600 | loss: 4.414510 | lr: 1.3449e-03 | norm: 0.5424 | time: 19135.89ms | tok/sec: 27398.15\r\n",
      "step:   601 | loss: 4.391244 | lr: 1.3471e-03 | norm: 0.5653 | time: 15436.63ms | tok/sec: 33963.90\r\n",
      "step:   602 | loss: 4.440339 | lr: 1.3494e-03 | norm: 0.6886 | time: 15446.42ms | tok/sec: 33942.37\r\n",
      "step:   603 | loss: 4.512151 | lr: 1.3516e-03 | norm: 0.6234 | time: 15469.69ms | tok/sec: 33891.31\r\n",
      "step:   604 | loss: 4.590115 | lr: 1.3538e-03 | norm: 0.5932 | time: 15488.27ms | tok/sec: 33850.66\r\n",
      "step:   605 | loss: 4.625505 | lr: 1.3561e-03 | norm: 0.5598 | time: 15416.94ms | tok/sec: 34007.27\r\n",
      "step:   606 | loss: 4.578054 | lr: 1.3583e-03 | norm: 0.5029 | time: 15426.81ms | tok/sec: 33985.50\r\n",
      "step:   607 | loss: 4.594533 | lr: 1.3606e-03 | norm: 0.6213 | time: 15510.01ms | tok/sec: 33803.20\r\n",
      "step:   608 | loss: 4.577975 | lr: 1.3628e-03 | norm: 0.5739 | time: 15505.06ms | tok/sec: 33813.98\r\n",
      "step:   609 | loss: 4.564156 | lr: 1.3650e-03 | norm: 0.6254 | time: 15418.27ms | tok/sec: 34004.34\r\n",
      "step:   610 | loss: 4.583843 | lr: 1.3673e-03 | norm: 0.6192 | time: 15398.75ms | tok/sec: 34047.43\r\n",
      "step:   611 | loss: 4.605158 | lr: 1.3695e-03 | norm: 0.7402 | time: 15364.87ms | tok/sec: 34122.52\r\n",
      "step:   612 | loss: 4.608643 | lr: 1.3717e-03 | norm: 0.7054 | time: 15383.86ms | tok/sec: 34080.39\r\n",
      "step:   613 | loss: 4.527932 | lr: 1.3740e-03 | norm: 0.6219 | time: 15372.91ms | tok/sec: 34104.68\r\n",
      "step:   614 | loss: 4.603778 | lr: 1.3762e-03 | norm: 0.6350 | time: 15375.58ms | tok/sec: 34098.74\r\n",
      "step:   615 | loss: 4.612716 | lr: 1.3785e-03 | norm: 0.6106 | time: 15368.19ms | tok/sec: 34115.14\r\n",
      "step:   616 | loss: 4.553992 | lr: 1.3807e-03 | norm: 0.6693 | time: 15408.55ms | tok/sec: 34025.78\r\n",
      "step:   617 | loss: 4.601205 | lr: 1.3829e-03 | norm: 0.6622 | time: 15407.94ms | tok/sec: 34027.13\r\n",
      "step:   618 | loss: 4.533762 | lr: 1.3852e-03 | norm: 0.5651 | time: 15437.63ms | tok/sec: 33961.69\r\n",
      "step:   619 | loss: 4.523298 | lr: 1.3874e-03 | norm: 0.5388 | time: 15491.50ms | tok/sec: 33843.60\r\n",
      "step:   620 | loss: 4.582707 | lr: 1.3897e-03 | norm: 0.6137 | time: 15446.10ms | tok/sec: 33943.07\r\n",
      "step:   621 | loss: 4.527003 | lr: 1.3919e-03 | norm: 0.5568 | time: 15445.71ms | tok/sec: 33943.93\r\n",
      "step:   622 | loss: 4.550508 | lr: 1.3941e-03 | norm: 0.4542 | time: 15471.94ms | tok/sec: 33886.38\r\n",
      "step:   623 | loss: 4.514839 | lr: 1.3964e-03 | norm: 0.4380 | time: 15482.04ms | tok/sec: 33864.26\r\n",
      "step:   624 | loss: 4.506986 | lr: 1.3986e-03 | norm: 0.4896 | time: 15472.18ms | tok/sec: 33885.85\r\n",
      "validation loss: 4.4757\r\n",
      "step:   625 | loss: 4.485627 | lr: 1.4008e-03 | norm: 0.5628 | time: 19176.96ms | tok/sec: 27339.48\r\n",
      "step:   626 | loss: 4.528322 | lr: 1.4031e-03 | norm: 0.5472 | time: 15515.85ms | tok/sec: 33790.48\r\n",
      "step:   627 | loss: 4.444615 | lr: 1.4053e-03 | norm: 0.6175 | time: 15442.43ms | tok/sec: 33951.14\r\n",
      "step:   628 | loss: 4.477523 | lr: 1.4076e-03 | norm: 0.6527 | time: 15525.03ms | tok/sec: 33770.51\r\n",
      "step:   629 | loss: 4.457049 | lr: 1.4098e-03 | norm: 0.5531 | time: 15546.60ms | tok/sec: 33723.64\r\n",
      "step:   630 | loss: 4.459581 | lr: 1.4120e-03 | norm: 0.5287 | time: 15432.54ms | tok/sec: 33972.90\r\n",
      "step:   631 | loss: 4.438288 | lr: 1.4143e-03 | norm: 0.4715 | time: 15475.26ms | tok/sec: 33879.10\r\n",
      "step:   632 | loss: 4.438995 | lr: 1.4165e-03 | norm: 0.4495 | time: 15475.45ms | tok/sec: 33878.70\r\n",
      "step:   633 | loss: 4.430805 | lr: 1.4187e-03 | norm: 0.4548 | time: 15469.70ms | tok/sec: 33891.29\r\n",
      "step:   634 | loss: 4.437732 | lr: 1.4210e-03 | norm: 0.4150 | time: 15504.49ms | tok/sec: 33815.24\r\n",
      "step:   635 | loss: 4.459816 | lr: 1.4232e-03 | norm: 0.3986 | time: 15438.98ms | tok/sec: 33958.71\r\n",
      "step:   636 | loss: 4.472316 | lr: 1.4255e-03 | norm: 0.3958 | time: 15438.28ms | tok/sec: 33960.25\r\n",
      "step:   637 | loss: 4.635614 | lr: 1.4277e-03 | norm: 0.5415 | time: 15465.11ms | tok/sec: 33901.34\r\n",
      "step:   638 | loss: 4.400337 | lr: 1.4299e-03 | norm: 0.6802 | time: 15471.03ms | tok/sec: 33888.36\r\n",
      "step:   639 | loss: 4.416509 | lr: 1.4322e-03 | norm: 0.6848 | time: 15450.55ms | tok/sec: 33933.28\r\n",
      "step:   640 | loss: 4.308903 | lr: 1.4344e-03 | norm: 0.5667 | time: 15407.63ms | tok/sec: 34027.82\r\n",
      "step:   641 | loss: 4.357659 | lr: 1.4366e-03 | norm: 0.5565 | time: 15383.06ms | tok/sec: 34082.18\r\n",
      "step:   642 | loss: 4.371272 | lr: 1.4389e-03 | norm: 0.5608 | time: 15367.88ms | tok/sec: 34115.84\r\n",
      "step:   643 | loss: 4.389803 | lr: 1.4411e-03 | norm: 0.5460 | time: 15329.11ms | tok/sec: 34202.11\r\n",
      "step:   644 | loss: 4.354051 | lr: 1.4434e-03 | norm: 0.5421 | time: 15322.88ms | tok/sec: 34216.02\r\n",
      "step:   645 | loss: 4.307177 | lr: 1.4456e-03 | norm: 0.5437 | time: 15324.95ms | tok/sec: 34211.40\r\n",
      "step:   646 | loss: 4.336479 | lr: 1.4478e-03 | norm: 0.5631 | time: 15322.98ms | tok/sec: 34215.80\r\n",
      "step:   647 | loss: 4.344050 | lr: 1.4501e-03 | norm: 0.4744 | time: 15338.76ms | tok/sec: 34180.59\r\n",
      "step:   648 | loss: 4.337379 | lr: 1.4523e-03 | norm: 0.4827 | time: 15303.86ms | tok/sec: 34258.54\r\n",
      "step:   649 | loss: 4.304615 | lr: 1.4545e-03 | norm: 0.5183 | time: 15368.89ms | tok/sec: 34113.59\r\n",
      "validation loss: 4.4373\r\n",
      "step:   650 | loss: 4.441562 | lr: 1.4568e-03 | norm: 0.6112 | time: 19101.86ms | tok/sec: 27446.97\r\n",
      "step:   651 | loss: 4.494248 | lr: 1.4590e-03 | norm: 0.5997 | time: 15428.87ms | tok/sec: 33980.97\r\n",
      "step:   652 | loss: 4.590700 | lr: 1.4613e-03 | norm: 0.6029 | time: 15430.22ms | tok/sec: 33978.01\r\n",
      "step:   653 | loss: 4.500623 | lr: 1.4635e-03 | norm: 0.6396 | time: 15460.33ms | tok/sec: 33911.83\r\n",
      "step:   654 | loss: 4.454788 | lr: 1.4657e-03 | norm: 0.6298 | time: 15469.07ms | tok/sec: 33892.66\r\n",
      "step:   655 | loss: 4.509010 | lr: 1.4680e-03 | norm: 0.5628 | time: 15433.49ms | tok/sec: 33970.81\r\n",
      "step:   656 | loss: 4.484059 | lr: 1.4702e-03 | norm: 0.5009 | time: 15484.71ms | tok/sec: 33858.44\r\n",
      "step:   657 | loss: 4.435621 | lr: 1.4724e-03 | norm: 0.5246 | time: 15399.50ms | tok/sec: 34045.77\r\n",
      "step:   658 | loss: 4.449677 | lr: 1.4747e-03 | norm: 0.5323 | time: 15472.78ms | tok/sec: 33884.55\r\n",
      "step:   659 | loss: 4.490114 | lr: 1.4769e-03 | norm: 0.4504 | time: 15448.36ms | tok/sec: 33938.11\r\n",
      "step:   660 | loss: 4.417204 | lr: 1.4792e-03 | norm: 0.4579 | time: 15483.91ms | tok/sec: 33860.18\r\n",
      "step:   661 | loss: 4.461991 | lr: 1.4814e-03 | norm: 0.4039 | time: 15427.92ms | tok/sec: 33983.06\r\n",
      "step:   662 | loss: 4.373406 | lr: 1.4836e-03 | norm: 0.4393 | time: 15405.47ms | tok/sec: 34032.58\r\n",
      "step:   663 | loss: 4.405895 | lr: 1.4859e-03 | norm: 0.4986 | time: 15349.85ms | tok/sec: 34155.90\r\n",
      "step:   664 | loss: 4.399849 | lr: 1.4881e-03 | norm: 0.5606 | time: 15408.00ms | tok/sec: 34026.99\r\n",
      "step:   665 | loss: 4.360063 | lr: 1.4903e-03 | norm: 0.5601 | time: 15350.06ms | tok/sec: 34155.43\r\n",
      "step:   666 | loss: 4.424513 | lr: 1.4926e-03 | norm: 0.4887 | time: 15309.14ms | tok/sec: 34246.73\r\n",
      "step:   667 | loss: 4.391454 | lr: 1.4948e-03 | norm: 0.4619 | time: 15340.35ms | tok/sec: 34177.06\r\n",
      "step:   668 | loss: 4.361958 | lr: 1.4971e-03 | norm: 0.4714 | time: 15326.14ms | tok/sec: 34208.75\r\n",
      "step:   669 | loss: 4.382426 | lr: 1.4993e-03 | norm: 0.4167 | time: 15316.41ms | tok/sec: 34230.48\r\n",
      "step:   670 | loss: 4.322006 | lr: 1.5015e-03 | norm: 0.4660 | time: 15340.06ms | tok/sec: 34177.69\r\n",
      "step:   671 | loss: 4.396466 | lr: 1.5038e-03 | norm: 0.4312 | time: 15376.89ms | tok/sec: 34095.83\r\n",
      "step:   672 | loss: 4.344048 | lr: 1.5060e-03 | norm: 0.4024 | time: 15422.06ms | tok/sec: 33995.99\r\n",
      "step:   673 | loss: 4.330416 | lr: 1.5083e-03 | norm: 0.3832 | time: 15450.66ms | tok/sec: 33933.05\r\n",
      "step:   674 | loss: 4.307902 | lr: 1.5105e-03 | norm: 0.3414 | time: 15393.65ms | tok/sec: 34058.72\r\n",
      "validation loss: 4.3083\r\n",
      "step:   675 | loss: 4.290106 | lr: 1.5127e-03 | norm: 0.3393 | time: 19110.38ms | tok/sec: 27434.72\r\n",
      "step:   676 | loss: 4.324393 | lr: 1.5150e-03 | norm: 0.3872 | time: 15471.18ms | tok/sec: 33888.05\r\n",
      "step:   677 | loss: 4.351122 | lr: 1.5172e-03 | norm: 0.4768 | time: 15432.61ms | tok/sec: 33972.74\r\n",
      "step:   678 | loss: 4.322627 | lr: 1.5194e-03 | norm: 0.4870 | time: 15429.40ms | tok/sec: 33979.81\r\n",
      "step:   679 | loss: 4.297279 | lr: 1.5217e-03 | norm: 0.4870 | time: 15406.82ms | tok/sec: 34029.61\r\n",
      "step:   680 | loss: 4.323921 | lr: 1.5239e-03 | norm: 0.5275 | time: 15443.17ms | tok/sec: 33949.51\r\n",
      "step:   681 | loss: 4.314409 | lr: 1.5262e-03 | norm: 0.5011 | time: 15488.29ms | tok/sec: 33850.61\r\n",
      "step:   682 | loss: 4.305925 | lr: 1.5284e-03 | norm: 0.4504 | time: 15466.32ms | tok/sec: 33898.70\r\n",
      "step:   683 | loss: 4.329261 | lr: 1.5306e-03 | norm: 0.5379 | time: 15478.46ms | tok/sec: 33872.11\r\n",
      "step:   684 | loss: 4.288395 | lr: 1.5329e-03 | norm: 0.5661 | time: 15473.79ms | tok/sec: 33882.33\r\n",
      "step:   685 | loss: 4.215574 | lr: 1.5351e-03 | norm: 0.4746 | time: 15446.75ms | tok/sec: 33941.63\r\n",
      "step:   686 | loss: 4.182365 | lr: 1.5373e-03 | norm: 0.4618 | time: 15526.05ms | tok/sec: 33768.28\r\n",
      "step:   687 | loss: 4.232045 | lr: 1.5396e-03 | norm: 0.4443 | time: 15452.40ms | tok/sec: 33929.23\r\n",
      "step:   688 | loss: 4.204924 | lr: 1.5418e-03 | norm: 0.4506 | time: 15394.47ms | tok/sec: 34056.90\r\n",
      "step:   689 | loss: 4.203127 | lr: 1.5441e-03 | norm: 0.4668 | time: 15422.51ms | tok/sec: 33994.99\r\n",
      "step:   690 | loss: 4.241939 | lr: 1.5463e-03 | norm: 0.5665 | time: 15452.23ms | tok/sec: 33929.61\r\n",
      "step:   691 | loss: 4.226930 | lr: 1.5485e-03 | norm: 0.5189 | time: 15409.79ms | tok/sec: 34023.04\r\n",
      "step:   692 | loss: 4.248964 | lr: 1.5508e-03 | norm: 0.4338 | time: 15436.01ms | tok/sec: 33965.25\r\n",
      "step:   693 | loss: 4.172254 | lr: 1.5530e-03 | norm: 0.4623 | time: 15412.78ms | tok/sec: 34016.45\r\n",
      "step:   694 | loss: 4.244939 | lr: 1.5552e-03 | norm: 0.4182 | time: 15446.93ms | tok/sec: 33941.24\r\n",
      "step:   695 | loss: 4.145580 | lr: 1.5575e-03 | norm: 0.3594 | time: 15426.70ms | tok/sec: 33985.74\r\n",
      "step:   696 | loss: 4.277244 | lr: 1.5597e-03 | norm: 0.3595 | time: 15460.61ms | tok/sec: 33911.20\r\n",
      "step:   697 | loss: 4.402058 | lr: 1.5620e-03 | norm: 0.4606 | time: 15514.55ms | tok/sec: 33793.30\r\n",
      "step:   698 | loss: 4.489050 | lr: 1.5642e-03 | norm: 0.6453 | time: 15429.31ms | tok/sec: 33979.99\r\n",
      "step:   699 | loss: 4.388592 | lr: 1.5664e-03 | norm: 0.6575 | time: 15488.36ms | tok/sec: 33850.44\r\n",
      "validation loss: 4.3207\r\n",
      "step:   700 | loss: 4.381564 | lr: 1.5687e-03 | norm: 0.5599 | time: 19151.07ms | tok/sec: 27376.43\r\n",
      "step:   701 | loss: 4.366345 | lr: 1.5709e-03 | norm: 0.5970 | time: 15418.34ms | tok/sec: 34004.19\r\n",
      "step:   702 | loss: 4.369378 | lr: 1.5731e-03 | norm: 0.6701 | time: 15485.27ms | tok/sec: 33857.21\r\n",
      "step:   703 | loss: 4.386729 | lr: 1.5754e-03 | norm: 0.6409 | time: 15465.40ms | tok/sec: 33900.70\r\n",
      "step:   704 | loss: 4.404859 | lr: 1.5776e-03 | norm: 0.6131 | time: 15472.15ms | tok/sec: 33885.91\r\n",
      "step:   705 | loss: 4.355967 | lr: 1.5799e-03 | norm: 0.6644 | time: 15409.13ms | tok/sec: 34024.51\r\n",
      "step:   706 | loss: 4.370683 | lr: 1.5821e-03 | norm: 0.5858 | time: 15423.23ms | tok/sec: 33993.40\r\n",
      "step:   707 | loss: 4.410337 | lr: 1.5843e-03 | norm: 0.5742 | time: 15444.87ms | tok/sec: 33945.76\r\n",
      "step:   708 | loss: 4.396419 | lr: 1.5866e-03 | norm: 0.5337 | time: 15411.26ms | tok/sec: 34019.81\r\n",
      "step:   709 | loss: 4.430556 | lr: 1.5888e-03 | norm: 0.5382 | time: 15377.77ms | tok/sec: 34093.89\r\n",
      "step:   710 | loss: 4.349690 | lr: 1.5910e-03 | norm: 0.5417 | time: 15393.01ms | tok/sec: 34060.14\r\n",
      "step:   711 | loss: 4.343763 | lr: 1.5933e-03 | norm: 0.5430 | time: 15309.49ms | tok/sec: 34245.95\r\n",
      "step:   712 | loss: 4.319341 | lr: 1.5955e-03 | norm: 0.4551 | time: 15366.58ms | tok/sec: 34118.73\r\n",
      "step:   713 | loss: 4.304806 | lr: 1.5978e-03 | norm: 0.3917 | time: 15325.55ms | tok/sec: 34210.07\r\n",
      "step:   714 | loss: 4.272633 | lr: 1.6000e-03 | norm: 0.4441 | time: 15340.89ms | tok/sec: 34175.85\r\n",
      "step:   715 | loss: 4.382084 | lr: 1.6000e-03 | norm: 0.4784 | time: 15406.33ms | tok/sec: 34030.69\r\n",
      "step:   716 | loss: 4.334365 | lr: 1.6000e-03 | norm: 0.5623 | time: 15411.59ms | tok/sec: 34019.07\r\n",
      "step:   717 | loss: 4.339492 | lr: 1.6000e-03 | norm: 0.6410 | time: 15408.15ms | tok/sec: 34026.67\r\n",
      "step:   718 | loss: 4.406171 | lr: 1.6000e-03 | norm: 0.6340 | time: 15459.11ms | tok/sec: 33914.49\r\n",
      "step:   719 | loss: 4.343176 | lr: 1.6000e-03 | norm: 0.5863 | time: 15448.11ms | tok/sec: 33938.65\r\n",
      "step:   720 | loss: 4.343604 | lr: 1.6000e-03 | norm: 0.5504 | time: 15385.91ms | tok/sec: 34075.86\r\n",
      "step:   721 | loss: 4.220683 | lr: 1.6000e-03 | norm: 0.4374 | time: 15421.06ms | tok/sec: 33998.18\r\n",
      "step:   722 | loss: 4.270269 | lr: 1.6000e-03 | norm: 0.4077 | time: 15382.99ms | tok/sec: 34082.32\r\n",
      "step:   723 | loss: 4.256509 | lr: 1.6000e-03 | norm: 0.4187 | time: 15491.94ms | tok/sec: 33842.63\r\n",
      "step:   724 | loss: 4.257936 | lr: 1.6000e-03 | norm: 0.3644 | time: 15445.09ms | tok/sec: 33945.28\r\n",
      "validation loss: 4.2408\r\n",
      "step:   725 | loss: 4.291833 | lr: 1.6000e-03 | norm: 0.3496 | time: 19146.00ms | tok/sec: 27383.69\r\n",
      "step:   726 | loss: 4.288708 | lr: 1.6000e-03 | norm: 0.3827 | time: 15398.59ms | tok/sec: 34047.78\r\n",
      "step:   727 | loss: 4.233163 | lr: 1.6000e-03 | norm: 0.4752 | time: 15397.90ms | tok/sec: 34049.31\r\n",
      "step:   728 | loss: 4.226079 | lr: 1.6000e-03 | norm: 0.5081 | time: 15412.99ms | tok/sec: 34015.99\r\n",
      "step:   729 | loss: 4.228951 | lr: 1.6000e-03 | norm: 0.5088 | time: 15458.41ms | tok/sec: 33916.04\r\n",
      "step:   730 | loss: 4.234246 | lr: 1.6000e-03 | norm: 0.4801 | time: 15431.48ms | tok/sec: 33975.23\r\n",
      "step:   731 | loss: 4.241899 | lr: 1.6000e-03 | norm: 0.4034 | time: 15363.11ms | tok/sec: 34126.43\r\n",
      "step:   732 | loss: 4.138561 | lr: 1.6000e-03 | norm: 0.3354 | time: 15305.34ms | tok/sec: 34255.23\r\n",
      "step:   733 | loss: 4.121858 | lr: 1.6000e-03 | norm: 0.3430 | time: 15385.08ms | tok/sec: 34077.70\r\n",
      "step:   734 | loss: 4.107888 | lr: 1.6000e-03 | norm: 0.3731 | time: 15328.42ms | tok/sec: 34203.66\r\n",
      "step:   735 | loss: 4.104090 | lr: 1.6000e-03 | norm: 0.4153 | time: 15259.84ms | tok/sec: 34357.38\r\n",
      "step:   736 | loss: 4.147942 | lr: 1.6000e-03 | norm: 0.4392 | time: 15277.17ms | tok/sec: 34318.40\r\n",
      "step:   737 | loss: 4.098152 | lr: 1.6000e-03 | norm: 0.3718 | time: 15308.11ms | tok/sec: 34249.04\r\n",
      "step:   738 | loss: 4.111204 | lr: 1.6000e-03 | norm: 0.3277 | time: 15324.77ms | tok/sec: 34211.80\r\n",
      "step:   739 | loss: 4.134443 | lr: 1.6000e-03 | norm: 0.3754 | time: 15335.12ms | tok/sec: 34188.72\r\n",
      "step:   740 | loss: 4.072321 | lr: 1.6000e-03 | norm: 0.3450 | time: 15277.72ms | tok/sec: 34317.17\r\n",
      "step:   741 | loss: 4.086334 | lr: 1.6000e-03 | norm: 0.3393 | time: 15363.18ms | tok/sec: 34126.28\r\n",
      "step:   742 | loss: 4.053841 | lr: 1.6000e-03 | norm: 0.3615 | time: 15372.07ms | tok/sec: 34106.53\r\n",
      "step:   743 | loss: 4.086576 | lr: 1.6000e-03 | norm: 0.3692 | time: 15387.26ms | tok/sec: 34072.87\r\n",
      "step:   744 | loss: 4.258561 | lr: 1.6000e-03 | norm: 0.4275 | time: 15402.93ms | tok/sec: 34038.20\r\n",
      "step:   745 | loss: 4.224591 | lr: 1.6000e-03 | norm: 0.4481 | time: 15383.51ms | tok/sec: 34081.18\r\n",
      "step:   746 | loss: 4.281605 | lr: 1.6000e-03 | norm: 0.5090 | time: 15406.90ms | tok/sec: 34029.42\r\n",
      "step:   747 | loss: 4.314486 | lr: 1.6000e-03 | norm: 0.6105 | time: 15422.23ms | tok/sec: 33995.60\r\n",
      "step:   748 | loss: 4.263027 | lr: 1.6000e-03 | norm: 0.4340 | time: 15363.99ms | tok/sec: 34124.48\r\n",
      "step:   749 | loss: 4.224961 | lr: 1.6000e-03 | norm: 0.4304 | time: 15377.03ms | tok/sec: 34095.52\r\n",
      "validation loss: 4.1945\r\n",
      "step:   750 | loss: 4.282498 | lr: 1.6000e-03 | norm: 0.4220 | time: 19155.46ms | tok/sec: 27370.17\r\n",
      "step:   751 | loss: 4.347168 | lr: 1.6000e-03 | norm: 0.3876 | time: 15390.33ms | tok/sec: 34066.06\r\n",
      "step:   752 | loss: 4.250746 | lr: 1.6000e-03 | norm: 0.4911 | time: 15369.85ms | tok/sec: 34111.46\r\n",
      "step:   753 | loss: 4.324776 | lr: 1.6000e-03 | norm: 0.5437 | time: 15404.01ms | tok/sec: 34035.80\r\n",
      "step:   754 | loss: 4.268991 | lr: 1.6000e-03 | norm: 0.4754 | time: 15404.61ms | tok/sec: 34034.48\r\n",
      "step:   755 | loss: 4.256018 | lr: 1.6000e-03 | norm: 0.4333 | time: 15469.43ms | tok/sec: 33891.87\r\n",
      "step:   756 | loss: 4.251656 | lr: 1.6000e-03 | norm: 0.4099 | time: 15433.03ms | tok/sec: 33971.82\r\n",
      "step:   757 | loss: 4.221759 | lr: 1.6000e-03 | norm: 0.4061 | time: 15465.48ms | tok/sec: 33900.53\r\n",
      "step:   758 | loss: 4.240117 | lr: 1.6000e-03 | norm: 0.4133 | time: 15448.17ms | tok/sec: 33938.51\r\n",
      "step:   759 | loss: 4.299838 | lr: 1.6000e-03 | norm: 0.4444 | time: 15452.53ms | tok/sec: 33928.94\r\n",
      "step:   760 | loss: 4.216335 | lr: 1.6000e-03 | norm: 0.4615 | time: 15427.40ms | tok/sec: 33984.22\r\n",
      "step:   761 | loss: 4.280710 | lr: 1.6000e-03 | norm: 0.3966 | time: 15434.19ms | tok/sec: 33969.25\r\n",
      "step:   762 | loss: 4.280001 | lr: 1.6000e-03 | norm: 0.3463 | time: 18344.31ms | tok/sec: 28580.41\r\n",
      "step:   763 | loss: 4.209755 | lr: 1.6000e-03 | norm: 0.3635 | time: 15442.51ms | tok/sec: 33950.97\r\n",
      "step:   764 | loss: 4.222193 | lr: 1.6000e-03 | norm: 0.3802 | time: 15549.13ms | tok/sec: 33718.16\r\n",
      "step:   765 | loss: 4.188608 | lr: 1.6000e-03 | norm: 0.4782 | time: 15513.37ms | tok/sec: 33795.89\r\n",
      "step:   766 | loss: 4.253664 | lr: 1.6000e-03 | norm: 0.3944 | time: 15397.61ms | tok/sec: 34049.95\r\n",
      "step:   767 | loss: 4.183788 | lr: 1.6000e-03 | norm: 0.3470 | time: 15399.59ms | tok/sec: 34045.58\r\n",
      "step:   768 | loss: 4.153296 | lr: 1.6000e-03 | norm: 0.3542 | time: 15329.85ms | tok/sec: 34200.46\r\n",
      "step:   769 | loss: 4.143538 | lr: 1.6000e-03 | norm: 0.3487 | time: 15327.34ms | tok/sec: 34206.07\r\n",
      "step:   770 | loss: 4.191381 | lr: 1.6000e-03 | norm: 0.3858 | time: 15330.25ms | tok/sec: 34199.58\r\n",
      "step:   771 | loss: 4.163651 | lr: 1.6000e-03 | norm: 0.4312 | time: 15324.54ms | tok/sec: 34212.31\r\n",
      "step:   772 | loss: 4.209388 | lr: 1.6000e-03 | norm: 0.4367 | time: 15347.29ms | tok/sec: 34161.61\r\n",
      "step:   773 | loss: 4.190762 | lr: 1.6000e-03 | norm: 0.4761 | time: 15308.71ms | tok/sec: 34247.69\r\n",
      "step:   774 | loss: 4.205268 | lr: 1.6000e-03 | norm: 0.4566 | time: 15355.56ms | tok/sec: 34143.21\r\n",
      "validation loss: 4.1498\r\n",
      "step:   775 | loss: 4.142649 | lr: 1.6000e-03 | norm: 0.4259 | time: 19041.19ms | tok/sec: 27534.42\r\n",
      "step:   776 | loss: 4.149793 | lr: 1.6000e-03 | norm: 0.4166 | time: 15343.39ms | tok/sec: 34170.29\r\n",
      "step:   777 | loss: 4.139793 | lr: 1.6000e-03 | norm: 0.4162 | time: 15350.89ms | tok/sec: 34153.60\r\n",
      "step:   778 | loss: 4.080556 | lr: 1.6000e-03 | norm: 0.3581 | time: 15332.27ms | tok/sec: 34195.07\r\n",
      "step:   779 | loss: 4.159998 | lr: 1.6000e-03 | norm: 0.3802 | time: 15376.00ms | tok/sec: 34097.83\r\n",
      "step:   780 | loss: 4.066231 | lr: 1.6000e-03 | norm: 0.3876 | time: 15392.40ms | tok/sec: 34061.49\r\n",
      "step:   781 | loss: 4.052665 | lr: 1.6000e-03 | norm: 0.3546 | time: 15402.14ms | tok/sec: 34039.94\r\n",
      "step:   782 | loss: 3.998455 | lr: 1.6000e-03 | norm: 0.3796 | time: 15392.88ms | tok/sec: 34060.42\r\n",
      "step:   783 | loss: 4.009890 | lr: 1.6000e-03 | norm: 0.4060 | time: 15377.11ms | tok/sec: 34095.36\r\n",
      "step:   784 | loss: 4.032248 | lr: 1.6000e-03 | norm: 0.3898 | time: 15472.77ms | tok/sec: 33884.57\r\n",
      "step:   785 | loss: 4.128256 | lr: 1.6000e-03 | norm: 0.3791 | time: 15469.89ms | tok/sec: 33890.87\r\n",
      "step:   786 | loss: 4.018212 | lr: 1.6000e-03 | norm: 0.3714 | time: 15471.22ms | tok/sec: 33887.95\r\n",
      "step:   787 | loss: 4.083114 | lr: 1.5925e-03 | norm: 0.3969 | time: 15458.18ms | tok/sec: 33916.55\r\n",
      "step:   788 | loss: 4.036232 | lr: 1.5850e-03 | norm: 0.3853 | time: 15526.31ms | tok/sec: 33767.71\r\n",
      "step:   789 | loss: 4.063103 | lr: 1.5776e-03 | norm: 0.3681 | time: 15534.02ms | tok/sec: 33750.95\r\n",
      "step:   790 | loss: 4.167817 | lr: 1.5701e-03 | norm: 0.4386 | time: 15483.63ms | tok/sec: 33860.80\r\n",
      "step:   791 | loss: 4.177006 | lr: 1.5626e-03 | norm: 0.4905 | time: 15529.19ms | tok/sec: 33761.45\r\n",
      "step:   792 | loss: 4.200598 | lr: 1.5551e-03 | norm: 0.4841 | time: 15500.84ms | tok/sec: 33823.19\r\n",
      "step:   793 | loss: 4.214452 | lr: 1.5477e-03 | norm: 0.4560 | time: 15525.29ms | tok/sec: 33769.94\r\n",
      "step:   794 | loss: 4.181435 | lr: 1.5402e-03 | norm: 0.4019 | time: 15454.05ms | tok/sec: 33925.60\r\n",
      "step:   795 | loss: 4.209125 | lr: 1.5327e-03 | norm: 0.4095 | time: 15444.56ms | tok/sec: 33946.45\r\n",
      "step:   796 | loss: 4.235661 | lr: 1.5252e-03 | norm: 0.3589 | time: 15421.63ms | tok/sec: 33996.92\r\n",
      "step:   797 | loss: 4.146311 | lr: 1.5178e-03 | norm: 0.3396 | time: 15490.42ms | tok/sec: 33845.95\r\n",
      "step:   798 | loss: 4.165730 | lr: 1.5103e-03 | norm: 0.3095 | time: 15449.61ms | tok/sec: 33935.36\r\n",
      "step:   799 | loss: 4.142889 | lr: 1.5028e-03 | norm: 0.3067 | time: 15409.84ms | tok/sec: 34022.93\r\n",
      "validation loss: 4.0932\r\n",
      "step:   800 | loss: 4.175678 | lr: 1.4953e-03 | norm: 0.2878 | time: 19057.01ms | tok/sec: 27511.56\r\n",
      "step:   801 | loss: 4.129187 | lr: 1.4879e-03 | norm: 0.3080 | time: 15390.89ms | tok/sec: 34064.83\r\n",
      "step:   802 | loss: 4.181173 | lr: 1.4804e-03 | norm: 0.3061 | time: 15366.90ms | tok/sec: 34118.02\r\n",
      "step:   803 | loss: 4.169999 | lr: 1.4729e-03 | norm: 0.3349 | time: 15374.37ms | tok/sec: 34101.42\r\n",
      "step:   804 | loss: 4.173614 | lr: 1.4654e-03 | norm: 0.3835 | time: 15373.39ms | tok/sec: 34103.61\r\n",
      "step:   805 | loss: 4.137441 | lr: 1.4579e-03 | norm: 0.3370 | time: 15470.65ms | tok/sec: 33889.21\r\n",
      "step:   806 | loss: 4.122051 | lr: 1.4505e-03 | norm: 0.3120 | time: 15394.42ms | tok/sec: 34057.02\r\n",
      "step:   807 | loss: 4.131532 | lr: 1.4430e-03 | norm: 0.3154 | time: 15460.59ms | tok/sec: 33911.26\r\n",
      "step:   808 | loss: 4.170362 | lr: 1.4355e-03 | norm: 0.3516 | time: 15411.82ms | tok/sec: 34018.56\r\n",
      "step:   809 | loss: 4.134429 | lr: 1.4280e-03 | norm: 0.3826 | time: 15426.19ms | tok/sec: 33986.89\r\n",
      "step:   810 | loss: 4.130851 | lr: 1.4206e-03 | norm: 0.3731 | time: 15442.71ms | tok/sec: 33950.51\r\n",
      "step:   811 | loss: 4.081237 | lr: 1.4131e-03 | norm: 0.3609 | time: 15431.45ms | tok/sec: 33975.29\r\n",
      "step:   812 | loss: 4.100044 | lr: 1.4056e-03 | norm: 0.3658 | time: 15448.78ms | tok/sec: 33937.17\r\n",
      "step:   813 | loss: 4.086760 | lr: 1.3981e-03 | norm: 0.2943 | time: 15416.44ms | tok/sec: 34008.36\r\n",
      "step:   814 | loss: 4.090904 | lr: 1.3907e-03 | norm: 0.3016 | time: 15450.77ms | tok/sec: 33932.81\r\n",
      "step:   815 | loss: 4.061543 | lr: 1.3832e-03 | norm: 0.2812 | time: 15389.11ms | tok/sec: 34068.76\r\n",
      "step:   816 | loss: 4.038477 | lr: 1.3757e-03 | norm: 0.2777 | time: 15430.11ms | tok/sec: 33978.25\r\n",
      "step:   817 | loss: 4.023367 | lr: 1.3682e-03 | norm: 0.2887 | time: 15410.17ms | tok/sec: 34022.20\r\n",
      "step:   818 | loss: 4.068449 | lr: 1.3607e-03 | norm: 0.3091 | time: 15439.06ms | tok/sec: 33958.55\r\n",
      "step:   819 | loss: 4.074846 | lr: 1.3533e-03 | norm: 0.2766 | time: 15449.22ms | tok/sec: 33936.22\r\n",
      "step:   820 | loss: 4.038036 | lr: 1.3458e-03 | norm: 0.2606 | time: 15468.68ms | tok/sec: 33893.51\r\n",
      "step:   821 | loss: 4.033175 | lr: 1.3383e-03 | norm: 0.2779 | time: 15432.63ms | tok/sec: 33972.68\r\n",
      "step:   822 | loss: 4.108437 | lr: 1.3308e-03 | norm: 0.2946 | time: 15445.90ms | tok/sec: 33943.51\r\n",
      "step:   823 | loss: 4.043228 | lr: 1.3234e-03 | norm: 0.2986 | time: 15443.84ms | tok/sec: 33948.03\r\n",
      "step:   824 | loss: 3.907938 | lr: 1.3159e-03 | norm: 0.3146 | time: 15408.77ms | tok/sec: 34025.31\r\n",
      "validation loss: 4.0274\r\n",
      "step:   825 | loss: 3.906162 | lr: 1.3084e-03 | norm: 0.2649 | time: 19097.52ms | tok/sec: 27453.20\r\n",
      "step:   826 | loss: 3.965415 | lr: 1.3009e-03 | norm: 0.2631 | time: 15369.00ms | tok/sec: 34113.35\r\n",
      "step:   827 | loss: 3.944089 | lr: 1.2935e-03 | norm: 0.3080 | time: 15402.45ms | tok/sec: 34039.27\r\n",
      "step:   828 | loss: 3.965744 | lr: 1.2860e-03 | norm: 0.2896 | time: 15430.53ms | tok/sec: 33977.33\r\n",
      "step:   829 | loss: 3.917079 | lr: 1.2785e-03 | norm: 0.3075 | time: 15404.60ms | tok/sec: 34034.50\r\n",
      "step:   830 | loss: 3.913768 | lr: 1.2710e-03 | norm: 0.3261 | time: 15435.41ms | tok/sec: 33966.58\r\n",
      "step:   831 | loss: 3.932079 | lr: 1.2636e-03 | norm: 0.3348 | time: 15396.25ms | tok/sec: 34052.97\r\n",
      "step:   832 | loss: 3.816520 | lr: 1.2561e-03 | norm: 0.3227 | time: 15408.25ms | tok/sec: 34026.45\r\n",
      "step:   833 | loss: 3.895059 | lr: 1.2486e-03 | norm: 0.2939 | time: 15365.42ms | tok/sec: 34121.30\r\n",
      "step:   834 | loss: 3.877084 | lr: 1.2411e-03 | norm: 0.2576 | time: 15337.43ms | tok/sec: 34183.56\r\n",
      "step:   835 | loss: 4.042904 | lr: 1.2336e-03 | norm: 0.2854 | time: 15373.42ms | tok/sec: 34103.54\r\n",
      "step:   836 | loss: 4.043006 | lr: 1.2262e-03 | norm: 0.3100 | time: 15423.42ms | tok/sec: 33992.98\r\n",
      "step:   837 | loss: 4.030333 | lr: 1.2187e-03 | norm: 0.3075 | time: 15409.78ms | tok/sec: 34023.06\r\n",
      "step:   838 | loss: 4.094111 | lr: 1.2112e-03 | norm: 0.3077 | time: 15383.45ms | tok/sec: 34081.30\r\n",
      "step:   839 | loss: 4.066049 | lr: 1.2037e-03 | norm: 0.3069 | time: 15359.06ms | tok/sec: 34135.42\r\n",
      "step:   840 | loss: 4.040466 | lr: 1.1963e-03 | norm: 0.3107 | time: 15436.35ms | tok/sec: 33964.51\r\n",
      "step:   841 | loss: 4.115170 | lr: 1.1888e-03 | norm: 0.2528 | time: 15448.96ms | tok/sec: 33936.78\r\n",
      "step:   842 | loss: 4.073647 | lr: 1.1813e-03 | norm: 0.2518 | time: 15429.58ms | tok/sec: 33979.42\r\n",
      "step:   843 | loss: 4.026800 | lr: 1.1738e-03 | norm: 0.2457 | time: 15383.38ms | tok/sec: 34081.45\r\n",
      "step:   844 | loss: 4.097880 | lr: 1.1664e-03 | norm: 0.3440 | time: 15393.08ms | tok/sec: 34059.99\r\n",
      "step:   845 | loss: 4.074426 | lr: 1.1589e-03 | norm: 0.2905 | time: 15334.51ms | tok/sec: 34190.08\r\n",
      "step:   846 | loss: 4.082742 | lr: 1.1514e-03 | norm: 0.2993 | time: 15322.83ms | tok/sec: 34216.13\r\n",
      "step:   847 | loss: 4.052977 | lr: 1.1439e-03 | norm: 0.3325 | time: 15316.19ms | tok/sec: 34230.97\r\n",
      "step:   848 | loss: 4.106401 | lr: 1.1364e-03 | norm: 0.3118 | time: 15245.18ms | tok/sec: 34390.42\r\n",
      "step:   849 | loss: 4.064616 | lr: 1.1290e-03 | norm: 0.2841 | time: 15307.43ms | tok/sec: 34250.56\r\n",
      "validation loss: 3.9779\r\n",
      "step:   850 | loss: 4.043803 | lr: 1.1215e-03 | norm: 0.2612 | time: 19067.05ms | tok/sec: 27497.08\r\n",
      "step:   851 | loss: 4.026970 | lr: 1.1140e-03 | norm: 0.2400 | time: 15380.95ms | tok/sec: 34086.84\r\n",
      "step:   852 | loss: 4.083551 | lr: 1.1065e-03 | norm: 0.2550 | time: 15367.21ms | tok/sec: 34117.31\r\n",
      "step:   853 | loss: 4.077272 | lr: 1.0991e-03 | norm: 0.2575 | time: 15374.54ms | tok/sec: 34101.06\r\n",
      "step:   854 | loss: 4.012907 | lr: 1.0916e-03 | norm: 0.2446 | time: 15384.11ms | tok/sec: 34079.85\r\n",
      "step:   855 | loss: 4.036628 | lr: 1.0841e-03 | norm: 0.2331 | time: 15351.08ms | tok/sec: 34153.17\r\n",
      "step:   856 | loss: 4.037852 | lr: 1.0766e-03 | norm: 0.2408 | time: 15376.73ms | tok/sec: 34096.19\r\n",
      "step:   857 | loss: 4.079343 | lr: 1.0692e-03 | norm: 0.2563 | time: 15412.68ms | tok/sec: 34016.67\r\n",
      "step:   858 | loss: 3.953500 | lr: 1.0617e-03 | norm: 0.2674 | time: 15411.58ms | tok/sec: 34019.09\r\n",
      "step:   859 | loss: 3.992704 | lr: 1.0542e-03 | norm: 0.2575 | time: 15384.99ms | tok/sec: 34077.89\r\n",
      "step:   860 | loss: 3.971755 | lr: 1.0467e-03 | norm: 0.2707 | time: 15350.01ms | tok/sec: 34155.54\r\n",
      "step:   861 | loss: 3.956861 | lr: 1.0393e-03 | norm: 0.2501 | time: 15323.68ms | tok/sec: 34214.24\r\n",
      "step:   862 | loss: 3.993675 | lr: 1.0318e-03 | norm: 0.2524 | time: 15328.66ms | tok/sec: 34203.12\r\n",
      "step:   863 | loss: 3.991905 | lr: 1.0243e-03 | norm: 0.2735 | time: 15347.52ms | tok/sec: 34161.09\r\n",
      "step:   864 | loss: 3.926434 | lr: 1.0168e-03 | norm: 0.2935 | time: 15314.85ms | tok/sec: 34233.96\r\n",
      "step:   865 | loss: 3.975905 | lr: 1.0093e-03 | norm: 0.2307 | time: 15352.84ms | tok/sec: 34149.25\r\n",
      "step:   866 | loss: 3.930438 | lr: 1.0019e-03 | norm: 0.2429 | time: 15332.20ms | tok/sec: 34195.23\r\n",
      "step:   867 | loss: 3.974473 | lr: 9.9439e-04 | norm: 0.2554 | time: 15385.11ms | tok/sec: 34077.62\r\n",
      "step:   868 | loss: 3.904867 | lr: 9.8692e-04 | norm: 0.2311 | time: 15409.50ms | tok/sec: 34023.69\r\n",
      "step:   869 | loss: 3.842983 | lr: 9.7944e-04 | norm: 0.2158 | time: 15412.42ms | tok/sec: 34017.25\r\n",
      "step:   870 | loss: 3.880826 | lr: 9.7196e-04 | norm: 0.2423 | time: 15359.39ms | tok/sec: 34134.69\r\n",
      "step:   871 | loss: 3.817299 | lr: 9.6449e-04 | norm: 0.2213 | time: 15401.56ms | tok/sec: 34041.22\r\n",
      "step:   872 | loss: 3.841312 | lr: 9.5701e-04 | norm: 0.2566 | time: 15410.13ms | tok/sec: 34022.30\r\n",
      "step:   873 | loss: 3.861002 | lr: 9.4953e-04 | norm: 0.2788 | time: 15411.56ms | tok/sec: 34019.14\r\n",
      "step:   874 | loss: 3.850456 | lr: 9.4206e-04 | norm: 0.2814 | time: 15398.89ms | tok/sec: 34047.12\r\n",
      "validation loss: 3.9364\r\n",
      "step:   875 | loss: 3.769118 | lr: 9.3458e-04 | norm: 0.2613 | time: 19061.70ms | tok/sec: 27504.78\r\n",
      "step:   876 | loss: 3.760180 | lr: 9.2710e-04 | norm: 0.2460 | time: 15377.22ms | tok/sec: 34095.11\r\n",
      "step:   877 | loss: 3.893626 | lr: 9.1963e-04 | norm: 0.2498 | time: 15399.67ms | tok/sec: 34045.40\r\n",
      "step:   878 | loss: 3.826885 | lr: 9.1215e-04 | norm: 0.2378 | time: 15340.48ms | tok/sec: 34176.77\r\n",
      "step:   879 | loss: 3.858684 | lr: 9.0467e-04 | norm: 0.2160 | time: 15389.67ms | tok/sec: 34067.52\r\n",
      "step:   880 | loss: 3.864044 | lr: 8.9720e-04 | norm: 0.2843 | time: 15407.77ms | tok/sec: 34027.51\r\n",
      "step:   881 | loss: 3.910684 | lr: 8.8972e-04 | norm: 0.2671 | time: 15406.42ms | tok/sec: 34030.48\r\n",
      "step:   882 | loss: 4.077504 | lr: 8.8224e-04 | norm: 0.2650 | time: 15430.82ms | tok/sec: 33976.68\r\n",
      "step:   883 | loss: 4.063991 | lr: 8.7477e-04 | norm: 0.2631 | time: 15378.49ms | tok/sec: 34092.29\r\n",
      "step:   884 | loss: 3.972564 | lr: 8.6729e-04 | norm: 0.2520 | time: 15409.10ms | tok/sec: 34024.57\r\n",
      "step:   885 | loss: 4.029304 | lr: 8.5981e-04 | norm: 0.2423 | time: 15423.51ms | tok/sec: 33992.78\r\n",
      "step:   886 | loss: 3.942030 | lr: 8.5234e-04 | norm: 0.2329 | time: 15415.49ms | tok/sec: 34010.46\r\n",
      "step:   887 | loss: 3.983513 | lr: 8.4486e-04 | norm: 0.2311 | time: 15419.36ms | tok/sec: 34001.93\r\n",
      "step:   888 | loss: 3.986987 | lr: 8.3738e-04 | norm: 0.2253 | time: 15468.31ms | tok/sec: 33894.33\r\n",
      "step:   889 | loss: 3.988199 | lr: 8.2991e-04 | norm: 0.2442 | time: 15448.86ms | tok/sec: 33937.00\r\n",
      "step:   890 | loss: 3.975620 | lr: 8.2243e-04 | norm: 0.2453 | time: 15408.92ms | tok/sec: 34024.96\r\n",
      "step:   891 | loss: 3.957067 | lr: 8.1495e-04 | norm: 0.2334 | time: 15413.90ms | tok/sec: 34013.97\r\n",
      "step:   892 | loss: 3.992992 | lr: 8.0748e-04 | norm: 0.2167 | time: 15416.13ms | tok/sec: 34009.05\r\n",
      "step:   893 | loss: 3.935741 | lr: 8.0000e-04 | norm: 0.2228 | time: 15447.25ms | tok/sec: 33940.53\r\n",
      "step:   894 | loss: 3.949283 | lr: 7.9252e-04 | norm: 0.2160 | time: 15441.69ms | tok/sec: 33952.75\r\n",
      "step:   895 | loss: 3.969348 | lr: 7.8505e-04 | norm: 0.2117 | time: 15443.57ms | tok/sec: 33948.62\r\n",
      "step:   896 | loss: 3.932281 | lr: 7.7757e-04 | norm: 0.2123 | time: 15436.95ms | tok/sec: 33963.19\r\n",
      "step:   897 | loss: 3.930928 | lr: 7.7009e-04 | norm: 0.2049 | time: 15405.53ms | tok/sec: 34032.46\r\n",
      "step:   898 | loss: 3.949115 | lr: 7.6262e-04 | norm: 0.2180 | time: 15347.11ms | tok/sec: 34162.00\r\n",
      "step:   899 | loss: 3.925369 | lr: 7.5514e-04 | norm: 0.2121 | time: 15346.48ms | tok/sec: 34163.41\r\n",
      "validation loss: 3.8891\r\n",
      "step:   900 | loss: 3.940695 | lr: 7.4766e-04 | norm: 0.2100 | time: 19133.32ms | tok/sec: 27401.83\r\n",
      "step:   901 | loss: 3.922328 | lr: 7.4019e-04 | norm: 0.1859 | time: 15332.47ms | tok/sec: 34194.63\r\n",
      "step:   902 | loss: 3.948172 | lr: 7.3271e-04 | norm: 0.2206 | time: 15318.65ms | tok/sec: 34225.47\r\n",
      "step:   903 | loss: 3.922728 | lr: 7.2523e-04 | norm: 0.2074 | time: 15356.67ms | tok/sec: 34140.75\r\n",
      "step:   904 | loss: 3.850219 | lr: 7.1776e-04 | norm: 0.2128 | time: 15394.06ms | tok/sec: 34057.81\r\n",
      "step:   905 | loss: 3.938529 | lr: 7.1028e-04 | norm: 0.2161 | time: 15409.21ms | tok/sec: 34024.34\r\n",
      "step:   906 | loss: 3.879326 | lr: 7.0280e-04 | norm: 0.2075 | time: 15401.11ms | tok/sec: 34042.21\r\n",
      "step:   907 | loss: 3.860652 | lr: 6.9533e-04 | norm: 0.2070 | time: 15430.80ms | tok/sec: 33976.73\r\n",
      "step:   908 | loss: 3.871348 | lr: 6.8785e-04 | norm: 0.2070 | time: 15437.79ms | tok/sec: 33961.33\r\n",
      "step:   909 | loss: 3.890824 | lr: 6.8037e-04 | norm: 0.2099 | time: 15454.70ms | tok/sec: 33924.18\r\n",
      "step:   910 | loss: 3.928000 | lr: 6.7290e-04 | norm: 0.1895 | time: 15427.11ms | tok/sec: 33984.86\r\n",
      "step:   911 | loss: 3.900269 | lr: 6.6542e-04 | norm: 0.2094 | time: 15484.68ms | tok/sec: 33858.51\r\n",
      "step:   912 | loss: 3.917820 | lr: 6.5794e-04 | norm: 0.2153 | time: 15490.59ms | tok/sec: 33845.58\r\n",
      "step:   913 | loss: 3.904742 | lr: 6.5047e-04 | norm: 0.2236 | time: 15485.33ms | tok/sec: 33857.08\r\n",
      "step:   914 | loss: 3.901468 | lr: 6.4299e-04 | norm: 0.2018 | time: 15448.88ms | tok/sec: 33936.95\r\n",
      "step:   915 | loss: 3.732426 | lr: 6.3551e-04 | norm: 0.2174 | time: 15445.79ms | tok/sec: 33943.75\r\n",
      "step:   916 | loss: 3.775751 | lr: 6.2804e-04 | norm: 0.2187 | time: 15470.53ms | tok/sec: 33889.46\r\n",
      "step:   917 | loss: 3.792384 | lr: 6.2056e-04 | norm: 0.2272 | time: 15462.30ms | tok/sec: 33907.50\r\n",
      "step:   918 | loss: 3.770441 | lr: 6.1308e-04 | norm: 0.2014 | time: 15462.21ms | tok/sec: 33907.71\r\n",
      "step:   919 | loss: 3.757578 | lr: 6.0561e-04 | norm: 0.2163 | time: 15424.09ms | tok/sec: 33991.50\r\n",
      "step:   920 | loss: 3.722441 | lr: 5.9813e-04 | norm: 0.2092 | time: 15403.41ms | tok/sec: 34037.13\r\n",
      "step:   921 | loss: 3.754131 | lr: 5.9065e-04 | norm: 0.2214 | time: 15328.81ms | tok/sec: 34202.78\r\n",
      "step:   922 | loss: 3.789273 | lr: 5.8318e-04 | norm: 0.2208 | time: 15269.84ms | tok/sec: 34334.88\r\n",
      "step:   923 | loss: 3.779947 | lr: 5.7570e-04 | norm: 0.2138 | time: 15340.65ms | tok/sec: 34176.38\r\n",
      "step:   924 | loss: 3.764073 | lr: 5.6822e-04 | norm: 0.2003 | time: 15291.21ms | tok/sec: 34286.89\r\n",
      "validation loss: 3.8576\r\n",
      "step:   925 | loss: 3.779939 | lr: 5.6075e-04 | norm: 0.2025 | time: 18962.52ms | tok/sec: 27648.65\r\n",
      "step:   926 | loss: 3.800107 | lr: 5.5327e-04 | norm: 0.2089 | time: 15326.62ms | tok/sec: 34207.67\r\n",
      "step:   927 | loss: 3.939822 | lr: 5.4579e-04 | norm: 0.2057 | time: 15329.70ms | tok/sec: 34200.81\r\n",
      "step:   928 | loss: 3.957825 | lr: 5.3832e-04 | norm: 0.2137 | time: 15370.86ms | tok/sec: 34109.22\r\n",
      "step:   929 | loss: 3.885547 | lr: 5.3084e-04 | norm: 0.2015 | time: 15387.20ms | tok/sec: 34073.00\r\n",
      "step:   930 | loss: 3.880375 | lr: 5.2336e-04 | norm: 0.1933 | time: 15376.53ms | tok/sec: 34096.63\r\n",
      "step:   931 | loss: 3.921330 | lr: 5.1589e-04 | norm: 0.2168 | time: 15420.86ms | tok/sec: 33998.62\r\n",
      "step:   932 | loss: 3.897892 | lr: 5.0841e-04 | norm: 0.1931 | time: 15428.13ms | tok/sec: 33982.61\r\n",
      "step:   933 | loss: 3.959054 | lr: 5.0093e-04 | norm: 0.2069 | time: 15445.55ms | tok/sec: 33944.27\r\n",
      "step:   934 | loss: 3.887958 | lr: 4.9346e-04 | norm: 0.2048 | time: 15473.48ms | tok/sec: 33883.00\r\n",
      "step:   935 | loss: 3.957063 | lr: 4.8598e-04 | norm: 0.2187 | time: 15438.36ms | tok/sec: 33960.07\r\n",
      "step:   936 | loss: 3.948205 | lr: 4.7850e-04 | norm: 0.2044 | time: 15484.06ms | tok/sec: 33859.84\r\n",
      "step:   937 | loss: 3.930220 | lr: 4.7103e-04 | norm: 0.1997 | time: 15448.76ms | tok/sec: 33937.21\r\n",
      "step:   938 | loss: 3.888462 | lr: 4.6355e-04 | norm: 0.2018 | time: 15424.55ms | tok/sec: 33990.49\r\n",
      "step:   939 | loss: 3.942782 | lr: 4.5607e-04 | norm: 0.1970 | time: 15414.86ms | tok/sec: 34011.86\r\n",
      "step:   940 | loss: 3.914870 | lr: 4.4860e-04 | norm: 0.2011 | time: 15455.55ms | tok/sec: 33922.32\r\n",
      "step:   941 | loss: 3.867644 | lr: 4.4112e-04 | norm: 0.1862 | time: 15433.81ms | tok/sec: 33970.09\r\n",
      "step:   942 | loss: 3.963654 | lr: 4.3364e-04 | norm: 0.1925 | time: 15435.80ms | tok/sec: 33965.71\r\n",
      "step:   943 | loss: 3.894054 | lr: 4.2617e-04 | norm: 0.1892 | time: 15402.16ms | tok/sec: 34039.91\r\n",
      "step:   944 | loss: 3.891301 | lr: 4.1869e-04 | norm: 0.1929 | time: 15429.79ms | tok/sec: 33978.94\r\n",
      "step:   945 | loss: 3.892300 | lr: 4.1121e-04 | norm: 0.2024 | time: 15429.63ms | tok/sec: 33979.31\r\n",
      "step:   946 | loss: 3.875087 | lr: 4.0374e-04 | norm: 0.1825 | time: 15405.02ms | tok/sec: 34033.58\r\n",
      "step:   947 | loss: 3.888910 | lr: 3.9626e-04 | norm: 0.1878 | time: 15450.04ms | tok/sec: 33934.41\r\n",
      "step:   948 | loss: 3.898854 | lr: 3.8879e-04 | norm: 0.1847 | time: 15363.51ms | tok/sec: 34125.53\r\n",
      "step:   949 | loss: 3.896547 | lr: 3.8131e-04 | norm: 0.1828 | time: 15430.63ms | tok/sec: 33977.09\r\n",
      "validation loss: 3.8173\r\n",
      "step:   950 | loss: 3.856906 | lr: 3.7383e-04 | norm: 0.1756 | time: 19177.43ms | tok/sec: 27338.81\r\n",
      "step:   951 | loss: 3.857897 | lr: 3.6636e-04 | norm: 0.1798 | time: 15429.42ms | tok/sec: 33979.75\r\n",
      "step:   952 | loss: 3.799297 | lr: 3.5888e-04 | norm: 0.1684 | time: 15396.48ms | tok/sec: 34052.46\r\n",
      "step:   953 | loss: 3.781864 | lr: 3.5140e-04 | norm: 0.1754 | time: 18858.54ms | tok/sec: 27801.09\r\n",
      "step:   954 | loss: 3.851462 | lr: 3.4393e-04 | norm: 0.1657 | time: 15467.64ms | tok/sec: 33895.80\r\n",
      "step:   955 | loss: 3.845820 | lr: 3.3645e-04 | norm: 0.1843 | time: 15565.42ms | tok/sec: 33682.87\r\n",
      "step:   956 | loss: 3.810051 | lr: 3.2897e-04 | norm: 0.1683 | time: 15417.36ms | tok/sec: 34006.34\r\n",
      "step:   957 | loss: 3.793476 | lr: 3.2150e-04 | norm: 0.1794 | time: 15271.64ms | tok/sec: 34330.82\r\n",
      "step:   958 | loss: 3.769101 | lr: 3.1402e-04 | norm: 0.1766 | time: 15286.97ms | tok/sec: 34296.39\r\n",
      "step:   959 | loss: 3.805839 | lr: 3.0654e-04 | norm: 0.1765 | time: 15328.73ms | tok/sec: 34202.96\r\n",
      "step:   960 | loss: 3.859025 | lr: 2.9907e-04 | norm: 0.2158 | time: 15405.00ms | tok/sec: 34033.62\r\n",
      "step:   961 | loss: 3.738629 | lr: 2.9159e-04 | norm: 0.2030 | time: 15428.20ms | tok/sec: 33982.44\r\n",
      "step:   962 | loss: 3.709225 | lr: 2.8411e-04 | norm: 0.1810 | time: 15529.34ms | tok/sec: 33761.13\r\n",
      "step:   963 | loss: 3.701404 | lr: 2.7664e-04 | norm: 0.1947 | time: 15524.18ms | tok/sec: 33772.34\r\n",
      "step:   964 | loss: 3.665938 | lr: 2.6916e-04 | norm: 0.1802 | time: 15485.68ms | tok/sec: 33856.31\r\n",
      "step:   965 | loss: 3.729630 | lr: 2.6168e-04 | norm: 0.2010 | time: 15488.51ms | tok/sec: 33850.12\r\n",
      "step:   966 | loss: 3.682878 | lr: 2.5421e-04 | norm: 0.1714 | time: 15487.93ms | tok/sec: 33851.39\r\n",
      "step:   967 | loss: 3.683382 | lr: 2.4673e-04 | norm: 0.1719 | time: 15409.28ms | tok/sec: 34024.17\r\n",
      "step:   968 | loss: 3.694570 | lr: 2.3925e-04 | norm: 0.1796 | time: 15368.31ms | tok/sec: 34114.87\r\n",
      "step:   969 | loss: 3.701662 | lr: 2.3178e-04 | norm: 0.1671 | time: 15434.74ms | tok/sec: 33968.05\r\n",
      "step:   970 | loss: 3.721898 | lr: 2.2430e-04 | norm: 0.1767 | time: 15417.81ms | tok/sec: 34005.35\r\n",
      "step:   971 | loss: 3.739209 | lr: 2.1682e-04 | norm: 0.1644 | time: 15370.05ms | tok/sec: 34111.02\r\n",
      "step:   972 | loss: 3.712559 | lr: 2.0935e-04 | norm: 0.1694 | time: 15368.40ms | tok/sec: 34114.69\r\n",
      "step:   973 | loss: 3.822732 | lr: 2.0187e-04 | norm: 0.1665 | time: 15376.97ms | tok/sec: 34095.67\r\n",
      "step:   974 | loss: 3.851343 | lr: 1.9439e-04 | norm: 0.1622 | time: 15402.28ms | tok/sec: 34039.64\r\n",
      "validation loss: 3.7925\r\n",
      "step:   975 | loss: 3.787582 | lr: 1.8692e-04 | norm: 0.1767 | time: 19074.51ms | tok/sec: 27486.31\r\n",
      "step:   976 | loss: 3.886799 | lr: 1.7944e-04 | norm: 0.1685 | time: 15387.63ms | tok/sec: 34072.04\r\n",
      "step:   977 | loss: 3.881725 | lr: 1.7196e-04 | norm: 0.1770 | time: 15429.90ms | tok/sec: 33978.71\r\n",
      "step:   978 | loss: 3.884591 | lr: 1.6449e-04 | norm: 0.1629 | time: 15460.26ms | tok/sec: 33911.98\r\n",
      "step:   979 | loss: 3.881438 | lr: 1.5701e-04 | norm: 0.1675 | time: 15428.86ms | tok/sec: 33981.00\r\n",
      "step:   980 | loss: 3.856085 | lr: 1.4953e-04 | norm: 0.1654 | time: 15467.73ms | tok/sec: 33895.61\r\n",
      "step:   981 | loss: 3.863957 | lr: 1.4206e-04 | norm: 0.1587 | time: 15425.16ms | tok/sec: 33989.16\r\n",
      "step:   982 | loss: 3.803636 | lr: 1.3458e-04 | norm: 0.1565 | time: 15382.04ms | tok/sec: 34084.42\r\n",
      "step:   983 | loss: 3.855104 | lr: 1.2710e-04 | norm: 0.1458 | time: 15472.07ms | tok/sec: 33886.08\r\n",
      "step:   984 | loss: 3.847625 | lr: 1.1963e-04 | norm: 0.1531 | time: 15481.69ms | tok/sec: 33865.03\r\n",
      "step:   985 | loss: 3.811789 | lr: 1.1215e-04 | norm: 0.1586 | time: 15531.35ms | tok/sec: 33756.77\r\n",
      "step:   986 | loss: 3.824936 | lr: 1.0467e-04 | norm: 0.1462 | time: 15420.46ms | tok/sec: 33999.51\r\n",
      "step:   987 | loss: 3.830574 | lr: 9.7196e-05 | norm: 0.1518 | time: 15464.68ms | tok/sec: 33902.29\r\n",
      "step:   988 | loss: 3.862231 | lr: 8.9720e-05 | norm: 0.1612 | time: 15472.02ms | tok/sec: 33886.20\r\n",
      "step:   989 | loss: 3.814595 | lr: 8.2243e-05 | norm: 0.1496 | time: 15465.72ms | tok/sec: 33900.01\r\n",
      "step:   990 | loss: 3.805468 | lr: 7.4766e-05 | norm: 0.1442 | time: 15394.57ms | tok/sec: 34056.68\r\n",
      "step:   991 | loss: 3.851036 | lr: 6.7290e-05 | norm: 0.1498 | time: 15403.26ms | tok/sec: 34037.47\r\n",
      "step:   992 | loss: 3.835018 | lr: 5.9813e-05 | norm: 0.1502 | time: 15445.96ms | tok/sec: 33943.37\r\n",
      "step:   993 | loss: 3.818073 | lr: 5.2336e-05 | norm: 0.1418 | time: 15414.18ms | tok/sec: 34013.35\r\n",
      "step:   994 | loss: 3.786201 | lr: 4.4860e-05 | norm: 0.1530 | time: 15424.34ms | tok/sec: 33990.94\r\n",
      "step:   995 | loss: 3.798175 | lr: 3.7383e-05 | norm: 0.1354 | time: 15339.40ms | tok/sec: 34179.18\r\n",
      "step:   996 | loss: 3.771211 | lr: 2.9907e-05 | norm: 0.1372 | time: 15354.16ms | tok/sec: 34146.31\r\n",
      "step:   997 | loss: 3.791461 | lr: 2.2430e-05 | norm: 0.1399 | time: 15431.88ms | tok/sec: 33974.34\r\n",
      "step:   998 | loss: 3.790176 | lr: 1.4953e-05 | norm: 0.1334 | time: 15419.56ms | tok/sec: 34001.50\r\n",
      "validation loss: 3.7716\r\n",
      "step:   999 | loss: 3.782467 | lr: 7.4766e-06 | norm: 0.1415 | time: 19930.17ms | tok/sec: 26306.25\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fcdcf",
   "metadata": {
    "papermill": {
     "duration": 0.155624,
     "end_time": "2024-07-10T15:05:25.342369",
     "exception": false,
     "start_time": "2024-07-10T15:05:25.186745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## If you want to run on a single device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b7b903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:05:25.655780Z",
     "iopub.status.busy": "2024-07-10T15:05:25.655408Z",
     "iopub.status.idle": "2024-07-10T15:05:25.659667Z",
     "shell.execute_reply": "2024-07-10T15:05:25.658882Z"
    },
    "papermill": {
     "duration": 0.16306,
     "end_time": "2024-07-10T15:05:25.661502",
     "exception": false,
     "start_time": "2024-07-10T15:05:25.498442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d5662e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:05:25.984806Z",
     "iopub.status.busy": "2024-07-10T15:05:25.984449Z",
     "iopub.status.idle": "2024-07-10T15:05:25.988441Z",
     "shell.execute_reply": "2024-07-10T15:05:25.987669Z"
    },
    "papermill": {
     "duration": 0.169054,
     "end_time": "2024-07-10T15:05:25.990339",
     "exception": false,
     "start_time": "2024-07-10T15:05:25.821285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# num_return_sequences = 5\n",
    "# # this is not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711b2a47",
   "metadata": {
    "_cell_guid": "ba7816d1-4ccd-474c-ae38-5cb99ac6e7fc",
    "_uuid": "b44fbc42-eec7-44ab-917a-5d383e443954",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-10T15:05:26.307770Z",
     "iopub.status.busy": "2024-07-10T15:05:26.307417Z",
     "iopub.status.idle": "2024-07-10T15:05:37.187664Z",
     "shell.execute_reply": "2024-07-10T15:05:37.186583Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 11.041108,
     "end_time": "2024-07-10T15:05:37.190213",
     "exception": false,
     "start_time": "2024-07-10T15:05:26.149105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, and I've tried to show people the answer to the \"my\" answer.\n",
      "- The last question to\n",
      "> Hello, I'm a language model, and I'm looking at what a \"tribalist: The Problem of Knowledge\". This doesn't mean\n",
      "> Hello, I'm a language model, i am a bit confused so i can't tell an actual situation then just repeat it until it looks like your\n",
      "> Hello, I'm a language model, and the problem is that it has a relatively cheap method of using a particular function because of its size and meaning\n",
      "> Hello, I'm a language model, and it's very easy to use in most cases.<|endoftext|>The following question: A. What is the role\n"
     ]
    }
   ],
   "source": [
    "# Generate some test responses\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from model import GPT, GPTConfig\n",
    "import tiktoken\n",
    "\n",
    "model = GPT(GPTConfig(), 0)\n",
    "model_dict = torch.load(\"log/model_00999.pt\")['model']\n",
    "\n",
    "state_dict = {}\n",
    "\n",
    "for key in model_dict:\n",
    "    new_key = key.replace(\"_orig_mod.\", \"\")\n",
    "    state_dict[new_key] = model_dict[key]\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "num_return_seq = 5\n",
    "max_length = 30\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_seq, 1)\n",
    "x = tokens.to('cuda')\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(x) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here become (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "for i in range(num_return_seq):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5354353,
     "sourceId": 8905566,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 63588,
     "sourceId": 75682,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15844.453504,
   "end_time": "2024-07-10T15:05:38.102070",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-10T10:41:33.648566",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
